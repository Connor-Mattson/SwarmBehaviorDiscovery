{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Requested to build new dataset in folder that contains items",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [3], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mswarmset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SwarmDataset, DataBuilder\n\u001B[0;32m----> 5\u001B[0m baseline_data \u001B[38;5;241m=\u001B[39m \u001B[43mDataBuilder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/full\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m24\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m baseline_data\u001B[38;5;241m.\u001B[39mcreate()\n\u001B[1;32m      7\u001B[0m baseline_data\u001B[38;5;241m.\u001B[39mevolution\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/Desktop/research/SwarmNoveltyNetwork/data/swarmset.py:109\u001B[0m, in \u001B[0;36mDataBuilder.__init__\u001B[0;34m(self, data_dir, is_anti, is_similar, fixed_a_b, steps, agents)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfixed_a_b \u001B[38;5;241m=\u001B[39m fixed_a_b\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 109\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRequested to build new dataset in folder that contains items\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevolution, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen \u001B[38;5;241m=\u001B[39m HaltedEvolution\u001B[38;5;241m.\u001B[39mdefaultEvolver(steps\u001B[38;5;241m=\u001B[39msteps, n_agents\u001B[38;5;241m=\u001B[39magents)\n",
      "\u001B[0;31mException\u001B[0m: Requested to build new dataset in folder that contains items"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "\n",
    "baseline_data = DataBuilder(\"data/full\", steps=1200, agents=24)\n",
    "baseline_data.create()\n",
    "baseline_data.evolution.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embed into Latent Space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from networks.embedding import NoveltyEmbedding\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding().to(device)\n",
    "print(network)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def network_from_numpy(network, anchor_img, pos_img, neg_img):\n",
    "    anchor_input = torch.from_numpy(anchor_img).to(device).float()\n",
    "    pos_input = torch.from_numpy(pos_img).to(device).float()\n",
    "    neg_input = torch.from_numpy(neg_img).to(device).float()\n",
    "\n",
    "    anchor_out = network(anchor_input.unsqueeze(0))\n",
    "    pos_out = network(pos_input.unsqueeze(0))\n",
    "    neg_out = network(neg_input.unsqueeze(0))\n",
    "    return anchor_out, pos_out, neg_out\n",
    "\n",
    "def batch_network_from_numpy(network, anchor_list, pos_list, neg_list):\n",
    "    anchor_input = torch.from_numpy(anchor_list).to(device).float()\n",
    "    pos_input = torch.from_numpy(pos_list).to(device).float()\n",
    "    neg_input = torch.from_numpy(neg_list).to(device).float()\n",
    "\n",
    "    anchor_out = network(anchor_input)\n",
    "    pos_out = network(pos_input)\n",
    "    neg_out = network(neg_input)\n",
    "    return anchor_out, pos_out, neg_out\n",
    "\n",
    "def combine_hierarchy_samples(class_a, class_b):\n",
    "    out = []\n",
    "    for ancA, posA, negA in class_a:\n",
    "        for ancB, posB, negB in class_b:\n",
    "            if posB != posA:\n",
    "                out.append((posA, posB, negA))\n",
    "                out.append((posB, posA, negB))\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Human in the Loop\n",
    "from sklearn.manifold import TSNE\n",
    "import pygame\n",
    "import torch\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from ui.class_similarity import SimilarityGUI\n",
    "from networks.archive import DataAggregationArchive\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "keys={\n",
    "    pygame.K_KP_PLUS : -1,\n",
    "    pygame.K_KP_PERIOD: -1,\n",
    "    pygame.K_KP_0 : 0,\n",
    "    pygame.K_KP_1 : 1,\n",
    "    pygame.K_KP_2 : 2,\n",
    "    pygame.K_KP_3 : 3,\n",
    "    pygame.K_KP_4 : 4,\n",
    "}\n",
    "\n",
    "gui_labels = {\n",
    "    -1 : \"Random\",\n",
    "    0 : \"0\",\n",
    "    1 : \"1\",\n",
    "    2 : \"2\",\n",
    "    3 : \"3\",\n",
    "    4 : \"4\",\n",
    "}\n",
    "\n",
    "CLUSTERS = 8\n",
    "def humanInput(anchor_dataset, network, optim, loss_fn, data_archive, random_indices, stop_at=None):\n",
    "    # Begin by clustering all known embeddings into n classes.\n",
    "    # Sample from these clusters and use the medoids as the anchors to hopefully scoop up hard samples.\n",
    "    network.eval()\n",
    "    archive = NoveltyArchive()\n",
    "    for i, (anchor_encoding, genome, _, _, _, _, _) in enumerate(anchor_dataset):\n",
    "        if stop_at and i > stop_at:\n",
    "            break\n",
    "        if i in random_indices.archive:\n",
    "            archive.addToArchive(vec=np.array([-1] * 15), genome=genome)\n",
    "        else:\n",
    "            anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "            embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "            archive.addToArchive(vec=embedding, genome=genome)\n",
    "\n",
    "    print(\"Query + Cluter at size: \", len(archive.archive))\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=CLUSTERS, random_state=0).fit(archive.archive)\n",
    "    labels = kmedoids.labels_\n",
    "    medoids = kmedoids.medoid_indices_\n",
    "\n",
    "    network.train()\n",
    "    user_help_cases, avg_loss, total_attempts = 0, 0, 1\n",
    "    skip_clusters = []\n",
    "    cluster_sampling = {}\n",
    "    for cluster_class in range(CLUSTERS + 1):\n",
    "\n",
    "        # Run Triplet Query on the Medoids themselves\n",
    "        if cluster_class == 0:\n",
    "            samples = [medoids[med] for med in range(0, len(medoids))]\n",
    "            subject_images = [anchor_dataset[sampled_class][0] for sampled_class in samples]\n",
    "            ui_input = SimilarityGUI(None, subject_images, keystrokes=keys, labels=gui_labels)\n",
    "            ui_input.run()\n",
    "            user_responses = np.array(ui_input.assignment)\n",
    "\n",
    "            # Assign all the skipped classes\n",
    "            for i, res in enumerate(user_responses):\n",
    "                if res < 0:\n",
    "                    skip_clusters.append(i)\n",
    "                    random_indices.append_scalars([medoids[i]])\n",
    "\n",
    "            # Run triplet loss on medoids.\n",
    "            for i in range(5):\n",
    "                same_class = np.where(user_responses == i)[0]\n",
    "                different_class = np.where(user_responses != i)[0]\n",
    "                if len(same_class) < 2:\n",
    "                    continue\n",
    "                for j in range(len(same_class)):\n",
    "                    for k in range(i + 1, len(same_class)):\n",
    "                        for l in range(len(different_class)):\n",
    "                            anchor_image = anchor_dataset[medoids[j]][0]\n",
    "                            pos_image = anchor_dataset[medoids[k]][0]\n",
    "                            neg_image = anchor_dataset[medoids[l]][0]\n",
    "\n",
    "                            optimizer.zero_grad()\n",
    "                            anchor_out, pos_out, neg_out = network_from_numpy(network, anchor_image, pos_image, neg_image)\n",
    "\n",
    "                            loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "                            avg_loss += loss.item()\n",
    "                            total_attempts += 1\n",
    "                            print(\"User loss: \", loss)\n",
    "                            if loss.item() > 0:\n",
    "                                loss.backward()\n",
    "                                optim.step()\n",
    "                                user_help_cases += 1\n",
    "                                print(\"BACKPROP!\")\n",
    "\n",
    "                            # Add to User Collected Archive\n",
    "                            data_archive.append(medoids[j], medoids[k], medoids[l])\n",
    "                            for random in skip_clusters:\n",
    "                                data_archive.append(medoids[j], medoids[k], medoids[random])\n",
    "\n",
    "        else:\n",
    "            cluster_class -= 1\n",
    "            if cluster_class in skip_clusters:\n",
    "                continue\n",
    "\n",
    "            # Initialize Cluster Sampling\n",
    "            cluster_sampling[cluster_class] = []\n",
    "\n",
    "            examples = np.where(labels == cluster_class)[0]\n",
    "            samples = np.random.choice(examples, min(10, len(examples)), False)\n",
    "\n",
    "            anchor_image = anchor_dataset[medoids[cluster_class]][0]\n",
    "            subject_images = [anchor_dataset[sampled_class][0] for sampled_class in samples]\n",
    "            ui_input = SimilarityGUI(anchor_image, subject_images)\n",
    "            ui_input.run()\n",
    "            user_responses = np.array(ui_input.assignment)\n",
    "\n",
    "            different_class = np.where(user_responses == 1)[0]\n",
    "            same_class = np.where(user_responses == 0)[0]\n",
    "            for i in range(len(different_class)):\n",
    "                for j in range(i, len(same_class)):\n",
    "                    pos_index = same_class[j]\n",
    "                    neg_index = different_class[i]\n",
    "                    anchor_image = anchor_dataset[medoids[cluster_class]][0]\n",
    "                    pos_image = anchor_dataset[samples[pos_index]][0]\n",
    "                    neg_image = anchor_dataset[samples[neg_index]][0]\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    anchor_out, pos_out, neg_out = network_from_numpy(network, anchor_image, pos_image, neg_image)\n",
    "\n",
    "                    loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "                    avg_loss += loss.item()\n",
    "                    total_attempts += 1\n",
    "                    # print(\"User loss: \", loss)\n",
    "                    if loss.item() > 0:\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        user_help_cases += 1\n",
    "                        # print(\"BACKPROP!\")\n",
    "\n",
    "                    # Add to User Collected Archive\n",
    "                    data_archive.append(medoids[cluster_class], samples[pos_index], samples[neg_index])\n",
    "                    cluster_sampling[cluster_class].append((medoids[cluster_class], samples[pos_index], samples[neg_index]))\n",
    "\n",
    "\n",
    "    # Run Hierarchical Comparison in the cluster elements.\n",
    "    for i in range(5):\n",
    "        same_class = np.where(user_responses == i)[0]\n",
    "        for j in range(len(same_class)):\n",
    "            for k in range(i + 1, len(same_class)):\n",
    "                if j in cluster_sampling and k in cluster_sampling:\n",
    "                    combined_samples = combine_hierarchy_samples(cluster_sampling[j], cluster_sampling[k])\n",
    "                    for anc, pos, neg in combined_samples:\n",
    "                        data_archive.append(anc, pos, neg)\n",
    "\n",
    "    return user_help_cases, avg_loss / total_attempts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Data at size: 1\n",
      "Average Loss: 0.0\n",
      "Epoch: 0\n",
      "HIL TIME!\n",
      "Query + Cluter at size:  1101\n",
      "Improvement Count: 0, loss: 0.0\n",
      "User Data at size: 1\n",
      "Average Loss: 0.0\n",
      "Epoch: 1\n",
      "HIL TIME!\n",
      "Query + Cluter at size:  1201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 44\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(HIL_archive) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (self_l_hist \u001B[38;5;129;01mand\u001B[39;00m self_l_hist[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m<\u001B[39m EPSILON) \u001B[38;5;129;01mor\u001B[39;00m repeats:\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHIL TIME!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 44\u001B[0m     improvements, human_loss \u001B[38;5;241m=\u001B[39m \u001B[43mhumanInput\u001B[49m\u001B[43m(\u001B[49m\u001B[43manchor_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mHIL_archive\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_archive\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_at\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_set_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImprovement Count: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimprovements\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhuman_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     46\u001B[0m     human_l_hist[epoch] \u001B[38;5;241m=\u001B[39m human_loss\n",
      "Cell \u001B[0;32mIn [2], line 63\u001B[0m, in \u001B[0;36mhumanInput\u001B[0;34m(anchor_dataset, network, optim, loss_fn, data_archive, random_indices, stop_at)\u001B[0m\n\u001B[1;32m     61\u001B[0m subject_images \u001B[38;5;241m=\u001B[39m [anchor_dataset[sampled_class][\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m sampled_class \u001B[38;5;129;01min\u001B[39;00m samples]\n\u001B[1;32m     62\u001B[0m ui_input \u001B[38;5;241m=\u001B[39m SimilarityGUI(\u001B[38;5;28;01mNone\u001B[39;00m, subject_images, keystrokes\u001B[38;5;241m=\u001B[39mkeys, labels\u001B[38;5;241m=\u001B[39mgui_labels)\n\u001B[0;32m---> 63\u001B[0m \u001B[43mui_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m user_responses \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(ui_input\u001B[38;5;241m.\u001B[39massignment)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# Assign all the skipped classes\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/research/SwarmNoveltyNetwork/ui/class_similarity.py:105\u001B[0m, in \u001B[0;36mSimilarityGUI.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, img \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubject_s):\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshowImg(img, i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 105\u001B[0m     \u001B[43mpygame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m pygame\u001B[38;5;241m.\u001B[39mquit()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from generation.evolution import ModifiedHaltingEvolution\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from NovelSwarmBehavior.novel_swarms.config.ResultsConfig import ResultsConfig\n",
    "from NovelSwarmBehavior.novel_swarms.results.results import main as results\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from matplotlib import pyplot as plot\n",
    "import numpy as np\n",
    "\n",
    "TRAIN = True\n",
    "CLUSTER_AND_DISPLAY = True\n",
    "WRITE_OUT = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "network = NoveltyEmbedding().to(device)\n",
    "anchor_dataset = SwarmDataset(\"data/full\", rank=0)\n",
    "evolution, screen = ModifiedHaltingEvolution.defaultEvolver(steps=500, n_agents=10)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "# Margin was 10\n",
    "loss_fn = torch.nn.TripletMarginLoss(margin=15)\n",
    "\n",
    "EPOCHS = 50\n",
    "self_l_hist = []\n",
    "random_indices = []\n",
    "human_l_hist = [0.0 for _ in range(EPOCHS)]\n",
    "HIL_archive = DataAggregationArchive()\n",
    "random_archive = DataAggregationArchive(scalar=True)\n",
    "data_set_size = 1000\n",
    "EPSILON = 0.5\n",
    "\n",
    "if TRAIN:\n",
    "    STOP_FLAG = False\n",
    "    for epoch in range(EPOCHS):\n",
    "        if STOP_FLAG:\n",
    "            break\n",
    "\n",
    "        repeats = False if len(self_l_hist) < 3 else (self_l_hist[-3] < sum(self_l_hist[-3:]) / 3)\n",
    "        if len(HIL_archive) == 0 or (self_l_hist and self_l_hist[-1] < EPSILON) or repeats:\n",
    "            print(\"HIL TIME!\")\n",
    "            improvements, human_loss = humanInput(anchor_dataset, network, optimizer, loss_fn, HIL_archive, random_archive, stop_at=data_set_size)\n",
    "            print(f\"Improvement Count: {improvements}, loss: {human_loss}\")\n",
    "            human_l_hist[epoch] = human_loss\n",
    "\n",
    "            HIL_archive.save_to_file(\"data/queries/triplets2.csv\")\n",
    "            random_archive.save_to_file(\"data/queries/random2.csv\")\n",
    "\n",
    "        data_set_size += 100\n",
    "\n",
    "        loss_sum = 0\n",
    "        total_loss = 0\n",
    "        # Train on known archive\n",
    "        print(f\"User Data at size: {len(HIL_archive)}\")\n",
    "\n",
    "        # Train on past user information]\n",
    "        BATCH_SIZE = 20\n",
    "        anchor_list, pos_list, neg_list = None, None, None\n",
    "        for i, (anchor, pos, neg) in enumerate(HIL_archive):\n",
    "            anchor_encoding = np.expand_dims(anchor_dataset[anchor][0], axis=0)\n",
    "            similar_encoding = np.expand_dims(anchor_dataset[pos][0], axis=0)\n",
    "            anti_encoding = np.expand_dims(anchor_dataset[neg][0], axis=0)\n",
    "\n",
    "            if anchor_list is None:\n",
    "                anchor_list = np.array([anchor_encoding])\n",
    "                pos_list = np.array([similar_encoding])\n",
    "                neg_list = np.array([anti_encoding])\n",
    "\n",
    "            else:\n",
    "                anchor_list = np.concatenate((anchor_list, np.array([anchor_encoding])))\n",
    "                pos_list = np.concatenate((pos_list, np.array([similar_encoding])))\n",
    "                neg_list = np.concatenate((neg_list, np.array([anti_encoding])))\n",
    "\n",
    "            if len(anchor_list) >= BATCH_SIZE:\n",
    "                optimizer.zero_grad()\n",
    "                anchor_out, pos_out, neg_out = batch_network_from_numpy(network, anchor_list, pos_list, neg_list)\n",
    "                loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "                loss_sum += loss.item()\n",
    "                if loss.item() > 0:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                total_loss += 1\n",
    "                anchor_list, pos_list, neg_list = None, None, None\n",
    "\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                print(f\"Epoch Progress: {(i*100) / len(HIL_archive)}%, Immediate Loss: {loss.item()}\")\n",
    "\n",
    "        print(f\"Average Loss: {loss_sum / (total_loss + 1)}\")\n",
    "        self_l_hist.append(loss_sum / (total_loss + 1))\n",
    "\n",
    "        if len(self_l_hist) > 3 and self_l_hist[-1] == self_l_hist[-2] and self_l_hist[-2] == self_l_hist[-3]:\n",
    "            loss_sum = 0\n",
    "\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "\n",
    "evolution.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save Model\n",
    "network.save_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from NovelSwarmBehavior.novel_swarms.config.ResultsConfig import ResultsConfig\n",
    "from NovelSwarmBehavior.novel_swarms.results.results import main as results\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding().to(device)\n",
    "network.load_model(\"testA\")\n",
    "anchor_dataset = SwarmDataset(\"data/full\", rank=0)\n",
    "\n",
    "WRITE_OUT = True\n",
    "if WRITE_OUT:\n",
    "    network.eval()\n",
    "    test_archive = NoveltyArchive()\n",
    "    for i, (anchor_encoding, genome, _, _, _, _, _) in enumerate(anchor_dataset):\n",
    "        if i > 5000:\n",
    "            break\n",
    "        anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "        embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "        test_archive.addToArchive(embedding, genome)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/Desktop/research/SwarmNoveltyNetwork/.env/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [14], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m config \u001B[38;5;241m=\u001B[39m ResultsConfig(archive\u001B[38;5;241m=\u001B[39mtest_archive, k_clusters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, world_config\u001B[38;5;241m=\u001B[39mworld_config, tsne_perplexity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, tsne_early_exaggeration\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, skip_tsne\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      8\u001B[0m gui \u001B[38;5;241m=\u001B[39m ClusteringGUI(config)\n\u001B[0;32m----> 9\u001B[0m \u001B[43mgui\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplayGUI\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# results(config)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m pygame\u001B[38;5;241m.\u001B[39mquit()\n",
      "File \u001B[0;32m~/Desktop/research/SwarmNoveltyNetwork/NovelSwarmBehavior/novel_swarms/results/Cluster.py:102\u001B[0m, in \u001B[0;36mCluster.displayGUI\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    100\u001B[0m pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mset_caption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClustered Swarm Novelty\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    101\u001B[0m screen \u001B[38;5;241m=\u001B[39m pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mset_mode((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mGUI_WIDTH, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mGUI_HEIGHT))\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunDisplayLoop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscreen\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/research/SwarmNoveltyNetwork/NovelSwarmBehavior/novel_swarms/results/Cluster.py:138\u001B[0m, in \u001B[0;36mCluster.runDisplayLoop\u001B[0;34m(self, screen)\u001B[0m\n\u001B[1;32m    136\u001B[0m screen\u001B[38;5;241m.\u001B[39mfill((\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cluster_point \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoint_population:\n\u001B[0;32m--> 138\u001B[0m     \u001B[43mcluster_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscreen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cluster_center \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcluster_medoids:\n\u001B[1;32m    141\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdraw\u001B[38;5;241m.\u001B[39mcircle(screen, (\u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m), (\u001B[38;5;28mint\u001B[39m(cluster_center[\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;28mint\u001B[39m(cluster_center[\u001B[38;5;241m1\u001B[39m])),\n\u001B[1;32m    142\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMEDOID_RADIUS, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/research/SwarmNoveltyNetwork/NovelSwarmBehavior/novel_swarms/results/ClusterPoint.py:19\u001B[0m, in \u001B[0;36mClusterPoint.draw\u001B[0;34m(self, screen)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdraw\u001B[39m(\u001B[38;5;28mself\u001B[39m, screen):\n\u001B[1;32m     18\u001B[0m     filled \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_highlighted \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 19\u001B[0m     \u001B[43mpygame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcircle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscreen\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mradius\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilled\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "import pygame\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=test_archive, k_clusters=8, world_config=world_config, tsne_perplexity=1, tsne_early_exaggeration=1, skip_tsne=False)\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()\n",
    "# results(config)\n",
    "pygame.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering + Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/Desktop/research/SwarmNoveltyNetwork/.env/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "\n",
    "# Cluster over saved behaviors\n",
    "archive = NoveltyArchive()\n",
    "for i, (_, genome, behavior, _, _, _, _) in enumerate(anchor_dataset):\n",
    "    archive.addToArchive(vec=behavior, genome=genome)\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=archive, k_clusters=7, world_config=world_config, tsne_perplexity=20, tsne_early_exaggeration=2, skip_tsne=False)\n",
    "\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(human_l_hist, \"b\", label='Human Loss')\n",
    "plt.plot(self_l_hist, \"r\", label='Self Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Time (Epochs)\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
