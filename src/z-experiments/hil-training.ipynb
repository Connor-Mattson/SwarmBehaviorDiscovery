{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from src.networks.embedding import NoveltyEmbedding\n",
    "from src.networks.archive import DataAggregationArchive\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
    "from src.networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list\n",
    "\n",
    "def resizeInput(X, w=200):\n",
    "    frame = X.astype(np.uint8)\n",
    "    resized = cv2.resize(frame, dsize=(w, w), interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def translate(img, offset=(10, 10)):\n",
    "    h, w = img.shape\n",
    "    xoff, yoff = offset\n",
    "    if xoff < 0: xpadding = (0, -xoff)\n",
    "    else: xpadding = (xoff, 0)\n",
    "    if yoff < 0: ypadding = (0, -yoff)\n",
    "    else: ypadding = (yoff, 0)\n",
    "    img = np.pad(img, (xpadding, ypadding))\n",
    "\n",
    "    if xoff >= 0 and yoff >= 0:\n",
    "        return img[:w, :w]\n",
    "    elif xoff < 0 and yoff >= 0:\n",
    "        return img[-w:, :w]\n",
    "    elif xoff >= 0 and yoff < 0:\n",
    "        return img[:w, -w:]\n",
    "    return img[-w:, -w:]\n",
    "\n",
    "def zoom_at(img, zoom, coord=None):\n",
    "    # Adapted from https://stackoverflow.com/questions/69050464/zoom-into-image-with-opencv\n",
    "    h, w = [ zoom * i for i in img.shape ]\n",
    "    if coord is None: cx, cy = w/2, h/2\n",
    "    else: cx, cy = [ zoom*c for c in coord ]\n",
    "    img = cv2.resize( img, (0, 0), fx=zoom, fy=zoom)\n",
    "    img = img[ int(round(cy - h/zoom * .5)) : int(round(cy + h/zoom * .5)),\n",
    "               int(round(cx - w/zoom * .5)) : int(round(cx + w/zoom * .5))]\n",
    "    return img\n",
    "\n",
    "def get_color_distortion(X, s=3.0):\n",
    "    X = X + s * np.random.randn(X.shape[0], X.shape[1])\n",
    "    return X\n",
    "\n",
    "def getRandomFlip(X):\n",
    "    tmp = torch.tensor(X).unsqueeze(0)\n",
    "    flipper_A = RandomHorizontalFlip(0.5)\n",
    "    flipper_B = RandomVerticalFlip(0.5)\n",
    "    image = flipper_A(flipper_B(tmp))\n",
    "    image = image.squeeze(0).numpy()\n",
    "    return image\n",
    "\n",
    "def getRandomTransformation(image, k=2):\n",
    "    transformation_choices = [\"Rotation\", \"Blur\", \"Zoom\", \"Translate\", \"Distort\", \"ResizedCrop\"]\n",
    "    # weights = [0.4, 0.3, 0.0, 0.2]\n",
    "    # weights = [1.0, 0.0, 0.0, 0.0]\n",
    "    # choices = random.choices(transformation_choices, weights, k=k)\n",
    "    choices = [\"ResizedCrop\"]\n",
    "    # choices = []\n",
    "    if \"RandomFlip\" in choices:\n",
    "        image = getRandomFlip(image)\n",
    "    if \"ResizedCrop\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(0.5)\n",
    "        cropper = RandomResizedCrop(size=(50,50), scale=(0.6, 1.0), ratio=(1.0, 1.0))\n",
    "        image = flipper(cropper(tmp))\n",
    "        image = image.squeeze(0).numpy()\n",
    "    if \"Rotation\" in choices:\n",
    "        theta = random.choice([90, 180, 270])\n",
    "        image = ndimage.rotate(image, theta)\n",
    "    if \"Blur\" in choices:\n",
    "        blur = random.choice([0.5, 1.0, 1.5])\n",
    "        image = ndimage.gaussian_filter(image, sigma=blur)\n",
    "    if \"Zoom\" in choices:\n",
    "        # zoom = random.choice([1.06, 1.12, 1.18])\n",
    "        padding = random.choice([10])\n",
    "        padded = np.pad(image, padding, mode='constant')\n",
    "        image = resizeInput(padded, 50)\n",
    "    if \"Translate\" in choices:\n",
    "        offsets = [i for i in range(-10, 10, 2)]\n",
    "        offset = (random.choice(offsets), random.choice(offsets))\n",
    "        # offset = (2, 2)\n",
    "        image = translate(image, offset)\n",
    "    if \"Distort\" in choices:\n",
    "        strength = random.choice([3.0, 5.0, 10.0])\n",
    "        image = get_color_distortion(image, s=strength)\n",
    "    if \"Flip\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(1.0)\n",
    "        image = flipper(tmp)\n",
    "        image = image.squeeze(0).numpy()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3204\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Triplets based off of labeled classes (IID_TRIPLETS) or triplets based off ensemble Queries\n",
    "\"\"\"\n",
    "FROM_SCRATCH = False\n",
    "HEURISTIC = False\n",
    "IID_TRIPLETS = False\n",
    "EIGHT_FOR_EIGHT_TRIPLETS = True\n",
    "TWO_SENSOR = False\n",
    "\n",
    "if TWO_SENSOR:\n",
    "    TRUTH_FILE = \"gecco-two-sensor-classes.txt\" if not HEURISTIC else \"heuristic-two-sensor.txt\"\n",
    "    DATASET = SwarmDataset(\"../data/gecco-two-sensor\", rank=0) if not HEURISTIC else SwarmDataset(\"../data/gecco-filtered-two-sensor\")\n",
    "    ENSEMBLE_PATH = \"../checkpoints/ensembles/01-28-23-2S-Pre-B\" if not HEURISTIC else \"../checkpoints/ensembles/01-30-23-2S-Heur-Pre-B\"\n",
    "else:\n",
    "    TRUTH_FILE = \"original-hand-labeled-classes.txt\" if not HEURISTIC else \"heuristic-simple-model-classes.txt\"\n",
    "    DATASET = SwarmDataset(\"../data/full-mini\", rank=0) if not HEURISTIC else SwarmDataset(\"../data/filtered-full\")\n",
    "    ENSEMBLE_PATH = \"../checkpoints/ensembles/01-20-23-baseline\" if not HEURISTIC else \"../checkpoints/ensembles/01-26-23-heuristic-BL-pretraining\"\n",
    "\n",
    "if FROM_SCRATCH:\n",
    "    ENSEMBLE_PATH = None\n",
    "OUT = \"../data/oracle\"\n",
    "classes = [-1 for i in range(500)]\n",
    "with open(os.path.join(OUT, TRUTH_FILE), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i > len(classes) - 1:\n",
    "            break\n",
    "        triplet = CSVLineToVec(line)\n",
    "        classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "triplets = []\n",
    "\n",
    "if IID_TRIPLETS:\n",
    "    for i, i_c in enumerate(classes):\n",
    "        if i_c == 0:\n",
    "            continue\n",
    "        continue_to_top = False\n",
    "        for j, j_c in enumerate(classes):\n",
    "            if j_c != i_c:\n",
    "                continue\n",
    "            if i == j:\n",
    "                continue\n",
    "            for k, k_c in enumerate(classes):\n",
    "                if k_c == 0:\n",
    "                    continue\n",
    "                if k_c == i_c or k_c == j_c:\n",
    "                    continue\n",
    "                # if i_c == 0:\n",
    "                #     if not (i, i, k) in triplets:\n",
    "                #         triplets.append((i, i, k))\n",
    "                #         continue_to_top = True\n",
    "                triplets.append((i, j, k))\n",
    "            if continue_to_top:\n",
    "                break\n",
    "\n",
    "elif EIGHT_FOR_EIGHT_TRIPLETS:\n",
    "    for m in range(0, len(classes), 8):\n",
    "        if m + 8 > len(classes):\n",
    "            continue\n",
    "        temp_classes = classes[m:m+8]\n",
    "        for i, i_c in enumerate(temp_classes):\n",
    "            continue_to_top = False\n",
    "            for j, j_c in enumerate(temp_classes):\n",
    "                if j_c != i_c:\n",
    "                    continue\n",
    "                if i == j:\n",
    "                    continue\n",
    "                for k, k_c in enumerate(temp_classes):\n",
    "                    if k_c == i_c or k_c == j_c:\n",
    "                        continue\n",
    "                    if not (m + i, m + j, m + k) in triplets:\n",
    "                        triplets.append((m + i, m + j, m + k))\n",
    "\n",
    "# Else, use an ensemble to create the triplets.\n",
    "else:\n",
    "    print(\"No Implementation Yet\")\n",
    "\n",
    "print(len(triplets))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 8, 13) (0, 0, 1)\n",
      "(80, 87, 83) (4, 4, 1)\n",
      "(245, 243, 240) (0, 0, 2)\n",
      "(131, 128, 132) (2, 2, 0)\n",
      "(425, 427, 431) (0, 0, 4)\n",
      "(217, 218, 219) (0, 0, 5)\n",
      "(245, 247, 244) (0, 0, 4)\n",
      "(415, 414, 411) (0, 0, 2)\n",
      "(28, 31, 27) (0, 0, 4)\n",
      "(241, 245, 240) (0, 0, 2)\n",
      "(31, 25, 29) (0, 0, 4)\n",
      "(253, 254, 252) (0, 0, 4)\n",
      "(114, 117, 116) (0, 0, 2)\n",
      "(340, 336, 339) (0, 0, 5)\n",
      "(132, 134, 131) (0, 0, 2)\n",
      "(378, 379, 376) (0, 0, 4)\n",
      "(121, 122, 120) (0, 0, 1)\n",
      "(125, 124, 120) (0, 0, 1)\n",
      "(152, 156, 158) (0, 0, 1)\n",
      "(336, 341, 342) (0, 0, 5)\n",
      "(24, 28, 29) (0, 0, 4)\n",
      "(111, 105, 106) (0, 0, 2)\n",
      "(125, 122, 120) (0, 0, 1)\n",
      "(304, 308, 309) (2, 2, 0)\n",
      "(356, 353, 355) (0, 0, 2)\n",
      "(347, 345, 349) (0, 0, 2)\n",
      "(382, 379, 376) (0, 0, 4)\n",
      "(459, 456, 461) (0, 0, 5)\n",
      "(110, 105, 106) (0, 0, 2)\n",
      "(306, 305, 304) (0, 0, 2)\n",
      "(369, 370, 372) (0, 0, 1)\n",
      "(358, 356, 355) (0, 0, 2)\n",
      "(276, 279, 272) (0, 0, 2)\n",
      "(9, 8, 13) (0, 0, 1)\n",
      "(235, 237, 232) (0, 0, 4)\n",
      "(296, 303, 298) (0, 0, 4)\n",
      "(446, 441, 440) (0, 0, 3)\n",
      "(437, 432, 439) (0, 0, 4)\n",
      "(257, 260, 262) (0, 0, 2)\n",
      "(203, 201, 207) (0, 0, 1)\n",
      "(140, 139, 136) (0, 0, 1)\n",
      "(321, 324, 322) (0, 0, 1)\n",
      "(202, 203, 206) (0, 0, 2)\n",
      "(236, 237, 238) (0, 0, 2)\n",
      "(481, 484, 482) (0, 0, 4)\n",
      "(251, 254, 248) (0, 0, 2)\n",
      "(356, 357, 355) (0, 0, 2)\n",
      "(209, 212, 208) (0, 0, 4)\n",
      "(276, 275, 272) (0, 0, 2)\n",
      "(375, 374, 372) (0, 0, 1)\n",
      "(412, 415, 411) (0, 0, 2)\n",
      "(47, 43, 45) (0, 0, 2)\n",
      "(197, 192, 199) (0, 0, 4)\n",
      "(236, 233, 238) (0, 0, 2)\n",
      "(409, 408, 411) (0, 0, 2)\n",
      "(211, 214, 210) (4, 4, 0)\n",
      "(154, 159, 158) (0, 0, 1)\n",
      "(245, 241, 244) (0, 0, 4)\n",
      "(103, 99, 97) (0, 0, 4)\n",
      "(101, 99, 97) (0, 0, 4)\n",
      "(98, 102, 97) (0, 0, 4)\n",
      "(47, 41, 42) (0, 0, 4)\n",
      "(98, 103, 97) (0, 0, 4)\n",
      "(65, 69, 70) (0, 0, 5)\n",
      "(130, 129, 131) (0, 0, 2)\n",
      "(105, 109, 106) (0, 0, 2)\n",
      "(446, 447, 440) (0, 0, 3)\n",
      "(460, 459, 458) (0, 0, 4)\n",
      "(23, 21, 22) (0, 0, 1)\n",
      "(141, 138, 136) (0, 0, 1)\n",
      "(341, 337, 342) (0, 0, 5)\n",
      "(245, 246, 240) (0, 0, 2)\n",
      "(72, 79, 75) (0, 0, 4)\n",
      "(371, 375, 372) (0, 0, 1)\n",
      "(311, 307, 304) (0, 0, 2)\n",
      "(123, 126, 120) (0, 0, 1)\n",
      "(463, 456, 461) (0, 0, 5)\n",
      "(123, 125, 120) (0, 0, 1)\n",
      "(282, 287, 285) (0, 0, 5)\n",
      "(122, 127, 120) (0, 0, 1)\n",
      "(378, 383, 376) (0, 0, 4)\n",
      "(348, 345, 344) (0, 0, 2)\n",
      "(352, 357, 355) (0, 0, 2)\n",
      "(138, 139, 136) (0, 0, 1)\n",
      "(237, 236, 238) (0, 0, 2)\n",
      "(28, 25, 27) (0, 0, 4)\n",
      "(254, 251, 248) (0, 0, 2)\n",
      "(129, 130, 131) (0, 0, 2)\n",
      "(54, 51, 52) (0, 0, 1)\n",
      "(9, 8, 14) (0, 0, 4)\n",
      "(441, 444, 440) (0, 0, 3)\n",
      "(295, 289, 293) (4, 4, 0)\n",
      "(116, 112, 118) (2, 2, 4)\n",
      "(299, 302, 297) (0, 0, 4)\n",
      "(250, 255, 252) (0, 0, 4)\n",
      "(190, 188, 184) (0, 0, 3)\n",
      "(472, 479, 475) (0, 0, 2)\n",
      "(459, 457, 461) (0, 0, 5)\n",
      "(463, 457, 462) (0, 0, 4)\n",
      "(344, 349, 351) (2, 2, 0)\n",
      "(305, 307, 308) (0, 0, 2)\n",
      "(280, 283, 281) (0, 0, 4)\n",
      "(470, 469, 465) (0, 0, 4)\n",
      "(309, 307, 308) (0, 0, 2)\n",
      "(443, 447, 440) (0, 0, 3)\n",
      "(327, 326, 325) (0, 0, 2)\n",
      "(115, 119, 118) (0, 0, 4)\n",
      "(24, 26, 29) (0, 0, 4)\n",
      "(34, 38, 37) (0, 0, 1)\n",
      "(143, 137, 136) (0, 0, 1)\n",
      "(173, 172, 175) (0, 0, 2)\n",
      "(81, 86, 80) (0, 0, 4)\n",
      "(277, 274, 272) (0, 0, 2)\n",
      "(104, 110, 108) (0, 0, 4)\n",
      "(61, 59, 57) (0, 0, 4)\n",
      "(56, 63, 57) (0, 0, 4)\n",
      "(5, 0, 1) (0, 0, 1)\n",
      "(85, 80, 81) (4, 4, 0)\n",
      "(262, 261, 259) (2, 2, 0)\n",
      "(130, 135, 128) (0, 0, 2)\n",
      "(242, 241, 244) (0, 0, 4)\n",
      "(259, 258, 261) (0, 0, 2)\n",
      "(185, 188, 184) (0, 0, 3)\n",
      "(203, 200, 206) (0, 0, 2)\n",
      "(188, 190, 186) (0, 0, 2)\n",
      "(473, 479, 475) (0, 0, 2)\n",
      "(24, 31, 29) (0, 0, 4)\n",
      "(26, 31, 29) (0, 0, 4)\n",
      "(28, 25, 29) (0, 0, 4)\n",
      "(260, 263, 261) (0, 0, 2)\n",
      "(314, 312, 315) (0, 0, 2)\n",
      "(291, 292, 289) (0, 0, 4)\n",
      "(193, 196, 199) (0, 0, 4)\n",
      "(80, 87, 84) (4, 4, 0)\n",
      "(200, 201, 207) (0, 0, 1)\n",
      "(107, 104, 106) (0, 0, 2)\n",
      "(27, 29, 26) (4, 4, 0)\n",
      "(117, 114, 118) (0, 0, 4)\n",
      "(469, 466, 464) (0, 0, 5)\n",
      "(287, 282, 285) (0, 0, 5)\n",
      "(291, 288, 295) (0, 0, 4)\n",
      "(346, 350, 344) (0, 0, 2)\n",
      "(223, 222, 220) (0, 0, 4)\n",
      "(427, 428, 431) (0, 0, 4)\n",
      "(337, 336, 342) (0, 0, 5)\n",
      "(288, 290, 289) (0, 0, 4)\n",
      "(382, 378, 376) (0, 0, 4)\n",
      "(4, 2, 1) (0, 0, 1)\n",
      "(324, 327, 322) (0, 0, 1)\n",
      "(247, 245, 240) (0, 0, 2)\n",
      "(92, 88, 89) (0, 0, 4)\n",
      "(317, 318, 315) (0, 0, 2)\n",
      "(259, 257, 256) (0, 0, 3)\n",
      "(442, 443, 440) (0, 0, 3)\n",
      "(321, 323, 322) (0, 0, 1)\n",
      "(91, 88, 89) (0, 0, 4)\n",
      "(342, 339, 343) (5, 5, 0)\n",
      "(299, 296, 301) (0, 0, 2)\n",
      "(16, 23, 19) (0, 0, 1)\n",
      "(25, 31, 29) (0, 0, 4)\n",
      "(456, 460, 462) (0, 0, 4)\n",
      "(477, 478, 475) (0, 0, 2)\n",
      "(190, 191, 186) (0, 0, 2)\n",
      "(241, 246, 240) (0, 0, 2)\n",
      "(335, 330, 331) (0, 0, 1)\n",
      "(469, 471, 467) (0, 0, 1)\n",
      "(141, 140, 136) (0, 0, 1)\n",
      "(436, 435, 438) (0, 0, 4)\n",
      "(303, 299, 298) (0, 0, 4)\n",
      "(12, 15, 14) (0, 0, 4)\n",
      "(223, 216, 220) (0, 0, 4)\n",
      "(66, 65, 70) (0, 0, 5)\n",
      "(309, 305, 304) (0, 0, 2)\n",
      "(67, 64, 68) (0, 0, 1)\n",
      "(79, 78, 75) (0, 0, 4)\n",
      "(84, 81, 80) (0, 0, 4)\n",
      "(203, 201, 206) (0, 0, 2)\n",
      "(353, 359, 355) (0, 0, 2)\n",
      "(69, 66, 68) (0, 0, 1)\n",
      "(40, 42, 44) (4, 4, 2)\n",
      "(371, 368, 372) (0, 0, 1)\n",
      "(218, 221, 220) (0, 0, 4)\n",
      "(135, 129, 133) (0, 0, 4)\n",
      "(113, 117, 112) (0, 0, 2)\n",
      "(481, 486, 482) (0, 0, 4)\n",
      "(171, 174, 175) (0, 0, 2)\n",
      "(132, 134, 133) (0, 0, 4)\n",
      "(460, 459, 461) (0, 0, 5)\n",
      "(65, 64, 68) (0, 0, 1)\n",
      "(259, 263, 262) (0, 0, 2)\n",
      "(63, 61, 60) (0, 0, 4)\n",
      "(72, 77, 75) (0, 0, 4)\n",
      "(42, 40, 47) (4, 4, 0)\n",
      "(433, 437, 439) (0, 0, 4)\n",
      "(280, 287, 281) (0, 0, 4)\n",
      "(412, 409, 411) (0, 0, 2)\n",
      "(60, 57, 62) (4, 4, 0)\n",
      "(85, 87, 82) (4, 4, 0)\n",
      "(205, 201, 206) (0, 0, 2)\n",
      "(201, 205, 207) (0, 0, 1)\n",
      "(12, 8, 14) (0, 0, 4)\n",
      "(210, 212, 208) (0, 0, 4)\n",
      "(71, 65, 70) (0, 0, 5)\n",
      "(171, 170, 175) (0, 0, 2)\n",
      "(210, 213, 211) (0, 0, 4)\n",
      "(9, 15, 14) (0, 0, 4)\n",
      "(196, 193, 199) (0, 0, 4)\n",
      "(365, 366, 362) (0, 0, 2)\n",
      "(32, 38, 37) (0, 0, 1)\n",
      "(257, 260, 256) (0, 0, 3)\n",
      "(121, 123, 120) (0, 0, 1)\n",
      "(56, 58, 60) (0, 0, 4)\n",
      "(117, 113, 112) (0, 0, 2)\n",
      "(125, 126, 120) (0, 0, 1)\n",
      "(347, 345, 344) (0, 0, 2)\n",
      "(286, 283, 281) (0, 0, 4)\n",
      "(343, 336, 342) (0, 0, 5)\n",
      "(302, 299, 297) (0, 0, 4)\n",
      "(67, 66, 68) (0, 0, 1)\n",
      "(350, 351, 344) (0, 0, 2)\n",
      "(397, 394, 395) (0, 0, 4)\n",
      "(311, 305, 304) (0, 0, 2)\n",
      "(25, 31, 27) (0, 0, 4)\n",
      "(456, 459, 458) (0, 0, 4)\n",
      "(203, 205, 204) (0, 0, 4)\n",
      "(245, 243, 244) (0, 0, 4)\n",
      "(338, 343, 339) (0, 0, 5)\n",
      "(306, 311, 304) (0, 0, 2)\n",
      "(140, 141, 136) (0, 0, 1)\n",
      "(57, 60, 56) (4, 4, 0)\n",
      "(287, 284, 281) (0, 0, 4)\n",
      "(366, 360, 363) (0, 0, 2)\n",
      "(319, 318, 315) (0, 0, 2)\n",
      "(188, 185, 186) (0, 0, 2)\n",
      "(86, 81, 85) (0, 0, 4)\n",
      "(121, 126, 120) (0, 0, 1)\n",
      "(155, 154, 157) (0, 0, 2)\n",
      "(334, 333, 331) (0, 0, 1)\n",
      "(350, 346, 344) (0, 0, 2)\n",
      "(347, 351, 344) (0, 0, 2)\n",
      "(242, 243, 244) (0, 0, 4)\n",
      "(79, 76, 74) (0, 0, 2)\n",
      "(153, 156, 158) (0, 0, 1)\n",
      "(132, 135, 133) (0, 0, 4)\n",
      "(96, 102, 97) (0, 0, 4)\n",
      "(16, 21, 19) (0, 0, 1)\n",
      "(287, 280, 285) (0, 0, 5)\n",
      "(293, 294, 295) (0, 0, 4)\n",
      "(303, 302, 298) (0, 0, 4)\n",
      "(426, 430, 431) (0, 0, 4)\n",
      "(245, 242, 244) (0, 0, 4)\n",
      "(443, 445, 440) (0, 0, 3)\n",
      "(71, 69, 68) (0, 0, 1)\n",
      "(428, 430, 429) (0, 0, 4)\n",
      "(473, 474, 475) (0, 0, 2)\n",
      "(114, 115, 116) (0, 0, 2)\n",
      "(398, 393, 395) (0, 0, 4)\n",
      "(303, 302, 301) (0, 0, 2)\n",
      "(311, 310, 308) (0, 0, 2)\n",
      "(290, 291, 295) (0, 0, 4)\n",
      "(197, 195, 199) (0, 0, 4)\n",
      "(408, 415, 411) (0, 0, 2)\n",
      "(150, 148, 149) (0, 0, 4)\n",
      "(85, 87, 83) (4, 4, 1)\n",
      "(433, 435, 434) (0, 0, 1)\n",
      "(166, 162, 160) (0, 0, 2)\n",
      "(32, 33, 37) (0, 0, 1)\n",
      "(76, 77, 74) (0, 0, 2)\n",
      "(217, 218, 220) (0, 0, 4)\n",
      "(28, 24, 27) (0, 0, 4)\n",
      "(222, 216, 219) (0, 0, 5)\n",
      "(435, 436, 434) (0, 0, 1)\n",
      "(64, 71, 68) (0, 0, 1)\n",
      "(185, 189, 184) (0, 0, 3)\n",
      "(484, 486, 482) (0, 0, 4)\n",
      "(213, 209, 211) (0, 0, 4)\n",
      "(193, 194, 199) (0, 0, 4)\n",
      "(323, 321, 322) (0, 0, 1)\n",
      "(442, 444, 440) (0, 0, 3)\n",
      "(141, 137, 136) (0, 0, 1)\n",
      "(191, 188, 184) (0, 0, 3)\n",
      "(33, 35, 37) (0, 0, 1)\n",
      "(26, 30, 29) (0, 0, 4)\n",
      "(324, 326, 325) (0, 0, 2)\n",
      "(469, 466, 465) (0, 0, 4)\n",
      "(233, 236, 234) (0, 0, 2)\n",
      "(148, 151, 149) (0, 0, 4)\n",
      "(16, 17, 20) (0, 0, 3)\n",
      "(435, 433, 434) (0, 0, 1)\n",
      "(128, 131, 132) (2, 2, 0)\n",
      "(311, 306, 304) (0, 0, 2)\n",
      "(209, 210, 208) (0, 0, 4)\n",
      "(437, 432, 434) (0, 0, 1)\n",
      "(4, 6, 1) (0, 0, 1)\n",
      "(258, 257, 256) (0, 0, 3)\n",
      "(268, 266, 267) (0, 0, 2)\n",
      "(159, 154, 157) (0, 0, 2)\n",
      "(283, 284, 285) (0, 0, 5)\n",
      "(436, 433, 439) (0, 0, 4)\n",
      "(201, 203, 206) (0, 0, 2)\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(triplets)\n",
    "for j in triplets[:300]:\n",
    "    print(j, (classes[j[0]], classes[j[1]], classes[j[2]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 0, loss: 0.0, windowed_loss: 50\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 1, loss: 0.0, windowed_loss: 50\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 2, loss: 0.0, windowed_loss: 50\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 3, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 4, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 5, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 6, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 7, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 8, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 9, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 10, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 11, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 12, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 13, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 14, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 15, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 16, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00017: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 00017: reducing learning rate of group 0 to 9.0000e-03.\n",
      "Epoch 00017: reducing learning rate of group 0 to 9.0000e-03.\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 17, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 18, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 19, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 20, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 21, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 22, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 23, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 24, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 25, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 26, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 27, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 28, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 29, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 30, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 31, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.009000000000000001, 0.009000000000000001, 0.009000000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 32, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00033: reducing learning rate of group 0 to 8.1000e-03.\n",
      "Epoch 00033: reducing learning rate of group 0 to 8.1000e-03.\n",
      "Epoch 00033: reducing learning rate of group 0 to 8.1000e-03.\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 33, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 34, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 35, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 36, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 37, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 38, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 39, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 40, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 41, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 42, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 43, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 44, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 45, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 46, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 47, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.008100000000000001, 0.008100000000000001, 0.008100000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 48, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00049: reducing learning rate of group 0 to 7.2900e-03.\n",
      "Epoch 00049: reducing learning rate of group 0 to 7.2900e-03.\n",
      "Epoch 00049: reducing learning rate of group 0 to 7.2900e-03.\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 49, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 50, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 51, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 52, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 53, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 54, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 55, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 56, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 57, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 58, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 59, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 60, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 61, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 62, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 63, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.007290000000000001, 0.007290000000000001, 0.007290000000000001]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 64, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00065: reducing learning rate of group 0 to 6.5610e-03.\n",
      "Epoch 00065: reducing learning rate of group 0 to 6.5610e-03.\n",
      "Epoch 00065: reducing learning rate of group 0 to 6.5610e-03.\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 65, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 66, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 67, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 68, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 69, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 70, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 71, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 72, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 73, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 74, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 75, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 76, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 77, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 78, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 79, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.006561000000000002, 0.006561000000000002, 0.006561000000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 80, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00081: reducing learning rate of group 0 to 5.9049e-03.\n",
      "Epoch 00081: reducing learning rate of group 0 to 5.9049e-03.\n",
      "Epoch 00081: reducing learning rate of group 0 to 5.9049e-03.\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 81, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 82, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 83, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 84, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 85, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 86, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 87, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 88, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 89, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 90, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 91, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 92, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 93, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 94, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 95, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005904900000000002, 0.005904900000000002, 0.005904900000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 96, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00097: reducing learning rate of group 0 to 5.3144e-03.\n",
      "Epoch 00097: reducing learning rate of group 0 to 5.3144e-03.\n",
      "Epoch 00097: reducing learning rate of group 0 to 5.3144e-03.\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 97, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 98, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 99, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 100, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 101, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 102, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 103, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 104, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 105, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 106, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 107, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 108, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 109, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 110, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 111, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.005314410000000002, 0.005314410000000002, 0.005314410000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 112, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00113: reducing learning rate of group 0 to 4.7830e-03.\n",
      "Epoch 00113: reducing learning rate of group 0 to 4.7830e-03.\n",
      "Epoch 00113: reducing learning rate of group 0 to 4.7830e-03.\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 113, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 114, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 115, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 116, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 117, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 118, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 119, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 120, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 121, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 122, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 123, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 124, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 125, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 126, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 127, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004782969000000002, 0.004782969000000002, 0.004782969000000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 128, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00129: reducing learning rate of group 0 to 4.3047e-03.\n",
      "Epoch 00129: reducing learning rate of group 0 to 4.3047e-03.\n",
      "Epoch 00129: reducing learning rate of group 0 to 4.3047e-03.\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 129, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 130, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 131, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 132, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 133, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 134, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 135, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 136, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 137, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 138, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 139, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 140, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 141, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 142, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 143, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.004304672100000002, 0.004304672100000002, 0.004304672100000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 144, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00145: reducing learning rate of group 0 to 3.8742e-03.\n",
      "Epoch 00145: reducing learning rate of group 0 to 3.8742e-03.\n",
      "Epoch 00145: reducing learning rate of group 0 to 3.8742e-03.\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 145, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 146, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 147, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 148, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 149, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 150, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 151, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 152, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 153, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 154, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 155, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 156, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 157, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 158, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 159, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003874204890000002, 0.003874204890000002, 0.003874204890000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 160, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00161: reducing learning rate of group 0 to 3.4868e-03.\n",
      "Epoch 00161: reducing learning rate of group 0 to 3.4868e-03.\n",
      "Epoch 00161: reducing learning rate of group 0 to 3.4868e-03.\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 161, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 162, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 163, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 164, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 165, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 166, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 167, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 168, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 169, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 170, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 171, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 172, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 173, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 174, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 175, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003486784401000002, 0.003486784401000002, 0.003486784401000002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 176, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00177: reducing learning rate of group 0 to 3.1381e-03.\n",
      "Epoch 00177: reducing learning rate of group 0 to 3.1381e-03.\n",
      "Epoch 00177: reducing learning rate of group 0 to 3.1381e-03.\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 177, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 178, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 179, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 180, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 181, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 182, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 183, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 184, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 185, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 186, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 187, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 188, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 189, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 190, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 191, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.003138105960900002, 0.003138105960900002, 0.003138105960900002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 192, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00193: reducing learning rate of group 0 to 2.8243e-03.\n",
      "Epoch 00193: reducing learning rate of group 0 to 2.8243e-03.\n",
      "Epoch 00193: reducing learning rate of group 0 to 2.8243e-03.\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 193, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 194, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 195, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 196, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 197, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 198, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 199, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 200, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 201, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 202, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 203, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 204, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 205, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 206, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 207, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0028242953648100018, 0.0028242953648100018, 0.0028242953648100018]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 208, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00209: reducing learning rate of group 0 to 2.5419e-03.\n",
      "Epoch 00209: reducing learning rate of group 0 to 2.5419e-03.\n",
      "Epoch 00209: reducing learning rate of group 0 to 2.5419e-03.\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 209, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 210, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 211, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 212, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 213, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 214, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 215, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 216, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 217, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 218, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 219, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 220, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 221, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 222, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 223, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0025418658283290017, 0.0025418658283290017, 0.0025418658283290017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 224, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00225: reducing learning rate of group 0 to 2.2877e-03.\n",
      "Epoch 00225: reducing learning rate of group 0 to 2.2877e-03.\n",
      "Epoch 00225: reducing learning rate of group 0 to 2.2877e-03.\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 225, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 226, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 227, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 228, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 229, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 230, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 231, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 232, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 233, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 234, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 235, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 236, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 237, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 238, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 239, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0022876792454961017, 0.0022876792454961017, 0.0022876792454961017]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 240, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00241: reducing learning rate of group 0 to 2.0589e-03.\n",
      "Epoch 00241: reducing learning rate of group 0 to 2.0589e-03.\n",
      "Epoch 00241: reducing learning rate of group 0 to 2.0589e-03.\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 241, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 242, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 243, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 244, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 245, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 246, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 247, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 248, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 249, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 250, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 251, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 252, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 253, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 254, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 255, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.0020589113209464917, 0.0020589113209464917, 0.0020589113209464917]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 256, loss: 0.0, windowed_loss: 0.0\n",
      "Epoch 00257: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 00257: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 00257: reducing learning rate of group 0 to 2.0000e-03.\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 257, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 258, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 259, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 260, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 261, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 262, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 263, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 264, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 265, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 266, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 267, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 268, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 269, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 270, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 271, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 272, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 273, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 274, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 275, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 276, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 277, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 278, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 279, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 280, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 281, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 282, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 283, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 284, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 285, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 286, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 287, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 288, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 289, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 290, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 291, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 292, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 293, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 294, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 295, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 296, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 297, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 298, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 299, loss: 0.0, windowed_loss: 0.0\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Total Pre-training Time: 0.14947819709777832\n"
     ]
    }
   ],
   "source": [
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "ensemble = Ensemble(size=3, output_size=5, lr=1e-2, weight_decay=0, new_model=True, dynamic_lr=True, manual_schedulers=True, init=\"Random\")\n",
    "if ENSEMBLE_PATH is not None:\n",
    "    ensemble.load_ensemble(ENSEMBLE_PATH, full=True)\n",
    "sampled_dataset = DATASET\n",
    "\n",
    "def pretraining(data, ensemble, data_cutoff=None, data_size=500):\n",
    "    if data_cutoff is None:\n",
    "        data_cutoff = len(data) - 1\n",
    "    # np.random.seed(0)\n",
    "    random.shuffle(triplets)\n",
    "    samples = triplets[:data_size]\n",
    "    total_loss = np.array([0.0 for i in range(len(ensemble.ensemble))])\n",
    "\n",
    "    BATCH_SIZE = 4096\n",
    "    total_updates = 0\n",
    "    total_batches = max(len(samples), data_size) // BATCH_SIZE\n",
    "\n",
    "    # Batch the data\n",
    "    for i in range(0, len(samples), BATCH_SIZE):\n",
    "        # AUGMENT_SIZE = 1\n",
    "        if i + (BATCH_SIZE) >= len(samples):\n",
    "            continue\n",
    "\n",
    "        print(f\"Unsupervised Training.. {(total_updates * 100) / total_batches}\")\n",
    "\n",
    "        temp_losses = np.array([0.0 for _ in ensemble.ensemble])\n",
    "\n",
    "        anchors = np.array([data[samples[i + j][0]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        pretraining = random.random() < 0.6\n",
    "        if pretraining:\n",
    "            positives = np.array(\n",
    "                [\n",
    "                    getRandomTransformation(data[samples[i + j][0]][0]) for j in range(BATCH_SIZE)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            positives = np.array(\n",
    "                [\n",
    "                    getRandomFlip(data[samples[i + j][1]][0]) for j in range(BATCH_SIZE)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        negatives = np.array([data[samples[i + j][2]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        anchors = np.expand_dims(anchors, axis=1)\n",
    "        positives = np.expand_dims(positives, axis=1)\n",
    "        negatives = np.expand_dims(negatives, axis=1)\n",
    "\n",
    "        losses = ensemble.train_batch(anchors, positives, negatives)\n",
    "        temp_losses += losses\n",
    "\n",
    "        total_loss += temp_losses\n",
    "        total_updates += 1\n",
    "\n",
    "    return total_loss, max(total_updates, 1)\n",
    "\n",
    "t_1 = time.time()\n",
    "epochs = 0\n",
    "loss_history = []\n",
    "while epochs < 300:\n",
    "    losses, total_updates = pretraining(sampled_dataset, ensemble, data_cutoff=9999, data_size=(4096 * 12))\n",
    "    average_loss = losses / total_updates\n",
    "    locale_loss = sum(average_loss) / len(average_loss)\n",
    "    loss_history.append(locale_loss)\n",
    "    loss = (sum(loss_history[-3:]) / 3) if len(loss_history) > 3 else 50\n",
    "    print(f\"Losses: {average_loss}\")\n",
    "    print(f\"Epoch {epochs}, loss: {locale_loss}, windowed_loss: {loss}\")\n",
    "    epochs += 1\n",
    "    ensemble.step_schedulers(losses)\n",
    "    print(f\"LR: {ensemble.get_lr()}\")\n",
    "\n",
    "print(f\"Total Pre-training Time: {time.time() - t_1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(f\"../checkpoints/ensembles/{int(time.time())}\", full=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
