{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embed into Latent Space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from generation.evolution import ModifiedHaltingEvolution\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.AngularMomentum import AngularMomentumBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.AverageSpeed import AverageSpeedBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.GroupRotationBehavior import GroupRotationBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.RadialVariance import RadialVarianceBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.ScatterBehavior import ScatterBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.SensorOffset import GeneElementDifference\n",
    "from NovelSwarmBehavior.novel_swarms.config.AgentConfig import DiffDriveAgentConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.EvolutionaryConfig import GeneticEvolutionConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.OutputTensorConfig import OutputTensorConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.WorldConfig import RectangularWorldConfig\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.GeneRule import GeneRule\n",
    "from NovelSwarmBehavior.novel_swarms.sensors.GenomeDependentSensor import GenomeBinarySensor\n",
    "from NovelSwarmBehavior.novel_swarms.sensors.SensorSet import SensorSet\n",
    "from generation.evolution import ModifiedHaltingEvolution, ModifiedNoveltyArchieve\n",
    "from generation.halted_evolution import HaltedEvolution\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "import numpy as np\n",
    "import pygame\n",
    "import cv2\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
    "\n",
    "def evolution_controller():\n",
    "    sensors = SensorSet([\n",
    "        GenomeBinarySensor(genome_id=8, draw=False),\n",
    "        GenomeBinarySensor(genome_id=9, draw=False)\n",
    "    ])\n",
    "\n",
    "    agent_config = DiffDriveAgentConfig(\n",
    "        sensors=sensors,\n",
    "        seed=None,\n",
    "    )\n",
    "\n",
    "    genotype = [\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=((1/3) * np.pi), _min=-((2/3) * np.pi), mutation_step=(np.pi/2), round_digits=4),\n",
    "        GeneRule(_max=((2/3) * np.pi), _min=-((1/3) * np.pi), mutation_step=(np.pi/2), round_digits=4),\n",
    "    ]\n",
    "\n",
    "    phenotype = [\n",
    "        AverageSpeedBehavior(),\n",
    "        AngularMomentumBehavior(),\n",
    "        RadialVarianceBehavior(),\n",
    "        ScatterBehavior(),\n",
    "        GroupRotationBehavior(),\n",
    "        # GeneElementDifference(8, 9)\n",
    "    ]\n",
    "\n",
    "    world_config = RectangularWorldConfig(\n",
    "        size=(500, 500),\n",
    "        n_agents=24,\n",
    "        seed=None,\n",
    "        behavior=phenotype,\n",
    "        agentConfig=agent_config,\n",
    "        padding=15\n",
    "    )\n",
    "\n",
    "    novelty_config = GeneticEvolutionConfig(\n",
    "        gene_rules=genotype,\n",
    "        phenotype_config=phenotype,\n",
    "        n_generations=50,\n",
    "        n_population=100,\n",
    "        crossover_rate=0.7,\n",
    "        mutation_rate=0.15,\n",
    "        world_config=world_config,\n",
    "        k_nn=15,\n",
    "        simulation_lifespan=2500,\n",
    "        display_novelty=True,\n",
    "        save_archive=True,\n",
    "        show_gui=True,\n",
    "    )\n",
    "\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"Evolutionary Novelty Search\")\n",
    "    screen = pygame.display.set_mode((world_config.w, world_config.h))\n",
    "\n",
    "    output_config = OutputTensorConfig(\n",
    "        timeless=True,\n",
    "        total_frames=80,\n",
    "        steps_between_frames=2,\n",
    "        screen=screen\n",
    "    )\n",
    "\n",
    "    halted_evolution = ModifiedHaltingEvolution(\n",
    "        world=world_config,\n",
    "        evolution_config=novelty_config,\n",
    "        output_config=output_config\n",
    "    )\n",
    "\n",
    "    return halted_evolution, screen\n",
    "\n",
    "def build_random_dataset():\n",
    "    halted_evolution, screen = evolution_controller()\n",
    "    baseline_data = DataBuilder(\"data/full-dual-sensors\", steps=1200, agents=30, ev=halted_evolution, screen=screen)\n",
    "    baseline_data.create()\n",
    "    baseline_data.evolution.close()\n",
    "\n",
    "def resizeInput(X, w=200):\n",
    "    frame = X.astype(np.uint8)\n",
    "    resized = cv2.resize(frame, dsize=(w, w), interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def translate(img, offset=(10, 10)):\n",
    "    h, w = img.shape\n",
    "    xoff, yoff = offset\n",
    "    if xoff < 0: xpadding = (0, -xoff)\n",
    "    else: xpadding = (xoff, 0)\n",
    "    if yoff < 0: ypadding = (0, -yoff)\n",
    "    else: ypadding = (yoff, 0)\n",
    "    img = np.pad(img, (xpadding, ypadding))\n",
    "\n",
    "    if xoff >= 0 and yoff >= 0:\n",
    "        return img[:w, :w]\n",
    "    elif xoff < 0 and yoff >= 0:\n",
    "        return img[-w:, :w]\n",
    "    elif xoff >= 0 and yoff < 0:\n",
    "        return img[:w, -w:]\n",
    "    return img[-w:, -w:]\n",
    "\n",
    "def zoom_at(img, zoom, coord=None):\n",
    "    # Adapted from https://stackoverflow.com/questions/69050464/zoom-into-image-with-opencv\n",
    "    h, w = [ zoom * i for i in img.shape ]\n",
    "    if coord is None: cx, cy = w/2, h/2\n",
    "    else: cx, cy = [ zoom*c for c in coord ]\n",
    "    img = cv2.resize( img, (0, 0), fx=zoom, fy=zoom)\n",
    "    img = img[ int(round(cy - h/zoom * .5)) : int(round(cy + h/zoom * .5)),\n",
    "               int(round(cx - w/zoom * .5)) : int(round(cx + w/zoom * .5))]\n",
    "    return img\n",
    "\n",
    "def get_color_distortion(X, s=3.0):\n",
    "    X = X + s * np.random.randn(X.shape[0], X.shape[1])\n",
    "    return X\n",
    "\n",
    "def getRandomTransformation(image, k=2):\n",
    "    transformation_choices = [\"Rotation\", \"Blur\", \"Zoom\", \"Translate\", \"Distort\", \"ResizedCrop\"]\n",
    "    # weights = [0.4, 0.3, 0.0, 0.2]\n",
    "    # weights = [1.0, 0.0, 0.0, 0.0]\n",
    "    # choices = random.choices(transformation_choices, weights, k=k)\n",
    "    choices = [\"ResizedCrop\", \"Rotation\"]\n",
    "    if \"ResizedCrop\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(0.5)\n",
    "        cropper = RandomResizedCrop(size=(50,50), scale=(0.6, 1.0), ratio=(1.0, 1.0))\n",
    "        image = flipper(cropper(tmp))\n",
    "        image = image.squeeze(0).numpy()\n",
    "    if \"Rotation\" in choices:\n",
    "        theta = random.choice([90, 180, 270])\n",
    "        image = ndimage.rotate(image, theta)\n",
    "    if \"Blur\" in choices:\n",
    "        blur = random.choice([0.5, 1.0, 1.5])\n",
    "        image = ndimage.gaussian_filter(image, sigma=blur)\n",
    "    if \"Zoom\" in choices:\n",
    "        # zoom = random.choice([1.06, 1.12, 1.18])\n",
    "        padding = random.choice([10])\n",
    "        padded = np.pad(image, padding, mode='constant')\n",
    "        image = resizeInput(padded, 50)\n",
    "    if \"Translate\" in choices:\n",
    "        offsets = [i for i in range(-10, 10, 2)]\n",
    "        offset = (random.choice(offsets), random.choice(offsets))\n",
    "        # offset = (2, 2)\n",
    "        image = translate(image, offset)\n",
    "    if \"Distort\" in choices:\n",
    "        strength = random.choice([3.0, 5.0, 10.0])\n",
    "        image = get_color_distortion(image, s=strength)\n",
    "    if \"Flip\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(1.0)\n",
    "        image = flipper(tmp)\n",
    "        image = image.squeeze(0).numpy()\n",
    "    return image\n",
    "\n",
    "def train(ensemble, anchor_img, pos_img, neg_img):\n",
    "    pos_images = np.stack([\n",
    "        [ndimage.rotate(pos_img, 90)],\n",
    "        [ndimage.rotate(pos_img, 180)],\n",
    "        [ndimage.rotate(pos_img, 270)],\n",
    "    ])\n",
    "\n",
    "    anchor_images = np.stack([[anchor_img] for _ in pos_images])\n",
    "    neg_images = np.stack([[neg_img] for _ in pos_images])\n",
    "    losses = ensemble.train_batch(anchor_images, pos_images, neg_images)\n",
    "    return np.array(losses)\n",
    "\n",
    "def evaluate(ensemble, anchor_img, pos_img, neg_img):\n",
    "    loss = ensemble.eval_triplet(anchor_img, pos_img, neg_img)\n",
    "    return loss\n",
    "\n",
    "def human_in_the_loop(anchor_dataset, network, optimizer, loss_fn, HIL_archive, random_archive, stop_at):\n",
    "    print(\"HIL TIME!\")\n",
    "    improvements, human_loss, triplet_helpfulness, embedded_archive = hil.humanInput(anchor_dataset, network, optimizer, loss_fn, HIL_archive, random_archive, stop_at)\n",
    "    print(f\"Improvement Count: {improvements}, loss: {human_loss}\")\n",
    "\n",
    "    HIL_archive.save_to_file(f\"data/queries/{trial_name}_hil.csv\")\n",
    "    random_archive.save_to_file(f\"data/queries/{trial_name}_rand.csv\")\n",
    "    return improvements, human_loss, triplet_helpfulness, embedded_archive\n",
    "\n",
    "def pretraining(data, ensemble, data_cutoff=None, data_size=500):\n",
    "    BATCH_SIZE = 4096\n",
    "    REPEATS = 1\n",
    "    if data_cutoff is None:\n",
    "        data_cutoff = len(data) - 1\n",
    "    # np.random.seed(0)\n",
    "    samples = np.random.random_integers(0, data_cutoff, (data_size, 2))\n",
    "    total_loss = np.array([0.0 for i in range(len(ensemble.ensemble))])\n",
    "    total_updates = 0\n",
    "    pull_set = [k for k in range(len(samples))]\n",
    "    random.shuffle(pull_set)\n",
    "\n",
    "    for r in range(REPEATS):\n",
    "        for index in range(0, len(pull_set), BATCH_SIZE):\n",
    "            i = pull_set[index]\n",
    "            if total_updates % 20 == 0:\n",
    "                print(f\"Unsupervised Training.. {(total_updates * BATCH_SIZE * 100) / data_size}\")\n",
    "\n",
    "            AUGMENT_SIZE = 1\n",
    "            if i + (BATCH_SIZE * AUGMENT_SIZE) >= len(pull_set):\n",
    "                continue\n",
    "\n",
    "            temp_losses = np.array([0.0 for _ in ensemble.ensemble])\n",
    "\n",
    "            anchors = np.array([data[samples[i + (j % AUGMENT_SIZE)][0]][0] for j in range(AUGMENT_SIZE * BATCH_SIZE)])\n",
    "            positives = np.array([getRandomTransformation(data[samples[i + (j % AUGMENT_SIZE)][0]][0], k=2) for j in range(AUGMENT_SIZE * BATCH_SIZE)])\n",
    "            negatives = np.array([data[samples[i + (j % AUGMENT_SIZE)][1]][0] for j in range(AUGMENT_SIZE * BATCH_SIZE)])\n",
    "\n",
    "            anchors = np.expand_dims(anchors, axis=1)\n",
    "            positives = np.expand_dims(positives, axis=1)\n",
    "            negatives = np.expand_dims(negatives, axis=1)\n",
    "\n",
    "            losses = ensemble.train_batch(anchors, positives, negatives)\n",
    "            temp_losses += losses\n",
    "\n",
    "            total_loss += temp_losses\n",
    "            total_updates += 1\n",
    "\n",
    "    return total_loss, max(total_updates, 1)\n",
    "\n",
    "# Save Model\n",
    "import torch\n",
    "import time\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from networks.ensemble import Ensemble\n",
    "\n",
    "PRETRAINING = True\n",
    "target = 0.01\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ensemble = Ensemble(size=3, output_size=5, lr=0.08, learning_decay=0.95, decay_step=2, threshold=19, margin=20, new_model=True, init=None)\n",
    "# ensemble.load_ensemble(\"dual-mini-A\")\n",
    "sampled_dataset = SwarmDataset(\"data/full-mini-dual\", rank=0)\n",
    "\n",
    "t_1 = time.time()\n",
    "losses = []\n",
    "WINDOW_SIZE = 5\n",
    "if PRETRAINING:\n",
    "    epochs = 0\n",
    "    while loss > target:\n",
    "        loss, total_updates = pretraining(sampled_dataset, ensemble, data_cutoff=2000, data_size=4096 * 2)\n",
    "        average_loss = loss / total_updates\n",
    "        print(f\"Epoch {epochs}, loss: {average_loss}, val: {sum(average_loss) / len(average_loss)}\")\n",
    "        epochs += 1\n",
    "        lr = ensemble.evaluate_lr(average_loss)\n",
    "        loss = sum(average_loss) / len(average_loss)\n",
    "        losses.append(loss)\n",
    "        loss = sum(losses[-WINDOW_SIZE:]) / WINDOW_SIZE if len(losses) > WINDOW_SIZE else 50\n",
    "        print(f\"LR: {lr}\")\n",
    "        print(f\"Windowed Loss: {loss}\")\n",
    "\n",
    "print(f\"Total Pre-training Time: {time.time() - t_1}\")\n",
    "ensemble.save_ensemble(f\"{int(time.time())}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(f\"{int(time.time())}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyze and Sample from the ensamble\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from networks.ensemble import Ensemble\n",
    "from matplotlib import pyplot as plot\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "\n",
    "ensemble = Ensemble(size=5, output_size=5, lr=5e-4, learning_decay=0.8, decay_step=4)\n",
    "ensemble.load_ensemble(\"surgery_ensemble\")\n",
    "sampled_dataset = SwarmDataset(\"data/full-dual-sensors\", rank=0)\n",
    "parent_folder = \"/home/connor/Desktop/SwarmsResults/AugmentedTests/Encoder_plus_original_bvec_Nov_7\"\n",
    "sub_folder = \"data\"\n",
    "retrieved_data = ContinuingDataset(directory=parent_folder, create=False, folder_name=sub_folder)\n",
    "retrieved_data = sampled_dataset\n",
    "\n",
    "archive = NoveltyArchive(\n",
    "    pheno_file=os.path.join(\"/home/connor/Desktop/SwarmsResults/AugmentedTests/Encoder_plus_original_bvec_Nov_7/behaviors\", \"1667764573_b__1667801491.csv\"),\n",
    "    geno_file=os.path.join(\"/home/connor/Desktop/SwarmsResults/AugmentedTests/Encoder_plus_original_bvec_Nov_7/controllers\", \"1667764573_g__1667801491.csv\"),\n",
    "    absolute=True\n",
    ")\n",
    "\n",
    "TRIALS = 1000\n",
    "rand_samp = [\n",
    "    (random.randrange(0, len(retrieved_data)), random.randrange(0, len(retrieved_data)), random.randrange(0, len(retrieved_data))) for i in range(TRIALS)\n",
    "]\n",
    "print(\"Samples Collected\")\n",
    "\n",
    "entropy = []\n",
    "for i in range(TRIALS):\n",
    "    # h = ensemble.entropy_agreement(retrieved_data[rand_samp[i][0]][0], positive=None, negative=retrieved_data[rand_samp[i][2]][0])\n",
    "    h = ensemble.entropy_agreement(retrieved_data[rand_samp[i][0]][0], positive=retrieved_data[rand_samp[i][1]][0], negative=retrieved_data[rand_samp[i][2]][0])\n",
    "    print(h)\n",
    "    entropy.append(h)\n",
    "print(\"Entropy Calculated\")\n",
    "\n",
    "agreement = [0, 0]\n",
    "for i in range(TRIALS):\n",
    "    # agree = ensemble.binary_agreement(retrieved_data[rand_samp[i][0]][0], positive=None,negative=retrieved_data[rand_samp[i][2]][0])\n",
    "    agree = ensemble.binary_agreement(retrieved_data[rand_samp[i][0]][0], positive=retrieved_data[rand_samp[i][1]][0],negative=retrieved_data[rand_samp[i][2]][0])\n",
    "    if agree:\n",
    "        agreement[1] += 1\n",
    "        continue\n",
    "    agreement[0] += 1\n",
    "print(\"Aggreement Calculated\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p = plot.hist(entropy)\n",
    "plot.title(\"Ensemble Agreement Over 1000 Triplets From Validation Dataset\")\n",
    "plot.xlabel(\"Entropy\")\n",
    "plot.ylabel(\"Number of Triplets\")\n",
    "plot.show()\n",
    "print(p)\n",
    "\n",
    "plot.bar([0, 1], agreement, color=[(1, 0, 0), (0, 0, 1)])\n",
    "plot.title(\"Ensemble Agreement Over 1000 Triplets From Validation Dataset\")\n",
    "plot.xlabel(\"Agreement\")\n",
    "plot.xticks([0, 1], [f\"Disagree ({agreement[0]})\", f\"Agree ({agreement[1]})\"])\n",
    "plot.ylabel(\"Number of Triplets\")\n",
    "plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample from Random Triplets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "entropy_i = [(e, i) for i, e in enumerate(entropy)]\n",
    "entropy_i.sort()\n",
    "\n",
    "for j in range(10):\n",
    "    e, i = entropy_i[j]\n",
    "    print(f\"{e}, {i}\")\n",
    "    fig, (ax1, ax2, ax3) = plot.subplots(1, 3)\n",
    "    ax1.imshow(retrieved_data[rand_samp[i][0]][0], cmap='Greys')\n",
    "    ax1.set_title(\"Anchor\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    # ax2.imshow(ndimage.rotate(retrieved_data[rand_samp[i][0]][0], 90), cmap='Greys')\n",
    "    ax2.imshow(retrieved_data[rand_samp[i][1]][0], cmap='Greys')\n",
    "    ax2.set_title(\"Positive\")\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax3.imshow(retrieved_data[rand_samp[i][2]][0], cmap='Greys')\n",
    "    ax3.set_title(\"Negative\")\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_yticks([])\n",
    "    plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform Evaluations of Medoid Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from data.msampler import ClusterSampler\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from networks.ensemble import Ensemble\n",
    "from matplotlib import pyplot as plot\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ensemble = Ensemble(size=5, output_size=5, lr=5e-4, learning_decay=0.8, decay_step=4, margin=10.0)\n",
    "# ensemble.load_ensemble(\"surgery_ensemble\")\n",
    "ensemble.load_ensemble(\"epoch4-revised-5\")\n",
    "sampled_dataset = SwarmDataset(\"data/full-dual-sensors\", rank=0)\n",
    "parent_folder = \"/home/connor/Desktop/SwarmsResults/AugmentedTests/Encoder_plus_original_bvec_Nov_7\"\n",
    "sub_folder = \"data\"\n",
    "# retrieved_data = ContinuingDataset(directory=parent_folder, create=False, folder_name=sub_folder)\n",
    "retrieved_data = sampled_dataset\n",
    "\n",
    "archive = NoveltyArchive(\n",
    "    pheno_file=os.path.join(\"/home/connor/Desktop/SwarmsResults/AugmentedTests/Encoder_plus_original_bvec_Nov_7/behaviors\", \"1667764573_b__1667801491.csv\"),\n",
    "    geno_file=os.path.join(\"/home/connor/Desktop/SwarmsResults/AugmentedTests/Encoder_plus_original_bvec_Nov_7/controllers\", \"1667764573_g__1667801491.csv\"),\n",
    "    absolute=True\n",
    ")\n",
    "\n",
    "archiveA = NoveltyArchive()\n",
    "network = ensemble.ensemble[0]\n",
    "network.eval()\n",
    "for i in range(len(retrieved_data)):\n",
    "    inp = torch.from_numpy(retrieved_data[i][0]).unsqueeze(0).to(device).float()\n",
    "    e = network(inp).cpu().detach().numpy().squeeze(0)\n",
    "    archiveA.addToArchive(e, retrieved_data[i][1])\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Transferring to Archive... {i}, {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster_samples = ClusterSampler(archive=archiveA, k=10)\n",
    "triplets = cluster_samples.get_type_a_set()\n",
    "print(triplets)\n",
    "\n",
    "agreement = [0 for i in range(len(ensemble.ensemble) + 1)]\n",
    "entropy = []\n",
    "# to_query = []\n",
    "for triplet in triplets:\n",
    "    n = ensemble.num_networks_correct(retrieved_data[triplet[0]][0], positive=retrieved_data[triplet[1]][0], negative=retrieved_data[triplet[2]][0])\n",
    "    e = ensemble.entropy_agreement(retrieved_data[triplet[0]][0], positive=retrieved_data[triplet[1]][0], negative=retrieved_data[triplet[2]][0])\n",
    "    entropy.append(e)\n",
    "\n",
    "    # if n < 2:\n",
    "    #     print(f\"Score {n} indexed to {len(to_query)}\")\n",
    "    #     to_query.append(triplet)\n",
    "\n",
    "\n",
    "    print(n, e)\n",
    "    agreement[n] += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot.hist(entropy)\n",
    "plot.title(\"Type A Ensemble Entropy over 10 Triplets\")\n",
    "plot.xlabel(\"Entropy\")\n",
    "plot.ylabel(\"Number of Triplets\")\n",
    "plot.show()\n",
    "\n",
    "plot.bar([i for i in range(len(ensemble.ensemble) + 1)], agreement, color=[(0.8, 0, 0), (0.9, 0, 0), (1.0, 0, 0), (0, 0, 1.0), (0, 0, 0.9), (0, 0, 0.8)])\n",
    "plot.title(\"Type A Ensemble Agreement\")\n",
    "plot.xlabel(\"Number of Ensemble Members that agree with Triplet\")\n",
    "plot.xticks([i for i in range(len(ensemble.ensemble) + 1)], [i for i in range(len(ensemble.ensemble) + 1)])\n",
    "plot.ylabel(\"Number of Type A Triplets\")\n",
    "plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieve Samples of High Uncertainty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for triplet in to_query:\n",
    "    fig, (ax1, ax2, ax3) = plot.subplots(1, 3)\n",
    "    ax1.imshow(retrieved_data[triplet[0]][0], cmap='Greys')\n",
    "    ax1.set_title(\"Anchor\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax2.imshow(retrieved_data[triplet[1]][0], cmap='Greys')\n",
    "    ax2.set_title(\"Positive\")\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax3.imshow(retrieved_data[triplet[2]][0], cmap='Greys')\n",
    "    ax3.set_title(\"Negative\")\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_yticks([])\n",
    "    plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test HIL Triplets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HIL Triplets provided from the user\n",
    "triplet_classes = [\n",
    "    [(0, 0), (0, 1), (2, 0), (2, 1), (3, 2)],\n",
    "    [(0, 2), (1, 0), (1, 1), (1, 2), (2, 2)],\n",
    "    [(3, 0)],\n",
    "    [(3, 1)]\n",
    "]\n",
    "\n",
    "permute_triplets = []\n",
    "rotate_triplets = []\n",
    "\n",
    "for i in range(len(triplet_classes)):\n",
    "    for j in range(len(triplet_classes[i])):\n",
    "        anchor = triplet_classes[i][j]\n",
    "        for k in range(j + 1, len(triplet_classes[i])):\n",
    "            positive = triplet_classes[i][k]\n",
    "            for l in range(i + 1, len(triplet_classes)):\n",
    "                for m in range(len(triplet_classes[l])):\n",
    "                    negative = triplet_classes[l][m]\n",
    "                    permute_triplets.append((anchor, positive, negative))\n",
    "\n",
    "for i in range(len(triplet_classes)):\n",
    "    for j in range(len(triplet_classes[i])):\n",
    "        anchor = triplet_classes[i][j]\n",
    "        for l in range(i + 1, len(triplet_classes)):\n",
    "            for m in range(len(triplet_classes[l])):\n",
    "                negative = triplet_classes[l][m]\n",
    "                rotate_triplets.append((anchor, negative))\n",
    "\n",
    "sampled_dataset = SwarmDataset(\"data/full-dual-sensors\", rank=0)\n",
    "ensemble = Ensemble(size=5, output_size=5, lr=5e-4, learning_decay=0.9, decay_step=3)\n",
    "ensemble.load_ensemble(\"epoch4-revised-4\")\n",
    "ensemble.training_mode()\n",
    "\n",
    "TARGET = 0.08\n",
    "average_loss = 1000\n",
    "epoch = 0\n",
    "# Training\n",
    "while average_loss > TARGET:\n",
    "    losses = [0.0 for _ in range(len(ensemble.ensemble))]\n",
    "    total = 0\n",
    "\n",
    "    for anchor, positive, negative in permute_triplets:\n",
    "        a, p, n = to_query[anchor[0]][anchor[1]], to_query[positive[0]][positive[1]], to_query[negative[0]][negative[1]]\n",
    "        loss = train(ensemble, sampled_dataset[a][0], sampled_dataset[p][0], sampled_dataset[n][0])\n",
    "        losses += loss\n",
    "        total += 1\n",
    "        if total % 20 == 0:\n",
    "            print(f\"Permute Progress... {total}\")\n",
    "\n",
    "    print(\"Permuation Triplets Complete\")\n",
    "\n",
    "    for anchor, negative in rotate_triplets:\n",
    "        a, n = to_query[anchor[0]][anchor[1]], to_query[negative[0]][negative[1]]\n",
    "        loss = train(ensemble, sampled_dataset[a][0], sampled_dataset[a][0], sampled_dataset[n][0])\n",
    "        losses += loss\n",
    "        total += 1\n",
    "        if total % 20 == 0:\n",
    "            print(f\"Rotation Progress... {total}\")\n",
    "\n",
    "    average_loss = sum(losses / total) / len(losses)\n",
    "\n",
    "    print(\"Rotation Triplets Complete\")\n",
    "    print(f\"Epoch: {epoch}, average loss: {average_loss}, losses: {losses / total}\")\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        losses = pretraining(sampled_dataset, ensemble, data_cutoff=None, data_size=500)\n",
    "        average_loss += (sum(losses) / len(losses))\n",
    "        print(f\"Average: {(sum(losses) / len(losses))}, Pretraining Loss: {losses}\")\n",
    "        print(f\"Combined Average Loss: {average_loss}\")\n",
    "\n",
    "    ensemble.step_schedulers()\n",
    "    epoch += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(\"epoch4-revised-5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training with HIL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from hil.HIL import HIL\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from networks.ensemble import Ensemble\n",
    "\n",
    "trial_name = f\"{str(int(time.time()))}\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ensemble = Ensemble(size=5)\n",
    "\n",
    "TRAIN = True\n",
    "SAVE_CLUSTER_IMAGES = True\n",
    "SAVE_CLUSTER_MEDOIDS = True\n",
    "PRETRAINING = True\n",
    "HUMAN_IN_LOOP = True\n",
    "SYNTHETIC_HIL = False\n",
    "EVOLUTION = True\n",
    "CLUSTER_AND_DISPLAY = True\n",
    "CONCAT_BVEC_AND_EVEC = True\n",
    "PASS_THROUGHS = 1\n",
    "RESULTS_DATA_SIZE = 10000\n",
    "EVOLUTIONS_PER_EPOCH = 50\n",
    "K_CLUSTERS = 8\n",
    "INITIAL_EVOLVES = 3\n",
    "\n",
    "anchor_dataset = ContinuingDataset(\"data\")\n",
    "sampled_dataset = SwarmDataset(\"data/full-dual-sensors\", rank=0)\n",
    "evolution, _ = evolution_controller()\n",
    "\n",
    "hil = HIL(name=trial_name, synthetic=SYNTHETIC_HIL, data_limiter=RESULTS_DATA_SIZE, clusters=K_CLUSTERS)\n",
    "HIL_archive = DataAggregationArchive()\n",
    "random_archive = DataAggregationArchive(scalar=True)\n",
    "EPSILON = 0.5\n",
    "\n",
    "simulation_time = 0\n",
    "evolution_time = 0\n",
    "training_time = 0\n",
    "hil_time = 0\n",
    "\n",
    "\n",
    "# TODO: Add Randomly-sampled Contrastive/Triplet Loss\n",
    "if PRETRAINING:\n",
    "    ensemble.load_model(\"two_sensor_subz1\")\n",
    "    print(\"Pretrained model loaded!\")\n",
    "\n",
    "dataset = anchor_dataset if EVOLUTION else sampled_dataset\n",
    "\n",
    "if TRAIN:\n",
    "    writer = SummaryWriter()\n",
    "    STOP_FLAG = False\n",
    "    for epoch in range(PASS_THROUGHS + 1):\n",
    "        if EVOLUTION:\n",
    "            start_time = time.time()\n",
    "            evolution.restart_screen()\n",
    "            for gen in range(EVOLUTIONS_PER_EPOCH if epoch > 0 else INITIAL_EVOLVES):\n",
    "                # Simulate current population + Save Data\n",
    "                for i in range(len(evolution.getPopulation())):\n",
    "                    # The collection of the original behavior vector below is only used to collect data to compare with the baseline\n",
    "                    visual_behavior, genome, baseline_behavior = evolution.next()\n",
    "                    dataset.new_entry(visual_behavior, genome, baseline_behavior)\n",
    "                simulation_time += (time.time() - start_time)\n",
    "\n",
    "                # Then, evolve\n",
    "                start_time = time.time()\n",
    "\n",
    "                # PARENT NETWORK?\n",
    "                embedded_archive = hil.getEmbeddedArchive(dataset, ensemble[0], concat_behavior=CONCAT_BVEC_AND_EVEC)\n",
    "                evolution.overwriteArchive(embedded_archive, random_archive)\n",
    "                embedded_behavior = embedded_archive.archive[-evolution.evolve_config.population:]\n",
    "                evolution.overwriteBehavior(embedded_behavior)\n",
    "                evolution.evolve()\n",
    "                evolution.restart_screen()\n",
    "                evolution_time += (time.time() - start_time)\n",
    "\n",
    "                print(f\"Evolution complete for e{epoch} and gen{gen}\")\n",
    "                print(f\"Size of Evolved Archive: {len(evolution.behavior_discovery.archive.archive)}\")\n",
    "\n",
    "                # Record the accuracy of the medoids with respect to the synthetic policy\n",
    "                medoid_acc, cluster_acc = 0, 0\n",
    "                if len(dataset) > 0 and SAVE_CLUSTER_MEDOIDS:\n",
    "                    if SYNTHETIC_HIL and EVOLUTION:\n",
    "                        hil.synthetic_knowledge = hil.syntheticBehaviorSpace(dataset)\n",
    "                        print(f\"Synthetic Human Knowledge Size: {len(hil.synthetic_knowledge.labels_)}\")\n",
    "                    medoid_acc, cluster_acc = hil.record_medoids(ensemble[0], dataset)\n",
    "\n",
    "                # Cluster current dataset, display clusters, and save for analysis\n",
    "                if len(dataset) > 0 and SAVE_CLUSTER_IMAGES:\n",
    "                    hil.embed_and_cluster(ensemble[0], dataset, auto_quit=True)\n",
    "                    evolution.restart_screen()\n",
    "\n",
    "                evolution.saveArchive(trial_name)\n",
    "\n",
    "        # Human in the Loop determines behavior embedding\n",
    "        if HUMAN_IN_LOOP and epoch < PASS_THROUGHS:\n",
    "            start_time = time.time()\n",
    "            improvements, human_loss, triplet_helpfulness, _ =   human_in_the_loop(\n",
    "                                                                dataset,\n",
    "                                                                ensemble[0],\n",
    "                                                                optimizer,\n",
    "                                                                loss_fn,\n",
    "                                                                HIL_archive,\n",
    "                                                                random_archive,\n",
    "                                                                len(dataset)\n",
    "                                                            )\n",
    "            hil_time += (time.time() - start_time)\n",
    "\n",
    "        # Train on past user information\n",
    "        if TRAIN:\n",
    "            start_time = time.time()\n",
    "            losses = None\n",
    "\n",
    "            average_loss = 50\n",
    "            loop = 0\n",
    "            BATCH_SIZE = 5\n",
    "            ensemble.training_mode()\n",
    "            while (average_loss > 0.01) and loop < 20:\n",
    "                loss_sum = 0\n",
    "                total_loss = 0\n",
    "                for i, (anchor, pos, neg) in enumerate(HIL_archive):\n",
    "                    anchor_encoding = dataset[anchor][0]\n",
    "                    similar_encoding = dataset[pos][0]\n",
    "                    anti_encoding = dataset[neg][0]\n",
    "                    losses = ensemble.train_triplet(anchor_encoding, similar_encoding, anti_encoding)\n",
    "                    total_loss += len(losses)\n",
    "                    loss_sum += sum(losses)\n",
    "\n",
    "                    if losses is not None and i > 0 and i % 20 == 0:\n",
    "                        print(f\"Epoch Progress: {(i*100) / len(HIL_archive)}%, Immediate Loss: {sum(losses) / len(losses)}\")\n",
    "\n",
    "                average_loss = loss_sum / (total_loss + 1)\n",
    "                print(f\"Loop: {loop}, Average Loss: {average_loss}\")\n",
    "                loop += 1\n",
    "\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            training_time += (time.time() - start_time)\n",
    "\n",
    "            writer.add_scalar(\"Loss/Average\", loss_sum / (total_loss + 1), epoch)\n",
    "\n",
    "        if SAVE_CLUSTER_MEDOIDS:\n",
    "            writer.add_scalar(\"Accuracy/MedoidClassification\", medoid_acc, epoch)\n",
    "            writer.add_scalar(\"Accuracy/RandomSampleClassification\", cluster_acc, epoch)\n",
    "\n",
    "        if HUMAN_IN_LOOP:\n",
    "            writer.add_scalar(\"Loss/TripletQuality\", triplet_helpfulness, epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Human_Queries\", epoch*8*9, epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Triplets_Generated\", len(HIL_archive), epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Random_Classes\", len(random_archive), epoch)\n",
    "\n",
    "        if EVOLUTION:\n",
    "            writer.add_scalar(\"Novelty/Highest\", evolution.behavior_discovery.getBestScore(), epoch)\n",
    "            writer.add_scalar(\"Novelty/Average\", evolution.behavior_discovery.getAverageScore(), epoch)\n",
    "\n",
    "        writer.add_scalar(\"Time/Simulation\", simulation_time, epoch)\n",
    "        writer.add_scalar(\"Time/HIL\", hil_time, epoch)\n",
    "        writer.add_scalar(\"Time/Evolution\", evolution_time, epoch)\n",
    "        writer.add_scalar(\"Time/Training\", training_time, epoch)\n",
    "\n",
    "evolution.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evolve and Display?\n",
    "print(len(evolution.archive.archive))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from NovelSwarmBehavior.novel_swarms.config.ResultsConfig import ResultsConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from data.swarmset import DataBuilder\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding().to(device)\n",
    "network.load_model(\"trialA-10-18-2022\")\n",
    "# anchor_dataset = SwarmDataset(\"data/full\", rank=0)\n",
    "\n",
    "WRITE_OUT = True\n",
    "if WRITE_OUT:\n",
    "    network.eval()\n",
    "    test_archive = NoveltyArchive()\n",
    "    for i in range(len(anchor_dataset)):\n",
    "        anchor_encoding, genome, _ = anchor_dataset[i]\n",
    "        anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "        embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "        test_archive.addToArchive(embedding, genome)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "import pygame\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=test_archive, k_clusters=6, world_config=world_config, tsne_perplexity=16, tsne_early_exaggeration=12, skip_tsne=False)\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()\n",
    "# results(config)\n",
    "pygame.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering + Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "\n",
    "# Cluster over saved behaviors\n",
    "archive = NoveltyArchive()\n",
    "for i, (_, genome, behavior, _, _, _, _) in enumerate(anchor_dataset):\n",
    "    archive.addToArchive(vec=behavior, genome=genome)\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=archive, k_clusters=7, world_config=world_config, tsne_perplexity=20, tsne_early_exaggeration=2, skip_tsne=False)\n",
    "\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(human_l_hist, \"b\", label='Human Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Time (Epochs)\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
