{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import random\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list\n",
    "\n",
    "def resizeInput(X, w=200):\n",
    "    frame = X.astype(np.uint8)\n",
    "    resized = cv2.resize(frame, dsize=(w, w), interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def translate(img, offset=(10, 10)):\n",
    "    h, w = img.shape\n",
    "    xoff, yoff = offset\n",
    "    if xoff < 0: xpadding = (0, -xoff)\n",
    "    else: xpadding = (xoff, 0)\n",
    "    if yoff < 0: ypadding = (0, -yoff)\n",
    "    else: ypadding = (yoff, 0)\n",
    "    img = np.pad(img, (xpadding, ypadding))\n",
    "\n",
    "    if xoff >= 0 and yoff >= 0:\n",
    "        return img[:w, :w]\n",
    "    elif xoff < 0 and yoff >= 0:\n",
    "        return img[-w:, :w]\n",
    "    elif xoff >= 0 and yoff < 0:\n",
    "        return img[:w, -w:]\n",
    "    return img[-w:, -w:]\n",
    "\n",
    "def zoom_at(img, zoom, coord=None):\n",
    "    # Adapted from https://stackoverflow.com/questions/69050464/zoom-into-image-with-opencv\n",
    "    h, w = [ zoom * i for i in img.shape ]\n",
    "    if coord is None: cx, cy = w/2, h/2\n",
    "    else: cx, cy = [ zoom*c for c in coord ]\n",
    "    img = cv2.resize( img, (0, 0), fx=zoom, fy=zoom)\n",
    "    img = img[ int(round(cy - h/zoom * .5)) : int(round(cy + h/zoom * .5)),\n",
    "               int(round(cx - w/zoom * .5)) : int(round(cx + w/zoom * .5))]\n",
    "    return img\n",
    "\n",
    "def get_color_distortion(X, s=3.0):\n",
    "    X = X + s * np.random.randn(X.shape[0], X.shape[1])\n",
    "    return X\n",
    "\n",
    "def getRandomTransformation(image, k=2):\n",
    "    transformation_choices = [\"Rotation\", \"Blur\", \"Zoom\", \"Translate\", \"Distort\", \"ResizedCrop\"]\n",
    "    # weights = [0.4, 0.3, 0.0, 0.2]\n",
    "    # weights = [1.0, 0.0, 0.0, 0.0]\n",
    "    # choices = random.choices(transformation_choices, weights, k=k)\n",
    "    choices = [\"ResizedCrop\"]\n",
    "    # choices = []\n",
    "    if \"RandomFlip\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper_A = RandomHorizontalFlip(0.5)\n",
    "        flipper_B = RandomVerticalFlip(0.5)\n",
    "        image = flipper_A(flipper_B(tmp))\n",
    "        image = image.squeeze(0).numpy()\n",
    "    if \"ResizedCrop\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(0.5)\n",
    "        cropper = RandomResizedCrop(size=(50,50), scale=(0.6, 1.0), ratio=(1.0, 1.0))\n",
    "        image = flipper(cropper(tmp))\n",
    "        image = image.squeeze(0).numpy()\n",
    "    if \"Rotation\" in choices:\n",
    "        theta = random.choice([90, 180, 270])\n",
    "        image = ndimage.rotate(image, theta)\n",
    "    if \"Blur\" in choices:\n",
    "        blur = random.choice([0.5, 1.0, 1.5])\n",
    "        image = ndimage.gaussian_filter(image, sigma=blur)\n",
    "    if \"Zoom\" in choices:\n",
    "        # zoom = random.choice([1.06, 1.12, 1.18])\n",
    "        padding = random.choice([10])\n",
    "        padded = np.pad(image, padding, mode='constant')\n",
    "        image = resizeInput(padded, 50)\n",
    "    if \"Translate\" in choices:\n",
    "        offsets = [i for i in range(-10, 10, 2)]\n",
    "        offset = (random.choice(offsets), random.choice(offsets))\n",
    "        # offset = (2, 2)\n",
    "        image = translate(image, offset)\n",
    "    if \"Distort\" in choices:\n",
    "        strength = random.choice([3.0, 5.0, 10.0])\n",
    "        image = get_color_distortion(image, s=strength)\n",
    "    if \"Flip\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(1.0)\n",
    "        image = flipper(tmp)\n",
    "        image = image.squeeze(0).numpy()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4181320\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Triplets based off of labeled classes (IID_TRIPLETS) or triplets based off ensemble Queries\n",
    "\"\"\"\n",
    "FROM_SCRATCH = False\n",
    "HEURISTIC = True\n",
    "IID_TRIPLETS = True\n",
    "TRUTH_FILE = \"original-hand-labeled-classes.txt\" if not HEURISTIC else \"heuristic-simple-model-classes.txt\"\n",
    "DATASET = SwarmDataset(\"../data/full-mini\", rank=0) if not HEURISTIC else SwarmDataset(\"../data/filtered-full\")\n",
    "ENSEMBLE_PATH = \"../checkpoints/ensembles/01-20-23-baseline\" if not HEURISTIC else \"../checkpoints/ensembles/01-26-23-heuristic-BL-pretraining\"\n",
    "if FROM_SCRATCH:\n",
    "    ENSEMBLE_PATH = None\n",
    "OUT = \"../data/oracle\"\n",
    "classes = [-1 for i in range(400)]\n",
    "with open(os.path.join(OUT, TRUTH_FILE), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i > len(classes) - 1:\n",
    "            break\n",
    "        triplet = CSVLineToVec(line)\n",
    "        classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "triplets = []\n",
    "\n",
    "if IID_TRIPLETS:\n",
    "    for i, i_c in enumerate(classes):\n",
    "        if i_c == 0:\n",
    "            continue\n",
    "        continue_to_top = False\n",
    "        for j, j_c in enumerate(classes):\n",
    "            if j_c != i_c:\n",
    "                continue\n",
    "            if i == j:\n",
    "                continue\n",
    "            for k, k_c in enumerate(classes):\n",
    "                # if k_c == 0:\n",
    "                #     continue\n",
    "                if k_c == i_c or k_c == j_c:\n",
    "                    continue\n",
    "                # if i_c == 0:\n",
    "                #     if not (i, i, k) in triplets:\n",
    "                #         triplets.append((i, i, k))\n",
    "                #         continue_to_top = True\n",
    "                triplets.append((i, j, k))\n",
    "            if continue_to_top:\n",
    "                break\n",
    "\n",
    "# Else, use an ensemble to create the triplets.\n",
    "else:\n",
    "    print(\"No Implementation Yet\")\n",
    "\n",
    "print(len(triplets))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 4, 1) (5, 5, 0)\n",
      "(0, 4, 2) (5, 5, 4)\n",
      "(0, 4, 3) (5, 5, 0)\n",
      "(0, 4, 5) (5, 5, 4)\n",
      "(0, 4, 6) (5, 5, 4)\n",
      "(0, 4, 7) (5, 5, 0)\n",
      "(0, 4, 8) (5, 5, 0)\n",
      "(0, 4, 9) (5, 5, 1)\n",
      "(0, 4, 11) (5, 5, 0)\n",
      "(0, 4, 13) (5, 5, 1)\n",
      "(0, 4, 14) (5, 5, 3)\n",
      "(0, 4, 15) (5, 5, 4)\n",
      "(0, 4, 16) (5, 5, 1)\n",
      "(0, 4, 17) (5, 5, 0)\n",
      "(0, 4, 18) (5, 5, 3)\n",
      "(0, 4, 19) (5, 5, 0)\n",
      "(0, 4, 20) (5, 5, 2)\n",
      "(0, 4, 21) (5, 5, 2)\n",
      "(0, 4, 23) (5, 5, 4)\n",
      "(0, 4, 24) (5, 5, 1)\n",
      "(0, 4, 25) (5, 5, 0)\n",
      "(0, 4, 26) (5, 5, 1)\n",
      "(0, 4, 27) (5, 5, 1)\n",
      "(0, 4, 28) (5, 5, 0)\n",
      "(0, 4, 30) (5, 5, 4)\n",
      "(0, 4, 31) (5, 5, 0)\n",
      "(0, 4, 32) (5, 5, 0)\n",
      "(0, 4, 33) (5, 5, 0)\n",
      "(0, 4, 34) (5, 5, 0)\n",
      "(0, 4, 35) (5, 5, 0)\n",
      "(0, 4, 36) (5, 5, 0)\n",
      "(0, 4, 37) (5, 5, 0)\n",
      "(0, 4, 38) (5, 5, 4)\n",
      "(0, 4, 39) (5, 5, 0)\n",
      "(0, 4, 40) (5, 5, 0)\n",
      "(0, 4, 41) (5, 5, 0)\n",
      "(0, 4, 42) (5, 5, 3)\n",
      "(0, 4, 43) (5, 5, 1)\n",
      "(0, 4, 44) (5, 5, 4)\n",
      "(0, 4, 45) (5, 5, 4)\n",
      "(0, 4, 46) (5, 5, 0)\n",
      "(0, 4, 47) (5, 5, 0)\n",
      "(0, 4, 48) (5, 5, 4)\n",
      "(0, 4, 49) (5, 5, 1)\n",
      "(0, 4, 50) (5, 5, 0)\n"
     ]
    }
   ],
   "source": [
    "for j in triplets[:45]:\n",
    "    print(j, (classes[j[0]], classes[j[1]], classes[j[2]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Adjusting learning rate of group 0 to 3.0000e-04.\n",
      "Adjusting learning rate of group 0 to 3.0000e-04.\n",
      "Adjusting learning rate of group 0 to 3.0000e-04.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [14.23053346 25.89887374 18.78745515]\n",
      "Epoch 0, loss: 19.638954117184593, windowed_loss: 50\n",
      "Adjusting learning rate of group 0 to 3.2255e-03.\n",
      "Adjusting learning rate of group 0 to 3.2255e-03.\n",
      "Adjusting learning rate of group 0 to 3.2255e-03.\n",
      "LR: [0.0032255105879601177, 0.0032255105879601177, 0.0032255105879601177]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [10.30940519 17.10629204 10.44111272]\n",
      "Epoch 1, loss: 12.618936652228946, windowed_loss: 50\n",
      "Adjusting learning rate of group 0 to 6.0706e-03.\n",
      "Adjusting learning rate of group 0 to 6.0706e-03.\n",
      "Adjusting learning rate of group 0 to 6.0706e-03.\n",
      "LR: [0.006070594095768738, 0.006070594095768738, 0.006070594095768738]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [5.99486732 6.15413782 4.31067886]\n",
      "Epoch 2, loss: 5.486561332430159, windowed_loss: 50\n",
      "Adjusting learning rate of group 0 to 8.8358e-03.\n",
      "Adjusting learning rate of group 0 to 8.8358e-03.\n",
      "Adjusting learning rate of group 0 to 8.8358e-03.\n",
      "LR: [0.00883580797560376, 0.00883580797560376, 0.00883580797560376]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [3.84455834 5.27069684 2.44304807]\n",
      "Epoch 3, loss: 3.8527677513304206, windowed_loss: 7.31942191199651\n",
      "Adjusting learning rate of group 0 to 1.1522e-02.\n",
      "Adjusting learning rate of group 0 to 1.1522e-02.\n",
      "Adjusting learning rate of group 0 to 1.1522e-02.\n",
      "LR: [0.011521713145533692, 0.011521713145533692, 0.011521713145533692]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [2.85532584 4.46299621 2.47992734]\n",
      "Epoch 4, loss: 3.2660831269763766, windowed_loss: 4.201804070245652\n",
      "Adjusting learning rate of group 0 to 1.4129e-02.\n",
      "Adjusting learning rate of group 0 to 1.4129e-02.\n",
      "Adjusting learning rate of group 0 to 1.4129e-02.\n",
      "LR: [0.014128874050690325, 0.014128874050690325, 0.014128874050690325]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [2.02108836 3.76691427 2.0062279 ]\n",
      "Epoch 5, loss: 2.5980768430800665, windowed_loss: 3.2389759071289546\n",
      "Adjusting learning rate of group 0 to 1.6658e-02.\n",
      "Adjusting learning rate of group 0 to 1.6658e-02.\n",
      "Adjusting learning rate of group 0 to 1.6658e-02.\n",
      "LR: [0.016657858726185294, 0.016657858726185294, 0.016657858726185294]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [1.59092455 3.04645113 1.64752649]\n",
      "Epoch 6, loss: 2.094967387971424, windowed_loss: 2.653042452675956\n",
      "Adjusting learning rate of group 0 to 1.9109e-02.\n",
      "Adjusting learning rate of group 0 to 1.9109e-02.\n",
      "Adjusting learning rate of group 0 to 1.9109e-02.\n",
      "LR: [0.019109238861838868, 0.019109238861838868, 0.019109238861838868]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [1.30799576 2.50919645 1.41461478]\n",
      "Epoch 7, loss: 1.7439356644948323, windowed_loss: 2.1456599651821073\n",
      "Adjusting learning rate of group 0 to 2.1484e-02.\n",
      "Adjusting learning rate of group 0 to 2.1484e-02.\n",
      "Adjusting learning rate of group 0 to 2.1484e-02.\n",
      "LR: [0.0214835898687926, 0.0214835898687926, 0.0214835898687926]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [1.11074647 2.11055871 1.27518432]\n",
      "Epoch 8, loss: 1.4988298359371364, windowed_loss: 1.779244296134464\n",
      "Adjusting learning rate of group 0 to 2.3781e-02.\n",
      "Adjusting learning rate of group 0 to 2.3781e-02.\n",
      "Adjusting learning rate of group 0 to 2.3781e-02.\n",
      "LR: [0.02378149094808132, 0.02378149094808132, 0.02378149094808132]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [0.88869682 1.84965127 1.08874123]\n",
      "Epoch 9, loss: 1.2756964422407606, windowed_loss: 1.50615398089091\n",
      "Adjusting learning rate of group 0 to 2.6004e-02.\n",
      "Adjusting learning rate of group 0 to 2.6004e-02.\n",
      "Adjusting learning rate of group 0 to 2.6004e-02.\n",
      "LR: [0.02600352516124342, 0.02600352516124342, 0.02600352516124342]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [0.70635225 1.70648113 0.9258838 ]\n",
      "Epoch 10, loss: 1.112905726546333, windowed_loss: 1.29581066824141\n",
      "Adjusting learning rate of group 0 to 2.5614e-02.\n",
      "Adjusting learning rate of group 0 to 2.5614e-02.\n",
      "Adjusting learning rate of group 0 to 2.5614e-02.\n",
      "LR: [0.02561444904736387, 0.02561444904736387, 0.02561444904736387]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 12.5\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 37.5\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 62.5\n",
      "Unsupervised Training.. 75.0\n",
      "Losses: [0.5449318  1.43934006 0.80449199]\n",
      "Epoch 11, loss: 0.9295879517282758, windowed_loss: 1.106063373505123\n",
      "Adjusting learning rate of group 0 to 2.5227e-02.\n",
      "Adjusting learning rate of group 0 to 2.5227e-02.\n",
      "Adjusting learning rate of group 0 to 2.5227e-02.\n",
      "LR: [0.025227333035442764, 0.025227333035442764, 0.025227333035442764]\n"
     ]
    }
   ],
   "source": [
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "ensemble = Ensemble(size=3, output_size=5, lr=3e-2, weight_decay=0, new_model=True, manual_schedulers=True, init=\"Random\")\n",
    "if ENSEMBLE_PATH is not None:\n",
    "    ensemble.load_ensemble(ENSEMBLE_PATH, full=True)\n",
    "sampled_dataset = DATASET\n",
    "\n",
    "def pretraining(data, ensemble, data_cutoff=None, data_size=500):\n",
    "    if data_cutoff is None:\n",
    "        data_cutoff = len(data) - 1\n",
    "    # np.random.seed(0)\n",
    "    random.shuffle(triplets)\n",
    "    samples = triplets[:data_size]\n",
    "    total_loss = np.array([0.0 for i in range(len(ensemble.ensemble))])\n",
    "\n",
    "    BATCH_SIZE = 4096\n",
    "    total_updates = 0\n",
    "    total_batches = max(len(samples), data_size) // BATCH_SIZE\n",
    "\n",
    "    # Batch the data\n",
    "    for i in range(0, len(samples), BATCH_SIZE):\n",
    "        # AUGMENT_SIZE = 1\n",
    "        if i + (BATCH_SIZE) >= len(samples):\n",
    "            continue\n",
    "\n",
    "        print(f\"Unsupervised Training.. {(total_updates * 100) / total_batches}\")\n",
    "\n",
    "        temp_losses = np.array([0.0 for _ in ensemble.ensemble])\n",
    "\n",
    "        anchors = np.array([data[samples[i + j][0]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        positives = np.array(\n",
    "            [\n",
    "                getRandomTransformation(data[samples[i + j][1]][0]) if samples[i + j][0] != samples[i + j][1] else getRandomTransformation(data[samples[i + j][0]][0])\n",
    "                for j in range(BATCH_SIZE)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        negatives = np.array([data[samples[i + j][2]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        anchors = np.expand_dims(anchors, axis=1)\n",
    "        positives = np.expand_dims(positives, axis=1)\n",
    "        negatives = np.expand_dims(negatives, axis=1)\n",
    "\n",
    "        losses = ensemble.train_batch(anchors, positives, negatives)\n",
    "        temp_losses += losses\n",
    "\n",
    "        total_loss += temp_losses\n",
    "        total_updates += 1\n",
    "\n",
    "    return total_loss, max(total_updates, 1)\n",
    "\n",
    "t_1 = time.time()\n",
    "epochs = 0\n",
    "loss_history = []\n",
    "while epochs < 150:\n",
    "    losses, total_updates = pretraining(sampled_dataset, ensemble, data_cutoff=9999, data_size=(4096 * 8))\n",
    "    average_loss = losses / total_updates\n",
    "    locale_loss = sum(average_loss) / len(average_loss)\n",
    "    loss_history.append(locale_loss)\n",
    "    loss = (sum(loss_history[-3:]) / 3) if len(loss_history) > 3 else 50\n",
    "    print(f\"Losses: {average_loss}\")\n",
    "    print(f\"Epoch {epochs}, loss: {locale_loss}, windowed_loss: {loss}\")\n",
    "    epochs += 1\n",
    "    ensemble.step_schedulers()\n",
    "    print(f\"LR: {ensemble.get_lr()}\")\n",
    "\n",
    "print(f\"Total Pre-training Time: {time.time() - t_1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(f\"../checkpoints/ensembles/{int(time.time())}\", full=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
