{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import random\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list\n",
    "\n",
    "def resizeInput(X, w=200):\n",
    "    frame = X.astype(np.uint8)\n",
    "    resized = cv2.resize(frame, dsize=(w, w), interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def translate(img, offset=(10, 10)):\n",
    "    h, w = img.shape\n",
    "    xoff, yoff = offset\n",
    "    if xoff < 0: xpadding = (0, -xoff)\n",
    "    else: xpadding = (xoff, 0)\n",
    "    if yoff < 0: ypadding = (0, -yoff)\n",
    "    else: ypadding = (yoff, 0)\n",
    "    img = np.pad(img, (xpadding, ypadding))\n",
    "\n",
    "    if xoff >= 0 and yoff >= 0:\n",
    "        return img[:w, :w]\n",
    "    elif xoff < 0 and yoff >= 0:\n",
    "        return img[-w:, :w]\n",
    "    elif xoff >= 0 and yoff < 0:\n",
    "        return img[:w, -w:]\n",
    "    return img[-w:, -w:]\n",
    "\n",
    "def zoom_at(img, zoom, coord=None):\n",
    "    # Adapted from https://stackoverflow.com/questions/69050464/zoom-into-image-with-opencv\n",
    "    h, w = [ zoom * i for i in img.shape ]\n",
    "    if coord is None: cx, cy = w/2, h/2\n",
    "    else: cx, cy = [ zoom*c for c in coord ]\n",
    "    img = cv2.resize( img, (0, 0), fx=zoom, fy=zoom)\n",
    "    img = img[ int(round(cy - h/zoom * .5)) : int(round(cy + h/zoom * .5)),\n",
    "               int(round(cx - w/zoom * .5)) : int(round(cx + w/zoom * .5))]\n",
    "    return img\n",
    "\n",
    "def get_color_distortion(X, s=3.0):\n",
    "    X = X + s * np.random.randn(X.shape[0], X.shape[1])\n",
    "    return X\n",
    "\n",
    "def getRandomTransformation(image, k=2):\n",
    "    transformation_choices = [\"Rotation\", \"Blur\", \"Zoom\", \"Translate\", \"Distort\", \"ResizedCrop\"]\n",
    "    # weights = [0.4, 0.3, 0.0, 0.2]\n",
    "    # weights = [1.0, 0.0, 0.0, 0.0]\n",
    "    # choices = random.choices(transformation_choices, weights, k=k)\n",
    "    choices = [\"ResizedCrop\", \"Rotation\"]\n",
    "    if \"ResizedCrop\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(0.5)\n",
    "        cropper = RandomResizedCrop(size=(50,50), scale=(0.6, 1.0), ratio=(1.0, 1.0))\n",
    "        image = flipper(cropper(tmp))\n",
    "        image = image.squeeze(0).numpy()\n",
    "    if \"Rotation\" in choices:\n",
    "        theta = random.choice([90, 180, 270])\n",
    "        image = ndimage.rotate(image, theta)\n",
    "    if \"Blur\" in choices:\n",
    "        blur = random.choice([0.5, 1.0, 1.5])\n",
    "        image = ndimage.gaussian_filter(image, sigma=blur)\n",
    "    if \"Zoom\" in choices:\n",
    "        # zoom = random.choice([1.06, 1.12, 1.18])\n",
    "        padding = random.choice([10])\n",
    "        padded = np.pad(image, padding, mode='constant')\n",
    "        image = resizeInput(padded, 50)\n",
    "    if \"Translate\" in choices:\n",
    "        offsets = [i for i in range(-10, 10, 2)]\n",
    "        offset = (random.choice(offsets), random.choice(offsets))\n",
    "        # offset = (2, 2)\n",
    "        image = translate(image, offset)\n",
    "    if \"Distort\" in choices:\n",
    "        strength = random.choice([3.0, 5.0, 10.0])\n",
    "        image = get_color_distortion(image, s=strength)\n",
    "    if \"Flip\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(1.0)\n",
    "        image = flipper(tmp)\n",
    "        image = image.squeeze(0).numpy()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127728\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Triplets based off of labeled classes (IID_TRIPLETS) or triplets based off ensemble Queries\n",
    "\"\"\"\n",
    "HEURISTIC = False\n",
    "IID_TRIPLETS = True\n",
    "TRUTH_FILE = \"original-hand-labeled-classes.txt\" if not HEURISTIC else \"NONE\"\n",
    "OUT = \"../data/oracle\"\n",
    "classes = [-1 for i in range(200)]\n",
    "with open(os.path.join(OUT, TRUTH_FILE), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i > len(classes) - 1:\n",
    "            break\n",
    "        triplet = CSVLineToVec(line)\n",
    "        classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "triplets = []\n",
    "\n",
    "if IID_TRIPLETS:\n",
    "    for i, i_c in enumerate(classes):\n",
    "        if i_c == 0:\n",
    "            continue\n",
    "        for j, j_c in enumerate(classes):\n",
    "            if j_c != i_c:\n",
    "                continue\n",
    "            for k, k_c in enumerate(classes):\n",
    "                if k_c == i_c or k_c == j_c:\n",
    "                    continue\n",
    "                triplets.append((i, j, k))\n",
    "\n",
    "# Else, use an ensemble to create the triplets.\n",
    "else:\n",
    "    print(\"No Implementation Yet\")\n",
    "\n",
    "print(len(triplets))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 0) (1, 1, 0)\n",
      "(1, 1, 2) (1, 1, 0)\n",
      "(1, 1, 3) (1, 1, 5)\n",
      "(1, 1, 4) (1, 1, 0)\n",
      "(1, 1, 5) (1, 1, 0)\n",
      "(1, 1, 6) (1, 1, 0)\n",
      "(1, 1, 7) (1, 1, 0)\n",
      "(1, 1, 8) (1, 1, 0)\n",
      "(1, 1, 9) (1, 1, 0)\n",
      "(1, 1, 10) (1, 1, 0)\n",
      "(1, 1, 11) (1, 1, 0)\n",
      "(1, 1, 12) (1, 1, 0)\n",
      "(1, 1, 14) (1, 1, 4)\n",
      "(1, 1, 15) (1, 1, 0)\n",
      "(1, 1, 16) (1, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "for j in triplets[:15]:\n",
    "    print(j, (classes[j[0]], classes[j[1]], classes[j[2]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Adjusting learning rate of group 0 to 4.0000e-02.\n",
      "Adjusting learning rate of group 0 to 4.0000e-02.\n",
      "Adjusting learning rate of group 0 to 4.0000e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [3.19862404 4.27749333 5.95398302]\n",
      "Epoch 0, loss: 4.476700129688427, windowed_loss: 50\n",
      "Adjusting learning rate of group 0 to 3.9204e-02.\n",
      "Adjusting learning rate of group 0 to 3.9204e-02.\n",
      "Adjusting learning rate of group 0 to 3.9204e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [1.99708962 0.93479285 1.48925438]\n",
      "Epoch 1, loss: 1.4737122853196436, windowed_loss: 50\n",
      "Adjusting learning rate of group 0 to 3.8416e-02.\n",
      "Adjusting learning rate of group 0 to 3.8416e-02.\n",
      "Adjusting learning rate of group 0 to 3.8416e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [1.13972479 0.95195495 3.50503589]\n",
      "Epoch 2, loss: 1.8655718737382003, windowed_loss: 50\n",
      "Adjusting learning rate of group 0 to 3.7636e-02.\n",
      "Adjusting learning rate of group 0 to 3.7636e-02.\n",
      "Adjusting learning rate of group 0 to 3.7636e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.61698651 0.27400455 1.14766134]\n",
      "Epoch 3, loss: 0.6795508001520428, windowed_loss: 1.3396116530699622\n",
      "Adjusting learning rate of group 0 to 3.6864e-02.\n",
      "Adjusting learning rate of group 0 to 3.6864e-02.\n",
      "Adjusting learning rate of group 0 to 3.6864e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.43330691 0.07311248 0.04777116]\n",
      "Epoch 4, loss: 0.18473018336820823, windowed_loss: 0.9099509524194839\n",
      "Adjusting learning rate of group 0 to 3.6100e-02.\n",
      "Adjusting learning rate of group 0 to 3.6100e-02.\n",
      "Adjusting learning rate of group 0 to 3.6100e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.06262725 0.04578008 0.01497708]\n",
      "Epoch 5, loss: 0.041128138966497876, windowed_loss: 0.30180304082891635\n",
      "Adjusting learning rate of group 0 to 3.5344e-02.\n",
      "Adjusting learning rate of group 0 to 3.5344e-02.\n",
      "Adjusting learning rate of group 0 to 3.5344e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.0117126  0.02974282 0.00017723]\n",
      "Epoch 6, loss: 0.013877551927561722, windowed_loss: 0.0799119580874226\n",
      "Adjusting learning rate of group 0 to 3.4596e-02.\n",
      "Adjusting learning rate of group 0 to 3.4596e-02.\n",
      "Adjusting learning rate of group 0 to 3.4596e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [1.58790436e-03 1.98017356e-02 3.06620233e-05]\n",
      "Epoch 7, loss: 0.007140100650709643, windowed_loss: 0.020715263848256414\n",
      "Adjusting learning rate of group 0 to 3.3856e-02.\n",
      "Adjusting learning rate of group 0 to 3.3856e-02.\n",
      "Adjusting learning rate of group 0 to 3.3856e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [9.04810915e-04 1.46820024e-03 2.46918850e-05]\n",
      "Epoch 8, loss: 0.0007992343456354193, windowed_loss: 0.007272295641302262\n",
      "Adjusting learning rate of group 0 to 3.3124e-02.\n",
      "Adjusting learning rate of group 0 to 3.3124e-02.\n",
      "Adjusting learning rate of group 0 to 3.3124e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [5.57333351e-04 8.90483990e-04 4.00586474e-05]\n",
      "Epoch 9, loss: 0.0004959586627220595, windowed_loss: 0.002811764553022374\n",
      "Adjusting learning rate of group 0 to 3.2400e-02.\n",
      "Adjusting learning rate of group 0 to 3.2400e-02.\n",
      "Adjusting learning rate of group 0 to 3.2400e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [3.26096230e-04 5.86281230e-04 3.63642109e-05]\n",
      "Epoch 10, loss: 0.00031624722384637397, windowed_loss: 0.0005371467440679509\n",
      "Adjusting learning rate of group 0 to 3.1684e-02.\n",
      "Adjusting learning rate of group 0 to 3.1684e-02.\n",
      "Adjusting learning rate of group 0 to 3.1684e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [2.20709172e-04 4.99524026e-04 1.49663237e-05]\n",
      "Epoch 11, loss: 0.000245066507348931, windowed_loss: 0.0003524241313057882\n",
      "Adjusting learning rate of group 0 to 3.0976e-02.\n",
      "Adjusting learning rate of group 0 to 3.0976e-02.\n",
      "Adjusting learning rate of group 0 to 3.0976e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00015862 0.00041838 0.        ]\n",
      "Epoch 12, loss: 0.00019233506573464282, windowed_loss: 0.00025121626564331595\n",
      "Adjusting learning rate of group 0 to 3.0276e-02.\n",
      "Adjusting learning rate of group 0 to 3.0276e-02.\n",
      "Adjusting learning rate of group 0 to 3.0276e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00013695 0.00035219 0.        ]\n",
      "Epoch 13, loss: 0.0001630487636492778, windowed_loss: 0.00020015011224428384\n",
      "Adjusting learning rate of group 0 to 2.9584e-02.\n",
      "Adjusting learning rate of group 0 to 2.9584e-02.\n",
      "Adjusting learning rate of group 0 to 2.9584e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00013917 0.00030173 0.        ]\n",
      "Epoch 14, loss: 0.00014696671046637077, windowed_loss: 0.00016745017995009712\n",
      "Adjusting learning rate of group 0 to 2.8900e-02.\n",
      "Adjusting learning rate of group 0 to 2.8900e-02.\n",
      "Adjusting learning rate of group 0 to 2.8900e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00013051 0.00026308 0.        ]\n",
      "Epoch 15, loss: 0.0001311970609290305, windowed_loss: 0.00014707084501489303\n",
      "Adjusting learning rate of group 0 to 2.8224e-02.\n",
      "Adjusting learning rate of group 0 to 2.8224e-02.\n",
      "Adjusting learning rate of group 0 to 2.8224e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00010941 0.00024639 0.        ]\n",
      "Epoch 16, loss: 0.00011860124356243559, windowed_loss: 0.00013225500498594562\n",
      "Adjusting learning rate of group 0 to 2.7556e-02.\n",
      "Adjusting learning rate of group 0 to 2.7556e-02.\n",
      "Adjusting learning rate of group 0 to 2.7556e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [7.71497707e-05 2.27961404e-04 0.00000000e+00]\n",
      "Epoch 17, loss: 0.00010170372483390634, windowed_loss: 0.00011716734310845748\n",
      "Adjusting learning rate of group 0 to 2.6896e-02.\n",
      "Adjusting learning rate of group 0 to 2.6896e-02.\n",
      "Adjusting learning rate of group 0 to 2.6896e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [3.97444913e-05 2.08353113e-04 0.00000000e+00]\n",
      "Epoch 18, loss: 8.269920144029842e-05, windowed_loss: 0.00010100138994554679\n",
      "Adjusting learning rate of group 0 to 2.6244e-02.\n",
      "Adjusting learning rate of group 0 to 2.6244e-02.\n",
      "Adjusting learning rate of group 0 to 2.6244e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [1.90063635e-05 1.88075990e-04 0.00000000e+00]\n",
      "Epoch 19, loss: 6.902745113738122e-05, windowed_loss: 8.447679247052866e-05\n",
      "Adjusting learning rate of group 0 to 2.5600e-02.\n",
      "Adjusting learning rate of group 0 to 2.5600e-02.\n",
      "Adjusting learning rate of group 0 to 2.5600e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [9.49704929e-06 1.67873470e-04 0.00000000e+00]\n",
      "Epoch 20, loss: 5.9123506509168175e-05, windowed_loss: 7.02833863622826e-05\n",
      "Adjusting learning rate of group 0 to 2.4964e-02.\n",
      "Adjusting learning rate of group 0 to 2.4964e-02.\n",
      "Adjusting learning rate of group 0 to 2.4964e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [2.45767766e-06 1.47021691e-04 0.00000000e+00]\n",
      "Epoch 21, loss: 4.982645623385906e-05, windowed_loss: 5.932580462680282e-05\n",
      "Adjusting learning rate of group 0 to 2.4336e-02.\n",
      "Adjusting learning rate of group 0 to 2.4336e-02.\n",
      "Adjusting learning rate of group 0 to 2.4336e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [2.85570540e-07 1.22407677e-04 0.00000000e+00]\n",
      "Epoch 22, loss: 4.089774904392099e-05, windowed_loss: 4.9949237262316076e-05\n",
      "Adjusting learning rate of group 0 to 2.3716e-02.\n",
      "Adjusting learning rate of group 0 to 2.3716e-02.\n",
      "Adjusting learning rate of group 0 to 2.3716e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.         0.00010726 0.        ]\n",
      "Epoch 23, loss: 3.575262743779408e-05, windowed_loss: 4.215894423852471e-05\n",
      "Adjusting learning rate of group 0 to 2.3104e-02.\n",
      "Adjusting learning rate of group 0 to 2.3104e-02.\n",
      "Adjusting learning rate of group 0 to 2.3104e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00000000e+00 9.00465454e-05 0.00000000e+00]\n",
      "Epoch 24, loss: 3.0015515143512394e-05, windowed_loss: 3.555529720840916e-05\n",
      "Adjusting learning rate of group 0 to 2.2500e-02.\n",
      "Adjusting learning rate of group 0 to 2.2500e-02.\n",
      "Adjusting learning rate of group 0 to 2.2500e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00000000e+00 7.80623917e-05 0.00000000e+00]\n",
      "Epoch 25, loss: 2.602079722990272e-05, windowed_loss: 3.0596313270403064e-05\n",
      "Adjusting learning rate of group 0 to 2.1904e-02.\n",
      "Adjusting learning rate of group 0 to 2.1904e-02.\n",
      "Adjusting learning rate of group 0 to 2.1904e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00000000e+00 6.55998085e-05 0.00000000e+00]\n",
      "Epoch 26, loss: 2.1866602843166683e-05, windowed_loss: 2.596763840552727e-05\n",
      "Adjusting learning rate of group 0 to 2.1316e-02.\n",
      "Adjusting learning rate of group 0 to 2.1316e-02.\n",
      "Adjusting learning rate of group 0 to 2.1316e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00000000e+00 5.32958581e-05 0.00000000e+00]\n",
      "Epoch 27, loss: 1.776528604809315e-05, windowed_loss: 2.1884228707054186e-05\n",
      "Adjusting learning rate of group 0 to 2.0736e-02.\n",
      "Adjusting learning rate of group 0 to 2.0736e-02.\n",
      "Adjusting learning rate of group 0 to 2.0736e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00000000e+00 4.62950463e-05 0.00000000e+00]\n",
      "Epoch 28, loss: 1.5431682088522503e-05, windowed_loss: 1.8354523659927443e-05\n",
      "Adjusting learning rate of group 0 to 2.0164e-02.\n",
      "Adjusting learning rate of group 0 to 2.0164e-02.\n",
      "Adjusting learning rate of group 0 to 2.0164e-02.\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 3.225806451612903\n",
      "Unsupervised Training.. 6.451612903225806\n",
      "Unsupervised Training.. 9.67741935483871\n",
      "Unsupervised Training.. 12.903225806451612\n",
      "Unsupervised Training.. 16.129032258064516\n",
      "Unsupervised Training.. 19.35483870967742\n",
      "Unsupervised Training.. 22.580645161290324\n",
      "Unsupervised Training.. 25.806451612903224\n",
      "Unsupervised Training.. 29.032258064516128\n",
      "Unsupervised Training.. 32.25806451612903\n",
      "Unsupervised Training.. 35.483870967741936\n",
      "Unsupervised Training.. 38.70967741935484\n",
      "Unsupervised Training.. 41.935483870967744\n",
      "Unsupervised Training.. 45.16129032258065\n",
      "Unsupervised Training.. 48.38709677419355\n",
      "Unsupervised Training.. 51.61290322580645\n",
      "Unsupervised Training.. 54.83870967741935\n",
      "Unsupervised Training.. 58.064516129032256\n",
      "Unsupervised Training.. 61.29032258064516\n",
      "Unsupervised Training.. 64.51612903225806\n",
      "Unsupervised Training.. 67.74193548387096\n",
      "Unsupervised Training.. 70.96774193548387\n",
      "Unsupervised Training.. 74.19354838709677\n",
      "Unsupervised Training.. 77.41935483870968\n",
      "Unsupervised Training.. 80.64516129032258\n",
      "Unsupervised Training.. 83.87096774193549\n",
      "Unsupervised Training.. 87.09677419354838\n",
      "Unsupervised Training.. 90.3225806451613\n",
      "Unsupervised Training.. 93.54838709677419\n",
      "Unsupervised Training.. 96.7741935483871\n",
      "Losses: [0.00000000e+00 3.94223964e-05 0.00000000e+00]\n",
      "Epoch 29, loss: 1.3140798796729376e-05, windowed_loss: 1.544592231111501e-05\n",
      "Adjusting learning rate of group 0 to 1.9600e-02.\n",
      "Adjusting learning rate of group 0 to 1.9600e-02.\n",
      "Adjusting learning rate of group 0 to 1.9600e-02.\n",
      "Total Pre-training Time: 863.9679560661316\n"
     ]
    }
   ],
   "source": [
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "ensemble = Ensemble(size=3, output_size=5, lr=4e-2, weight_decay=0, new_model=True, manual_schedulers=True, init=\"Random\")\n",
    "ensemble.load_ensemble(\"../checkpoints/ensembles/01-20-23-baseline\", full=True)\n",
    "sampled_dataset = SwarmDataset(\"../data/full-mini\", rank=0)\n",
    "\n",
    "def pretraining(data, ensemble, data_cutoff=None, data_size=500):\n",
    "    if data_cutoff is None:\n",
    "        data_cutoff = len(data) - 1\n",
    "    # np.random.seed(0)\n",
    "    samples = triplets\n",
    "    total_loss = np.array([0.0 for i in range(len(ensemble.ensemble))])\n",
    "\n",
    "    BATCH_SIZE = 4096\n",
    "    total_updates = 0\n",
    "    total_batches = max(len(samples), data_size) // BATCH_SIZE\n",
    "\n",
    "    # Batch the data\n",
    "    for i in range(0, len(samples), BATCH_SIZE):\n",
    "        # AUGMENT_SIZE = 1\n",
    "        if i + (BATCH_SIZE) >= len(samples):\n",
    "            continue\n",
    "\n",
    "        print(f\"Unsupervised Training.. {(total_updates * 100) / total_batches}\")\n",
    "\n",
    "        temp_losses = np.array([0.0 for _ in ensemble.ensemble])\n",
    "\n",
    "        anchors = np.array([data[samples[i + j][0]][0] for j in range(BATCH_SIZE)])\n",
    "        positives = np.array([data[samples[i + j][1]][0] for j in range(BATCH_SIZE)])\n",
    "        negatives = np.array([data[samples[i + j][2]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        anchors = np.expand_dims(anchors, axis=1)\n",
    "        positives = np.expand_dims(positives, axis=1)\n",
    "        negatives = np.expand_dims(negatives, axis=1)\n",
    "\n",
    "        losses = ensemble.train_batch(anchors, positives, negatives)\n",
    "        temp_losses += losses\n",
    "\n",
    "        total_loss += temp_losses\n",
    "        total_updates += 1\n",
    "\n",
    "    return total_loss, max(total_updates, 1)\n",
    "\n",
    "t_1 = time.time()\n",
    "epochs = 0\n",
    "loss_history = []\n",
    "while epochs < 30:\n",
    "    losses, total_updates = pretraining(sampled_dataset, ensemble, data_cutoff=9999, data_size=(4096 * 3))\n",
    "    average_loss = losses / total_updates\n",
    "    locale_loss = sum(average_loss) / len(average_loss)\n",
    "    loss_history.append(locale_loss)\n",
    "    loss = (sum(loss_history[-3:]) / 3) if len(loss_history) > 3 else 50\n",
    "    print(f\"Losses: {average_loss}\")\n",
    "    print(f\"Epoch {epochs}, loss: {locale_loss}, windowed_loss: {loss}\")\n",
    "    epochs += 1\n",
    "    ensemble.step_schedulers()\n",
    "\n",
    "print(f\"Total Pre-training Time: {time.time() - t_1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(f\"../checkpoints/ensembles/{int(time.time())}\", full=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
