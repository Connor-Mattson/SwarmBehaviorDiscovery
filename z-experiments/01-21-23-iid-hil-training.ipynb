{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import random\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list\n",
    "\n",
    "def resizeInput(X, w=200):\n",
    "    frame = X.astype(np.uint8)\n",
    "    resized = cv2.resize(frame, dsize=(w, w), interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def translate(img, offset=(10, 10)):\n",
    "    h, w = img.shape\n",
    "    xoff, yoff = offset\n",
    "    if xoff < 0: xpadding = (0, -xoff)\n",
    "    else: xpadding = (xoff, 0)\n",
    "    if yoff < 0: ypadding = (0, -yoff)\n",
    "    else: ypadding = (yoff, 0)\n",
    "    img = np.pad(img, (xpadding, ypadding))\n",
    "\n",
    "    if xoff >= 0 and yoff >= 0:\n",
    "        return img[:w, :w]\n",
    "    elif xoff < 0 and yoff >= 0:\n",
    "        return img[-w:, :w]\n",
    "    elif xoff >= 0 and yoff < 0:\n",
    "        return img[:w, -w:]\n",
    "    return img[-w:, -w:]\n",
    "\n",
    "def zoom_at(img, zoom, coord=None):\n",
    "    # Adapted from https://stackoverflow.com/questions/69050464/zoom-into-image-with-opencv\n",
    "    h, w = [ zoom * i for i in img.shape ]\n",
    "    if coord is None: cx, cy = w/2, h/2\n",
    "    else: cx, cy = [ zoom*c for c in coord ]\n",
    "    img = cv2.resize( img, (0, 0), fx=zoom, fy=zoom)\n",
    "    img = img[ int(round(cy - h/zoom * .5)) : int(round(cy + h/zoom * .5)),\n",
    "               int(round(cx - w/zoom * .5)) : int(round(cx + w/zoom * .5))]\n",
    "    return img\n",
    "\n",
    "def get_color_distortion(X, s=3.0):\n",
    "    X = X + s * np.random.randn(X.shape[0], X.shape[1])\n",
    "    return X\n",
    "\n",
    "def getRandomFlip(X):\n",
    "    tmp = torch.tensor(X).unsqueeze(0)\n",
    "    flipper_A = RandomHorizontalFlip(0.5)\n",
    "    flipper_B = RandomVerticalFlip(0.5)\n",
    "    image = flipper_A(flipper_B(tmp))\n",
    "    image = image.squeeze(0).numpy()\n",
    "    return image\n",
    "\n",
    "def getRandomTransformation(image, k=2):\n",
    "    transformation_choices = [\"Rotation\", \"Blur\", \"Zoom\", \"Translate\", \"Distort\", \"ResizedCrop\"]\n",
    "    # weights = [0.4, 0.3, 0.0, 0.2]\n",
    "    # weights = [1.0, 0.0, 0.0, 0.0]\n",
    "    # choices = random.choices(transformation_choices, weights, k=k)\n",
    "    choices = [\"ResizedCrop\"]\n",
    "    # choices = []\n",
    "    if \"RandomFlip\" in choices:\n",
    "        image = getRandomFlip(image)\n",
    "    if \"ResizedCrop\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(0.5)\n",
    "        cropper = RandomResizedCrop(size=(50,50), scale=(0.6, 1.0), ratio=(1.0, 1.0))\n",
    "        image = flipper(cropper(tmp))\n",
    "        image = image.squeeze(0).numpy()\n",
    "    if \"Rotation\" in choices:\n",
    "        theta = random.choice([90, 180, 270])\n",
    "        image = ndimage.rotate(image, theta)\n",
    "    if \"Blur\" in choices:\n",
    "        blur = random.choice([0.5, 1.0, 1.5])\n",
    "        image = ndimage.gaussian_filter(image, sigma=blur)\n",
    "    if \"Zoom\" in choices:\n",
    "        # zoom = random.choice([1.06, 1.12, 1.18])\n",
    "        padding = random.choice([10])\n",
    "        padded = np.pad(image, padding, mode='constant')\n",
    "        image = resizeInput(padded, 50)\n",
    "    if \"Translate\" in choices:\n",
    "        offsets = [i for i in range(-10, 10, 2)]\n",
    "        offset = (random.choice(offsets), random.choice(offsets))\n",
    "        # offset = (2, 2)\n",
    "        image = translate(image, offset)\n",
    "    if \"Distort\" in choices:\n",
    "        strength = random.choice([3.0, 5.0, 10.0])\n",
    "        image = get_color_distortion(image, s=strength)\n",
    "    if \"Flip\" in choices:\n",
    "        tmp = torch.tensor(image).unsqueeze(0)\n",
    "        flipper = RandomHorizontalFlip(1.0)\n",
    "        image = flipper(tmp)\n",
    "        image = image.squeeze(0).numpy()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532636\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Triplets based off of labeled classes (IID_TRIPLETS) or triplets based off ensemble Queries\n",
    "\"\"\"\n",
    "FROM_SCRATCH = False\n",
    "HEURISTIC = False\n",
    "IID_TRIPLETS = True\n",
    "TWO_SENSOR = True\n",
    "\n",
    "if TWO_SENSOR:\n",
    "    TRUTH_FILE = \"gecco-two-sensor-classes.txt\" if not HEURISTIC else \"\"\n",
    "    DATASET = SwarmDataset(\"../data/gecco-two-sensor\", rank=0) if not HEURISTIC else SwarmDataset(\"../data/filtered-full\")\n",
    "    ENSEMBLE_PATH = \"../checkpoints/ensembles/01-28-23-2S-Pre-B\" if not HEURISTIC else \"\"\n",
    "else:\n",
    "    TRUTH_FILE = \"original-hand-labeled-classes.txt\" if not HEURISTIC else \"heuristic-simple-model-classes.txt\"\n",
    "    DATASET = SwarmDataset(\"../data/full-mini\", rank=0) if not HEURISTIC else SwarmDataset(\"../data/filtered-full\")\n",
    "    ENSEMBLE_PATH = \"../checkpoints/ensembles/01-20-23-baseline\" if not HEURISTIC else \"../checkpoints/ensembles/01-26-23-heuristic-BL-pretraining\"\n",
    "\n",
    "if FROM_SCRATCH:\n",
    "    ENSEMBLE_PATH = None\n",
    "OUT = \"../data/oracle\"\n",
    "classes = [-1 for i in range(400)]\n",
    "with open(os.path.join(OUT, TRUTH_FILE), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i > len(classes) - 1:\n",
    "            break\n",
    "        triplet = CSVLineToVec(line)\n",
    "        classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "triplets = []\n",
    "\n",
    "if IID_TRIPLETS:\n",
    "    for i, i_c in enumerate(classes):\n",
    "        if i_c == 0:\n",
    "            continue\n",
    "        continue_to_top = False\n",
    "        for j, j_c in enumerate(classes):\n",
    "            if j_c != i_c:\n",
    "                continue\n",
    "            if i == j:\n",
    "                continue\n",
    "            for k, k_c in enumerate(classes):\n",
    "                if k_c == 0:\n",
    "                    continue\n",
    "                if k_c == i_c or k_c == j_c:\n",
    "                    continue\n",
    "                # if i_c == 0:\n",
    "                #     if not (i, i, k) in triplets:\n",
    "                #         triplets.append((i, i, k))\n",
    "                #         continue_to_top = True\n",
    "                triplets.append((i, j, k))\n",
    "            if continue_to_top:\n",
    "                break\n",
    "\n",
    "# Else, use an ensemble to create the triplets.\n",
    "else:\n",
    "    print(\"No Implementation Yet\")\n",
    "\n",
    "print(len(triplets))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215, 28, 314) (5, 5, 2)\n",
      "(284, 340, 390) (4, 4, 2)\n",
      "(371, 186, 219) (5, 5, 1)\n",
      "(164, 183, 398) (3, 3, 5)\n",
      "(146, 182, 152) (2, 2, 5)\n",
      "(142, 218, 266) (1, 1, 2)\n",
      "(340, 296, 222) (4, 4, 2)\n",
      "(106, 220, 107) (4, 4, 5)\n",
      "(67, 342, 222) (4, 4, 2)\n",
      "(3, 189, 68) (3, 3, 5)\n",
      "(128, 170, 93) (4, 4, 5)\n",
      "(214, 358, 159) (3, 3, 2)\n",
      "(95, 345, 77) (1, 1, 5)\n",
      "(107, 16, 35) (5, 5, 4)\n",
      "(214, 134, 390) (3, 3, 2)\n",
      "(152, 1, 70) (5, 5, 2)\n",
      "(162, 128, 252) (4, 4, 5)\n",
      "(86, 128, 199) (4, 4, 5)\n",
      "(176, 28, 95) (5, 5, 1)\n",
      "(87, 256, 105) (4, 4, 1)\n",
      "(371, 28, 342) (5, 5, 4)\n",
      "(59, 103, 37) (4, 4, 2)\n",
      "(269, 152, 13) (5, 5, 3)\n",
      "(204, 354, 364) (4, 4, 1)\n",
      "(329, 74, 343) (2, 2, 6)\n",
      "(270, 13, 41) (3, 3, 8)\n",
      "(247, 32, 327) (5, 5, 4)\n",
      "(307, 69, 63) (5, 5, 4)\n",
      "(134, 24, 222) (3, 3, 2)\n",
      "(332, 142, 176) (1, 1, 5)\n",
      "(145, 269, 167) (5, 5, 2)\n",
      "(128, 336, 109) (4, 4, 3)\n",
      "(103, 120, 28) (4, 4, 5)\n",
      "(354, 190, 252) (4, 4, 5)\n",
      "(390, 394, 35) (2, 2, 4)\n",
      "(82, 296, 172) (4, 4, 5)\n",
      "(218, 332, 41) (1, 1, 8)\n",
      "(111, 126, 44) (2, 2, 4)\n",
      "(362, 271, 314) (5, 5, 2)\n",
      "(199, 393, 103) (5, 5, 4)\n",
      "(342, 220, 330) (4, 4, 3)\n",
      "(390, 289, 69) (2, 2, 5)\n",
      "(128, 23, 293) (4, 4, 2)\n",
      "(129, 284, 362) (4, 4, 5)\n",
      "(281, 79, 159) (4, 4, 2)\n",
      "(1, 130, 96) (5, 5, 4)\n",
      "(249, 3, 135) (3, 3, 1)\n",
      "(239, 93, 84) (5, 5, 4)\n",
      "(392, 398, 103) (5, 5, 4)\n",
      "(164, 109, 130) (3, 3, 5)\n",
      "(1, 376, 285) (5, 5, 4)\n",
      "(176, 60, 222) (5, 5, 2)\n",
      "(310, 296, 271) (4, 4, 5)\n",
      "(129, 170, 134) (4, 4, 3)\n",
      "(215, 28, 24) (5, 5, 3)\n",
      "(172, 247, 25) (5, 5, 4)\n",
      "(152, 68, 345) (5, 5, 1)\n",
      "(336, 30, 294) (4, 4, 5)\n",
      "(329, 289, 98) (2, 2, 1)\n",
      "(247, 307, 149) (5, 5, 4)\n",
      "(120, 204, 293) (4, 4, 2)\n",
      "(376, 271, 304) (5, 5, 2)\n",
      "(16, 272, 354) (5, 5, 4)\n",
      "(364, 218, 106) (1, 1, 4)\n",
      "(231, 126, 123) (2, 2, 3)\n",
      "(103, 34, 6) (4, 4, 3)\n",
      "(172, 376, 49) (5, 5, 3)\n",
      "(168, 167, 79) (2, 2, 4)\n",
      "(93, 271, 359) (5, 5, 4)\n",
      "(314, 167, 340) (2, 2, 4)\n",
      "(24, 171, 329) (3, 3, 2)\n",
      "(354, 256, 6) (4, 4, 3)\n",
      "(29, 328, 359) (2, 2, 4)\n",
      "(342, 190, 183) (4, 4, 3)\n",
      "(284, 204, 176) (4, 4, 5)\n",
      "(398, 269, 13) (5, 5, 3)\n",
      "(275, 332, 309) (1, 1, 3)\n",
      "(358, 3, 307) (3, 3, 5)\n",
      "(23, 281, 24) (4, 4, 3)\n",
      "(266, 304, 84) (2, 2, 4)\n",
      "(310, 122, 232) (4, 4, 6)\n",
      "(126, 159, 281) (2, 2, 4)\n",
      "(370, 6, 122) (3, 3, 4)\n",
      "(183, 6, 340) (3, 3, 4)\n",
      "(109, 6, 190) (3, 3, 4)\n",
      "(284, 173, 328) (4, 4, 2)\n",
      "(162, 359, 361) (4, 4, 2)\n",
      "(371, 26, 128) (5, 5, 4)\n",
      "(359, 35, 218) (4, 4, 1)\n",
      "(73, 332, 109) (1, 1, 3)\n",
      "(70, 74, 296) (2, 2, 4)\n",
      "(49, 134, 281) (3, 3, 4)\n",
      "(330, 171, 186) (3, 3, 5)\n",
      "(123, 49, 218) (3, 3, 1)\n",
      "(249, 109, 173) (3, 3, 4)\n",
      "(69, 252, 82) (5, 5, 4)\n",
      "(294, 60, 285) (5, 5, 4)\n",
      "(189, 249, 68) (3, 3, 5)\n",
      "(44, 220, 49) (4, 4, 3)\n",
      "(1, 186, 87) (5, 5, 4)\n",
      "(106, 59, 109) (4, 4, 3)\n",
      "(361, 168, 204) (2, 2, 4)\n",
      "(93, 176, 86) (5, 5, 4)\n",
      "(0, 82, 345) (4, 4, 1)\n",
      "(94, 239, 129) (5, 5, 4)\n",
      "(361, 111, 219) (2, 2, 1)\n",
      "(329, 390, 264) (2, 2, 6)\n",
      "(309, 49, 26) (3, 3, 5)\n",
      "(130, 247, 25) (5, 5, 4)\n",
      "(143, 49, 37) (3, 3, 2)\n",
      "(281, 173, 47) (4, 4, 1)\n",
      "(59, 284, 271) (4, 4, 5)\n",
      "(60, 32, 135) (5, 5, 1)\n",
      "(392, 239, 182) (5, 5, 2)\n",
      "(129, 173, 49) (4, 4, 3)\n",
      "(77, 215, 30) (5, 5, 4)\n",
      "(1, 172, 106) (5, 5, 4)\n",
      "(256, 342, 69) (4, 4, 5)\n",
      "(329, 159, 327) (2, 2, 4)\n",
      "(28, 294, 220) (5, 5, 4)\n",
      "(340, 342, 370) (4, 4, 3)\n",
      "(390, 231, 1) (2, 2, 5)\n",
      "(6, 123, 264) (3, 3, 6)\n",
      "(51, 192, 58) (4, 4, 5)\n",
      "(84, 284, 392) (4, 4, 5)\n",
      "(219, 345, 19) (1, 1, 8)\n",
      "(23, 82, 358) (4, 4, 3)\n",
      "(172, 32, 159) (5, 5, 2)\n",
      "(95, 219, 304) (1, 1, 2)\n",
      "(87, 120, 146) (4, 4, 2)\n",
      "(165, 329, 214) (2, 2, 3)\n",
      "(30, 170, 77) (4, 4, 5)\n",
      "(30, 162, 266) (4, 4, 2)\n",
      "(328, 159, 41) (2, 2, 8)\n",
      "(25, 359, 392) (4, 4, 5)\n",
      "(239, 307, 44) (5, 5, 4)\n",
      "(268, 120, 109) (4, 4, 3)\n",
      "(257, 341, 47) (8, 8, 1)\n",
      "(329, 70, 239) (2, 2, 5)\n",
      "(204, 220, 390) (4, 4, 2)\n",
      "(392, 271, 51) (5, 5, 4)\n",
      "(307, 271, 222) (5, 5, 2)\n",
      "(214, 109, 314) (3, 3, 2)\n",
      "(30, 220, 219) (4, 4, 1)\n",
      "(269, 199, 126) (5, 5, 2)\n",
      "(296, 129, 145) (4, 4, 5)\n",
      "(285, 296, 232) (4, 4, 6)\n",
      "(25, 67, 130) (4, 4, 5)\n",
      "(298, 323, 342) (3, 3, 4)\n",
      "(0, 79, 37) (4, 4, 2)\n",
      "(214, 3, 58) (3, 3, 5)\n",
      "(190, 103, 74) (4, 4, 2)\n",
      "(50, 264, 215) (6, 6, 5)\n",
      "(33, 29, 323) (2, 2, 3)\n",
      "(222, 159, 170) (2, 2, 4)\n",
      "(336, 129, 293) (4, 4, 2)\n",
      "(398, 69, 189) (5, 5, 3)\n",
      "(247, 215, 67) (5, 5, 4)\n",
      "(51, 281, 33) (4, 4, 2)\n",
      "(392, 271, 158) (5, 5, 1)\n",
      "(370, 298, 146) (3, 3, 2)\n",
      "(162, 34, 142) (4, 4, 1)\n",
      "(94, 294, 323) (5, 5, 3)\n",
      "(189, 358, 219) (3, 3, 1)\n",
      "(215, 294, 190) (5, 5, 4)\n",
      "(296, 51, 41) (4, 4, 8)\n",
      "(79, 354, 266) (4, 4, 2)\n",
      "(34, 256, 168) (4, 4, 2)\n",
      "(170, 340, 358) (4, 4, 3)\n",
      "(358, 330, 132) (3, 3, 7)\n",
      "(107, 294, 87) (5, 5, 4)\n",
      "(96, 103, 164) (4, 4, 3)\n",
      "(173, 86, 214) (4, 4, 3)\n",
      "(23, 170, 214) (4, 4, 3)\n",
      "(24, 214, 296) (3, 3, 4)\n",
      "(358, 24, 29) (3, 3, 2)\n",
      "(93, 252, 29) (5, 5, 2)\n",
      "(94, 16, 296) (5, 5, 4)\n",
      "(51, 103, 189) (4, 4, 3)\n",
      "(314, 222, 16) (2, 2, 5)\n",
      "(310, 25, 132) (4, 4, 7)\n",
      "(284, 34, 270) (4, 4, 3)\n",
      "(59, 103, 16) (4, 4, 5)\n",
      "(28, 176, 314) (5, 5, 2)\n",
      "(98, 364, 256) (1, 1, 4)\n",
      "(231, 289, 189) (2, 2, 3)\n",
      "(25, 268, 269) (4, 4, 5)\n",
      "(122, 359, 158) (4, 4, 1)\n",
      "(93, 94, 25) (5, 5, 4)\n",
      "(247, 376, 23) (5, 5, 4)\n",
      "(189, 143, 354) (3, 3, 4)\n",
      "(142, 98, 24) (1, 1, 3)\n",
      "(340, 79, 24) (4, 4, 3)\n",
      "(96, 0, 215) (4, 4, 5)\n",
      "(74, 29, 103) (2, 2, 4)\n",
      "(281, 150, 143) (4, 4, 3)\n",
      "(164, 189, 314) (3, 3, 2)\n",
      "(47, 73, 32) (1, 1, 5)\n",
      "(275, 158, 190) (1, 1, 4)\n",
      "(103, 204, 123) (4, 4, 3)\n",
      "(122, 285, 343) (4, 4, 6)\n",
      "(162, 96, 257) (4, 4, 8)\n",
      "(268, 327, 171) (4, 4, 3)\n",
      "(296, 340, 272) (4, 4, 5)\n",
      "(170, 268, 13) (4, 4, 3)\n",
      "(35, 129, 69) (4, 4, 5)\n",
      "(392, 69, 13) (5, 5, 3)\n",
      "(247, 26, 296) (5, 5, 4)\n",
      "(0, 285, 328) (4, 4, 2)\n",
      "(215, 392, 44) (5, 5, 4)\n",
      "(336, 35, 123) (4, 4, 3)\n",
      "(343, 232, 269) (6, 6, 5)\n",
      "(293, 111, 307) (2, 2, 5)\n",
      "(126, 167, 143) (2, 2, 3)\n",
      "(50, 83, 68) (6, 6, 5)\n",
      "(35, 327, 294) (4, 4, 5)\n",
      "(25, 82, 271) (4, 4, 5)\n",
      "(310, 190, 145) (4, 4, 5)\n",
      "(252, 307, 146) (5, 5, 2)\n",
      "(73, 332, 165) (1, 1, 2)\n",
      "(296, 120, 107) (4, 4, 5)\n",
      "(247, 272, 285) (5, 5, 4)\n",
      "(150, 67, 107) (4, 4, 5)\n",
      "(343, 232, 159) (6, 6, 2)\n",
      "(376, 239, 143) (5, 5, 3)\n",
      "(30, 354, 390) (4, 4, 2)\n",
      "(29, 165, 24) (2, 2, 3)\n",
      "(30, 82, 332) (4, 4, 1)\n",
      "(129, 44, 58) (4, 4, 5)\n",
      "(340, 106, 330) (4, 4, 3)\n",
      "(106, 296, 249) (4, 4, 3)\n",
      "(359, 0, 16) (4, 4, 5)\n",
      "(58, 247, 164) (5, 5, 3)\n",
      "(215, 28, 79) (5, 5, 4)\n",
      "(149, 220, 330) (4, 4, 3)\n",
      "(354, 327, 298) (4, 4, 3)\n",
      "(84, 44, 172) (4, 4, 5)\n",
      "(135, 364, 120) (1, 1, 4)\n",
      "(231, 304, 252) (2, 2, 5)\n",
      "(77, 272, 86) (5, 5, 4)\n",
      "(152, 176, 149) (5, 5, 4)\n",
      "(86, 220, 164) (4, 4, 3)\n",
      "(170, 25, 304) (4, 4, 2)\n",
      "(327, 340, 95) (4, 4, 1)\n",
      "(307, 272, 289) (5, 5, 2)\n",
      "(249, 370, 289) (3, 3, 2)\n",
      "(122, 103, 83) (4, 4, 6)\n",
      "(314, 293, 93) (2, 2, 5)\n",
      "(392, 371, 204) (5, 5, 4)\n",
      "(170, 106, 158) (4, 4, 1)\n",
      "(93, 32, 170) (5, 5, 4)\n",
      "(82, 281, 314) (4, 4, 2)\n",
      "(398, 393, 50) (5, 5, 6)\n",
      "(146, 361, 73) (2, 2, 1)\n",
      "(70, 74, 330) (2, 2, 3)\n",
      "(129, 122, 294) (4, 4, 5)\n",
      "(49, 358, 68) (3, 3, 5)\n",
      "(34, 79, 142) (4, 4, 1)\n",
      "(93, 107, 126) (5, 5, 2)\n",
      "(275, 158, 281) (1, 1, 4)\n",
      "(284, 354, 364) (4, 4, 1)\n",
      "(275, 47, 289) (1, 1, 2)\n",
      "(220, 84, 266) (4, 4, 2)\n",
      "(35, 87, 165) (4, 4, 2)\n",
      "(281, 86, 361) (4, 4, 2)\n",
      "(26, 239, 159) (5, 5, 2)\n",
      "(96, 128, 24) (4, 4, 3)\n",
      "(70, 314, 393) (2, 2, 5)\n",
      "(120, 63, 165) (4, 4, 2)\n",
      "(272, 28, 162) (5, 5, 4)\n",
      "(214, 24, 32) (3, 3, 5)\n",
      "(95, 135, 270) (1, 1, 3)\n",
      "(328, 165, 164) (2, 2, 3)\n",
      "(152, 376, 143) (5, 5, 3)\n",
      "(70, 182, 285) (2, 2, 4)\n",
      "(170, 128, 167) (4, 4, 2)\n",
      "(135, 98, 103) (1, 1, 4)\n",
      "(94, 199, 264) (5, 5, 6)\n",
      "(146, 314, 392) (2, 2, 5)\n",
      "(269, 26, 346) (5, 5, 3)\n",
      "(296, 51, 98) (4, 4, 1)\n",
      "(329, 319, 63) (2, 2, 4)\n",
      "(164, 298, 77) (3, 3, 5)\n",
      "(268, 281, 215) (4, 4, 5)\n",
      "(26, 186, 222) (5, 5, 2)\n",
      "(323, 24, 83) (3, 3, 6)\n",
      "(59, 284, 341) (4, 4, 8)\n",
      "(271, 1, 134) (5, 5, 3)\n",
      "(0, 84, 165) (4, 4, 2)\n",
      "(393, 60, 182) (5, 5, 2)\n",
      "(336, 103, 252) (4, 4, 5)\n",
      "(354, 82, 275) (4, 4, 1)\n",
      "(59, 220, 126) (4, 4, 2)\n",
      "(285, 170, 232) (4, 4, 6)\n",
      "(393, 199, 168) (5, 5, 2)\n",
      "(239, 1, 126) (5, 5, 2)\n",
      "(87, 129, 158) (4, 4, 1)\n",
      "(281, 23, 298) (4, 4, 3)\n",
      "(359, 0, 394) (4, 4, 2)\n",
      "(272, 32, 336) (5, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(triplets)\n",
    "for j in triplets[:300]:\n",
    "    print(j, (classes[j[0]], classes[j[1]], classes[j[2]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.97291649 2.62597292 3.35117894]\n",
      "Epoch 0, loss: 2.6500227830626746, windowed_loss: 50\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.6397009  1.06935961 1.27433137]\n",
      "Epoch 1, loss: 0.9944639637163192, windowed_loss: 50\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.33672254 0.69358687 0.80902015]\n",
      "Epoch 2, loss: 0.6131098525542201, windowed_loss: 50\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.12381017 0.2845535  0.31451303]\n",
      "Epoch 3, loss: 0.24095889937245485, windowed_loss: 0.6161775718809981\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.19308402 0.44354072 0.53319698]\n",
      "Epoch 4, loss: 0.3899405720559033, windowed_loss: 0.4146697746608594\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.1659559  0.35433937 0.55721182]\n",
      "Epoch 5, loss: 0.35916903016693663, windowed_loss: 0.3300228338650983\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.09218929 0.19422646 0.31549369]\n",
      "Epoch 6, loss: 0.2006364824871222, windowed_loss: 0.31658202823665405\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.07539225 0.15070678 0.30964756]\n",
      "Epoch 7, loss: 0.17858219615211993, windowed_loss: 0.24612923626872626\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.08625457 0.14351881 0.44379109]\n",
      "Epoch 8, loss: 0.22452149054769313, windowed_loss: 0.20124672306231176\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.06210242 0.0920901  0.28815167]\n",
      "Epoch 9, loss: 0.1474480609776396, windowed_loss: 0.18351724922581755\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.05061474 0.0812984  0.27595188]\n",
      "Epoch 10, loss: 0.1359550099481236, windowed_loss: 0.16930818715781878\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.02270978 0.07143713 0.08122764]\n",
      "Epoch 11, loss: 0.05845818552894122, windowed_loss: 0.11395375215156815\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.03569701 0.07272463 0.22632592]\n",
      "Epoch 12, loss: 0.11158252117986028, windowed_loss: 0.10199857221897503\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.02699829 0.0551636  0.15572446]\n",
      "Epoch 13, loss: 0.07929545051107804, windowed_loss: 0.08311205240662652\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.02203761 0.0747809  0.17357029]\n",
      "Epoch 14, loss: 0.09012959973717279, windowed_loss: 0.09366919047603704\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.02281759 0.04872602 0.15733549]\n",
      "Epoch 15, loss: 0.07629303447902203, windowed_loss: 0.08190602824242428\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0213878  0.0457727  0.14267661]\n",
      "Epoch 16, loss: 0.06994570239983273, windowed_loss: 0.07878944553867584\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.01340874 0.05434008 0.10247194]\n",
      "Epoch 17, loss: 0.05674025441773913, windowed_loss: 0.0676596637655313\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.01284889 0.04280269 0.08684221]\n",
      "Epoch 18, loss: 0.04749793183961601, windowed_loss: 0.05806129621906262\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.01185952 0.04102283 0.09232005]\n",
      "Epoch 19, loss: 0.048400802029804756, windowed_loss: 0.05087966276238664\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.01148316 0.03960601 0.07753798]\n",
      "Epoch 20, loss: 0.04287571778916049, windowed_loss: 0.04625815055286042\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00598486 0.0463145  0.05062968]\n",
      "Epoch 21, loss: 0.034309677836118324, windowed_loss: 0.041862065885027853\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.01136225 0.02904993 0.07298604]\n",
      "Epoch 22, loss: 0.037799406881359486, windowed_loss: 0.03832826750221277\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00761211 0.03266771 0.06566718]\n",
      "Epoch 23, loss: 0.035315665642194675, windowed_loss: 0.03580825011989083\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00755712 0.03011357 0.06170117]\n",
      "Epoch 24, loss: 0.033123954933023815, windowed_loss: 0.035413009152192663\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00718408 0.03773359 0.04997967]\n",
      "Epoch 25, loss: 0.031632443692422275, windowed_loss: 0.03335735475588025\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00779731 0.02779659 0.04918462]\n",
      "Epoch 26, loss: 0.028259506618434734, windowed_loss: 0.031005301747960276\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00598959 0.02663883 0.03686653]\n",
      "Epoch 27, loss: 0.023164985814329353, windowed_loss: 0.02768564537506212\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00591447 0.01626365 0.04715966]\n",
      "Epoch 28, loss: 0.02311259474266659, windowed_loss: 0.02484569572514356\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00647763 0.02811965 0.03972775]\n",
      "Epoch 29, loss: 0.024775007034115715, windowed_loss: 0.023684195863703888\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00608862 0.02636874 0.03401978]\n",
      "Epoch 30, loss: 0.022159047424793243, windowed_loss: 0.02334888306719185\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00586195 0.022035   0.03452746]\n",
      "Epoch 31, loss: 0.020808138376609848, windowed_loss: 0.022580730945172933\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00416877 0.02290653 0.02904092]\n",
      "Epoch 32, loss: 0.018705408850854092, windowed_loss: 0.020557531550752392\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00599985 0.02098905 0.02855598]\n",
      "Epoch 33, loss: 0.01851496348778407, windowed_loss: 0.019342836905082667\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00336875 0.0166158  0.02212104]\n",
      "Epoch 34, loss: 0.014035195634333475, windowed_loss: 0.017085189324323877\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00371084 0.02476924 0.02834093]\n",
      "Epoch 35, loss: 0.018940338102931328, windowed_loss: 0.017163499075016294\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00214288 0.01851134 0.02352312]\n",
      "Epoch 36, loss: 0.014725782287617525, windowed_loss: 0.015900438674960778\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00552911 0.02364268 0.02011402]\n",
      "Epoch 37, loss: 0.01642860390358802, windowed_loss: 0.01669824143137896\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00279885 0.02229571 0.02281165]\n",
      "Epoch 38, loss: 0.01596873456781561, windowed_loss: 0.015707706919673717\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0025091  0.02737074 0.02207901]\n",
      "Epoch 39, loss: 0.01731961982494051, windowed_loss: 0.01657231943211471\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00233768 0.02791403 0.02025445]\n",
      "Epoch 40, loss: 0.016835386328624958, windowed_loss: 0.016707913573793694\n",
      "LR: [0.01, 0.01, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00230469 0.02524788 0.02085167]\n",
      "Epoch 41, loss: 0.016134746852471973, windowed_loss: 0.01676325100201248\n",
      "Epoch 00042: reducing learning rate of group 0 to 8.0000e-03.\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00198011 0.01867063 0.01616974]\n",
      "Epoch 42, loss: 0.012273493900217794, windowed_loss: 0.015081209027104907\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00258085 0.01991661 0.01557564]\n",
      "Epoch 43, loss: 0.01269103397587032, windowed_loss: 0.013699758242853362\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00161037 0.01307514 0.0150565 ]\n",
      "Epoch 44, loss: 0.009914001816827238, windowed_loss: 0.011626176564305117\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00236586 0.02004255 0.02009284]\n",
      "Epoch 45, loss: 0.014167081395333464, windowed_loss: 0.01225737239601034\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00107272 0.0236253  0.01241081]\n",
      "Epoch 46, loss: 0.012369607010799826, windowed_loss: 0.012150230074320175\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00157338 0.0259203  0.0162619 ]\n",
      "Epoch 47, loss: 0.014585192004839579, windowed_loss: 0.013707293470324289\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00254559 0.0217112  0.01885465]\n",
      "Epoch 48, loss: 0.014370479536327451, windowed_loss: 0.013775092850655618\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0014359  0.01973678 0.01422636]\n",
      "Epoch 49, loss: 0.011799680548861172, windowed_loss: 0.013585117363342733\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0023854  0.01788067 0.01330899]\n",
      "Epoch 50, loss: 0.011191689973753511, windowed_loss: 0.01245395001964738\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00091656 0.02311605 0.0100526 ]\n",
      "Epoch 51, loss: 0.011361734885157961, windowed_loss: 0.011451035135924215\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00168807 0.02221403 0.01432176]\n",
      "Epoch 52, loss: 0.012741286110020044, windowed_loss: 0.011764903656310506\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00238483 0.01735795 0.00846064]\n",
      "Epoch 53, loss: 0.009401141270769365, windowed_loss: 0.011168054088649125\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00132822 0.0169453  0.01254687]\n",
      "Epoch 54, loss: 0.01027346472961433, windowed_loss: 0.010805297370134581\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00224907 0.01515896 0.01048922]\n",
      "Epoch 55, loss: 0.009299083133087013, windowed_loss: 0.00965789637782357\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00148438 0.01299031 0.01146957]\n",
      "Epoch 56, loss: 0.008648086988338919, windowed_loss: 0.009406878283680087\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00167289 0.01546344 0.0121676 ]\n",
      "Epoch 57, loss: 0.009767973202873361, windowed_loss: 0.009238381108099764\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00183602 0.01833726 0.01191083]\n",
      "Epoch 58, loss: 0.010694703990311333, windowed_loss: 0.00970358806050787\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00210213 0.01481398 0.00899119]\n",
      "Epoch 59, loss: 0.008635764094916258, windowed_loss: 0.009699480429366985\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00175889 0.01493444 0.0097109 ]\n",
      "Epoch 60, loss: 0.008801408309602377, windowed_loss: 0.00937729213160999\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00061771 0.01599406 0.00848832]\n",
      "Epoch 61, loss: 0.008366697989968641, windowed_loss: 0.008601290131495758\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00193616 0.01653712 0.00659143]\n",
      "Epoch 62, loss: 0.008354901731240027, windowed_loss: 0.008507669343603681\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00188983 0.01507533 0.00841293]\n",
      "Epoch 63, loss: 0.008459363048049536, windowed_loss: 0.0083936542564194\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00030647 0.0116963  0.00821463]\n",
      "Epoch 64, loss: 0.006739131824085206, windowed_loss: 0.007851132201124923\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00073974 0.01575208 0.00890921]\n",
      "Epoch 65, loss: 0.008467008698393, windowed_loss: 0.007888501190175914\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00055738 0.01300248 0.00912654]\n",
      "Epoch 66, loss: 0.007562132439378536, windowed_loss: 0.007589424320618914\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00065938 0.01342405 0.00821673]\n",
      "Epoch 67, loss: 0.007433386209110419, windowed_loss: 0.007820842448960651\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00132327 0.01596553 0.01083517]\n",
      "Epoch 68, loss: 0.009374655077629017, windowed_loss: 0.008123391242039325\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00027082 0.01436666 0.00685952]\n",
      "Epoch 69, loss: 0.007165665964059757, windowed_loss: 0.007991235750266399\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00047138 0.01653392 0.00706494]\n",
      "Epoch 70, loss: 0.008023410273546522, windowed_loss: 0.008187910438411765\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00113488 0.00863593 0.00689579]\n",
      "Epoch 71, loss: 0.005555535723088365, windowed_loss: 0.006914870653564882\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00046309 0.01181538 0.0066233 ]\n",
      "Epoch 72, loss: 0.00630058983171528, windowed_loss: 0.0066265119427833885\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00109038 0.01755827 0.00734027]\n",
      "Epoch 73, loss: 0.008662972401714686, windowed_loss: 0.006839699318839443\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.001353   0.00937921 0.00776693]\n",
      "Epoch 74, loss: 0.006166382297647721, windowed_loss: 0.007043314843692562\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00035047 0.00849447 0.00654992]\n",
      "Epoch 75, loss: 0.005131618599548485, windowed_loss: 0.006653657766303631\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0009878 0.0129965 0.0055939]\n",
      "Epoch 76, loss: 0.006526065933884997, windowed_loss: 0.005941355610360401\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0011397  0.00972875 0.00626537]\n",
      "Epoch 77, loss: 0.005711273149107442, windowed_loss: 0.005789652560846975\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.000157   0.00644563 0.00532639]\n",
      "Epoch 78, loss: 0.00397633964365179, windowed_loss: 0.005404559575548077\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00022447 0.01055075 0.00582618]\n",
      "Epoch 79, loss: 0.00553380206904628, windowed_loss: 0.005073804953935171\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00053291 0.00906859 0.00518375]\n",
      "Epoch 80, loss: 0.0049284171369491205, windowed_loss: 0.004812852949882397\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00018128 0.00913806 0.00423458]\n",
      "Epoch 81, loss: 0.004517973598205682, windowed_loss: 0.00499339760140036\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0007969  0.01041694 0.00770502]\n",
      "Epoch 82, loss: 0.006306286477907138, windowed_loss: 0.00525089240435398\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00151751 0.0166111  0.00531162]\n",
      "Epoch 83, loss: 0.007813408575726278, windowed_loss: 0.0062125562172797\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00058393 0.01099433 0.00679188]\n",
      "Epoch 84, loss: 0.006123379437309323, windowed_loss: 0.006747691496980912\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00037808 0.00989572 0.00711448]\n",
      "Epoch 85, loss: 0.005796092759930725, windowed_loss: 0.006577626924322109\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00051829 0.01421176 0.00785271]\n",
      "Epoch 86, loss: 0.007527588330435031, windowed_loss: 0.0064823535092250265\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00072081 0.0103214  0.00704731]\n",
      "Epoch 87, loss: 0.006029842054527817, windowed_loss: 0.0064511743816311915\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0008189  0.01365127 0.00557878]\n",
      "Epoch 88, loss: 0.006682983204496629, windowed_loss: 0.0067468045298198255\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00069908 0.00750477 0.00627943]\n",
      "Epoch 89, loss: 0.004827756378235238, windowed_loss: 0.005846860545753227\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00069256 0.00371272 0.00246036]\n",
      "Epoch 90, loss: 0.002288548783822493, windowed_loss: 0.004599762788851453\n",
      "LR: [0.01, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00044104 0.00550322 0.00278966]\n",
      "Epoch 91, loss: 0.002911305224353617, windowed_loss: 0.0033425367954704496\n",
      "Epoch 00092: reducing learning rate of group 0 to 8.0000e-03.\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00012666 0.00846346 0.00347367]\n",
      "Epoch 92, loss: 0.004021264640896609, windowed_loss: 0.003073706216357573\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00017329 0.00673119 0.0045297 ]\n",
      "Epoch 93, loss: 0.003811395021550583, windowed_loss: 0.0035813216289336033\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00035606 0.01208027 0.00490768]\n",
      "Epoch 94, loss: 0.005781336377064387, windowed_loss: 0.004537998679837194\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00086502 0.00957314 0.00396898]\n",
      "Epoch 95, loss: 0.0048023782331835145, windowed_loss: 0.004798369877266162\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00056221 0.01068608 0.00565217]\n",
      "Epoch 96, loss: 0.005633485870379391, windowed_loss: 0.005405733493542431\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00059337 0.01061251 0.00429269]\n",
      "Epoch 97, loss: 0.005166189829734239, windowed_loss: 0.00520068464443238\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0004732  0.01243258 0.00268616]\n",
      "Epoch 98, loss: 0.005197314743065473, windowed_loss: 0.0053323301477263664\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00033556 0.01101743 0.00449904]\n",
      "Epoch 99, loss: 0.005284012267083833, windowed_loss: 0.005215838946627849\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00082907 0.00587041 0.00285817]\n",
      "Epoch 100, loss: 0.003185881358204466, windowed_loss: 0.00455573612278459\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [5.86889007e-05 8.49684074e-03 5.93101047e-03]\n",
      "Epoch 101, loss: 0.004828846702973048, windowed_loss: 0.0044329134427537826\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00055272 0.00438452 0.00446864]\n",
      "Epoch 102, loss: 0.0031352913797353253, windowed_loss: 0.003716673146970946\n",
      "LR: [0.008, 0.008, 0.01]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00041056 0.00948233 0.00579623]\n",
      "Epoch 103, loss: 0.005229706452651456, windowed_loss: 0.004397948178453276\n",
      "Epoch 00104: reducing learning rate of group 0 to 6.4000e-03.\n",
      "Epoch 00104: reducing learning rate of group 0 to 8.0000e-03.\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00058284 0.01285126 0.00430685]\n",
      "Epoch 104, loss: 0.005913648359251744, windowed_loss: 0.004759548730546175\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [8.09282064e-05 5.23632782e-03 2.40278972e-03]\n",
      "Epoch 105, loss: 0.002573348581790924, windowed_loss: 0.004572234464564708\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0002727  0.00923658 0.00280316]\n",
      "Epoch 106, loss: 0.004104145605004195, windowed_loss: 0.004197047515348954\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00029997 0.00608221 0.0042106 ]\n",
      "Epoch 107, loss: 0.0035309293166254506, windowed_loss: 0.0034028078344735236\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00025773 0.01147753 0.00505046]\n",
      "Epoch 108, loss: 0.005595238371328874, windowed_loss: 0.004410104430986173\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00225557 0.00261424]\n",
      "Epoch 109, loss: 0.0016232707509488773, windowed_loss: 0.0035831461463010673\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00068782 0.00678527 0.00334797]\n",
      "Epoch 110, loss: 0.003607016846989141, windowed_loss: 0.003608508656422297\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00024723 0.00747525 0.00359794]\n",
      "Epoch 111, loss: 0.0037734723000815425, windowed_loss: 0.003001253299339854\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00038792 0.00761806 0.00206292]\n",
      "Epoch 112, loss: 0.003356302213488203, windowed_loss: 0.003578930453519629\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0005486  0.01100352 0.00314302]\n",
      "Epoch 113, loss: 0.004898381927473979, windowed_loss: 0.004009385480347908\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0004124  0.00984193 0.0014687 ]\n",
      "Epoch 114, loss: 0.003907675459755189, windowed_loss: 0.0040541198669057895\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00051804 0.00698245 0.00121558]\n",
      "Epoch 115, loss: 0.002905360451250365, windowed_loss: 0.0039038059461598443\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00076683 0.00757903 0.00404282]\n",
      "Epoch 116, loss: 0.004129563147823016, windowed_loss: 0.0036475330196095238\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.23834068e-05 9.98223141e-03 1.54183974e-03]\n",
      "Epoch 117, loss: 0.0038488181872349796, windowed_loss: 0.0036279139287694537\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00016007 0.00646188 0.00570681]\n",
      "Epoch 118, loss: 0.004109582468641526, windowed_loss: 0.00402932126789984\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [5.31761484e-05 5.00000285e-03 3.31073661e-03]\n",
      "Epoch 119, loss: 0.002787971869111061, windowed_loss: 0.003582124174995856\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00038744 0.00438951 0.00304506]\n",
      "Epoch 120, loss: 0.002607333614970699, windowed_loss: 0.003168295984241095\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00044271 0.00597401 0.00236459]\n",
      "Epoch 121, loss: 0.0029271026000832067, windowed_loss: 0.002774136028054989\n",
      "LR: [0.008, 0.0064, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.47416508e-05 8.32220438e-03 4.23729606e-03]\n",
      "Epoch 122, loss: 0.004208080696337151, windowed_loss: 0.003247505637130352\n",
      "Epoch 00123: reducing learning rate of group 0 to 6.4000e-03.\n",
      "Epoch 00123: reducing learning rate of group 0 to 5.1200e-03.\n",
      "LR: [0.0064, 0.00512, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [7.65211880e-05 8.97327878e-03 3.87987342e-03]\n",
      "Epoch 123, loss: 0.00430989113043655, windowed_loss: 0.0038150248089523025\n",
      "LR: [0.0064, 0.00512, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0002331  0.00628311 0.00228195]\n",
      "Epoch 124, loss: 0.0029327223698298135, windowed_loss: 0.0038168980655345045\n",
      "LR: [0.0064, 0.00512, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [7.48798590e-05 4.52085635e-03 3.34546749e-03]\n",
      "Epoch 125, loss: 0.002647067899956848, windowed_loss: 0.0032965604667410703\n",
      "LR: [0.0064, 0.00512, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00044203 0.00443208 0.00191984]\n",
      "Epoch 126, loss: 0.0022646499741258044, windowed_loss: 0.0026148134146374885\n",
      "LR: [0.0064, 0.00512, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00038681 0.00384714 0.00202212]\n",
      "Epoch 127, loss: 0.0020853557595700927, windowed_loss: 0.0023323578778842485\n",
      "LR: [0.0064, 0.00512, 0.008]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.0054446  0.00258535]\n",
      "Epoch 128, loss: 0.0026766522364182906, windowed_loss: 0.0023422193233713956\n",
      "Epoch 00129: reducing learning rate of group 0 to 6.4000e-03.\n",
      "LR: [0.0064, 0.00512, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00032149 0.00817374 0.00411446]\n",
      "Epoch 129, loss: 0.004203232287457496, windowed_loss: 0.002988413427815293\n",
      "LR: [0.0064, 0.00512, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.54186717e-05 6.89459829e-03 1.64458122e-03]\n",
      "Epoch 130, loss: 0.002854866060343656, windowed_loss: 0.003244916861406481\n",
      "LR: [0.0064, 0.00512, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00026861 0.00621116 0.00306356]\n",
      "Epoch 131, loss: 0.003181107793793534, windowed_loss: 0.0034130687138648947\n",
      "LR: [0.0064, 0.00512, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.70737166e-05 5.40805608e-03 1.97547082e-03]\n",
      "Epoch 132, loss: 0.0024702002040364528, windowed_loss: 0.0028353913527245475\n",
      "LR: [0.0064, 0.00512, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00027866 0.00405453 0.00170697]\n",
      "Epoch 133, loss: 0.0020133884567202945, windowed_loss: 0.002554898818183427\n",
      "LR: [0.0064, 0.00512, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00017359 0.00422915 0.001931  ]\n",
      "Epoch 134, loss: 0.002111245567599932, windowed_loss: 0.0021982780761188934\n",
      "LR: [0.0064, 0.00512, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0002073  0.00654888 0.00137996]\n",
      "Epoch 135, loss: 0.0027120476306387877, windowed_loss: 0.0022788938849863383\n",
      "Epoch 00136: reducing learning rate of group 0 to 5.1200e-03.\n",
      "Epoch 00136: reducing learning rate of group 0 to 4.0960e-03.\n",
      "LR: [0.00512, 0.004096000000000001, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00010991 0.00550423 0.00218223]\n",
      "Epoch 136, loss: 0.0025987883070201583, windowed_loss: 0.002474027168419626\n",
      "LR: [0.00512, 0.004096000000000001, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.03202392e-05 2.08623572e-03 1.73404491e-03]\n",
      "Epoch 137, loss: 0.0012935336227669861, windowed_loss: 0.0022014565201419773\n",
      "LR: [0.00512, 0.004096000000000001, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00059157 0.0057108  0.00131975]\n",
      "Epoch 138, loss: 0.0025407069390921883, windowed_loss: 0.002144342956293111\n",
      "LR: [0.00512, 0.004096000000000001, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00010969 0.0104996  0.00439005]\n",
      "Epoch 139, loss: 0.004999779842116616, windowed_loss: 0.0029446734679919304\n",
      "LR: [0.00512, 0.004096000000000001, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00039968 0.00667089 0.00191313]\n",
      "Epoch 140, loss: 0.002994569357145916, windowed_loss: 0.0035116853794515735\n",
      "LR: [0.00512, 0.004096000000000001, 0.0064]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [3.56002287e-06 3.51231799e-03 1.75507028e-03]\n",
      "Epoch 141, loss: 0.0017569827649629476, windowed_loss: 0.00325044398807516\n",
      "Epoch 00142: reducing learning rate of group 0 to 5.1200e-03.\n",
      "LR: [0.00512, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00641284 0.0021845 ]\n",
      "Epoch 142, loss: 0.0028657786209474907, windowed_loss: 0.0025391102476854514\n",
      "LR: [0.00512, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.79093328e-05 7.61498562e-03 1.67471170e-03]\n",
      "Epoch 143, loss: 0.003119202217820919, windowed_loss: 0.002580654534577119\n",
      "LR: [0.00512, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00018386 0.00213774 0.00170963]\n",
      "Epoch 144, loss: 0.0013437464155934074, windowed_loss: 0.0024429090847872723\n",
      "LR: [0.00512, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0001241 0.0034853 0.00226  ]\n",
      "Epoch 145, loss: 0.001956465964516004, windowed_loss: 0.0021398048659767768\n",
      "LR: [0.00512, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [5.39015640e-05 7.03000006e-03 1.37904219e-03]\n",
      "Epoch 146, loss: 0.0028209812713391852, windowed_loss: 0.002040397883816199\n",
      "LR: [0.00512, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00421319 0.00196   ]\n",
      "Epoch 147, loss: 0.002057729458267039, windowed_loss: 0.0022783922313740763\n",
      "LR: [0.00512, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00023273 0.00413311 0.00198889]\n",
      "Epoch 148, loss: 0.002118246912053137, windowed_loss: 0.0023323192138864535\n",
      "Epoch 00149: reducing learning rate of group 0 to 4.0960e-03.\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00026084 0.00515407 0.00214928]\n",
      "Epoch 149, loss: 0.002521399465022665, windowed_loss: 0.0022324586117809467\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0001364  0.00203025 0.0015927 ]\n",
      "Epoch 150, loss: 0.0012531151825731451, windowed_loss: 0.0019642538532163157\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [5.57725064e-05 4.19101153e-03 1.45890056e-03]\n",
      "Epoch 151, loss: 0.001901894864259344, windowed_loss: 0.001892136503951718\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [4.76190312e-05 1.74676813e-03 7.61683353e-04]\n",
      "Epoch 152, loss: 0.0008520235053517603, windowed_loss: 0.001335677850728083\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.50537285e-06 2.45701149e-03 1.89468620e-03]\n",
      "Epoch 153, loss: 0.0014527343552220952, windowed_loss: 0.0014022175749444\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00014988 0.00312349 0.00262642]\n",
      "Epoch 154, loss: 0.001966594351512013, windowed_loss: 0.0014237840706952897\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [9.18351791e-05 3.73017483e-03 1.29627927e-03]\n",
      "Epoch 155, loss: 0.001706096428361806, windowed_loss: 0.0017084750450319713\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00016572 0.0040757  0.00154073]\n",
      "Epoch 156, loss: 0.0019273823409369497, windowed_loss: 0.001866691040270256\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00018454 0.00184689 0.00089053]\n",
      "Epoch 157, loss: 0.000973989622610988, windowed_loss: 0.0015358227973032478\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.82063401e-05 4.12998061e-03 1.93904476e-03]\n",
      "Epoch 158, loss: 0.002045743901169661, windowed_loss: 0.001649038621572533\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [7.98575241e-05 4.61961058e-03 1.12789463e-03]\n",
      "Epoch 159, loss: 0.0019424542446028102, windowed_loss: 0.001654062589461153\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00021211 0.00631086 0.00183166]\n",
      "Epoch 160, loss: 0.002784878354180943, windowed_loss: 0.002257692166651138\n",
      "LR: [0.004096000000000001, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00017479 0.00278679 0.00107898]\n",
      "Epoch 161, loss: 0.0013468539850278332, windowed_loss: 0.002024728861270529\n",
      "Epoch 00162: reducing learning rate of group 0 to 3.2768e-03.\n",
      "LR: [0.0032768000000000007, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00019483 0.00279825 0.00165136]\n",
      "Epoch 162, loss: 0.0015481432730501349, windowed_loss: 0.0018932918707529704\n",
      "LR: [0.0032768000000000007, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00024685 0.00396813 0.00247619]\n",
      "Epoch 163, loss: 0.0022303911321090927, windowed_loss: 0.0017084627967290204\n",
      "LR: [0.0032768000000000007, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0002007  0.00326512 0.00088277]\n",
      "Epoch 164, loss: 0.001449532609320048, windowed_loss: 0.0017426890048264252\n",
      "LR: [0.0032768000000000007, 0.004096000000000001, 0.00512]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0002302  0.00390681 0.00205852]\n",
      "Epoch 165, loss: 0.002065179135763284, windowed_loss: 0.0019150342923974749\n",
      "Epoch 00166: reducing learning rate of group 0 to 3.2768e-03.\n",
      "Epoch 00166: reducing learning rate of group 0 to 4.0960e-03.\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.80090693e-05 4.49977053e-03 1.49676170e-03]\n",
      "Epoch 166, loss: 0.0020048470998352227, windowed_loss: 0.0018398529483061848\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.11579556e-06 3.67322242e-03 2.21090611e-03]\n",
      "Epoch 167, loss: 0.0019620814445343885, windowed_loss: 0.0020107025600442983\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00016432 0.00563137 0.00107492]\n",
      "Epoch 168, loss: 0.0022902011758450303, windowed_loss: 0.002085709906738214\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00024636 0.00456894 0.00317771]\n",
      "Epoch 169, loss: 0.002664336116250717, windowed_loss: 0.0023055395788767123\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00441974 0.00230282]\n",
      "Epoch 170, loss: 0.002240855021007133, windowed_loss: 0.002398464104367627\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.68858940e-07 3.53501483e-03 6.96784732e-04]\n",
      "Epoch 171, loss: 0.001410822807387872, windowed_loss: 0.0021053379815485743\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.50083477e-05 3.97779962e-03 3.00058807e-03]\n",
      "Epoch 172, loss: 0.0023477986793626437, windowed_loss: 0.001999825502585883\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00053728 0.00374422 0.00141118]\n",
      "Epoch 173, loss: 0.0018975592190117547, windowed_loss: 0.0018853935685874236\n",
      "LR: [0.0032768000000000007, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.0039621  0.00100681]\n",
      "Epoch 174, loss: 0.0016563034362413666, windowed_loss: 0.0019672204448719214\n",
      "Epoch 00175: reducing learning rate of group 0 to 2.6214e-03.\n",
      "LR: [0.002621440000000001, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00012438 0.00209648 0.00109401]\n",
      "Epoch 175, loss: 0.0011049527392694445, windowed_loss: 0.0015529384648408553\n",
      "LR: [0.002621440000000001, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00018208 0.00306555 0.00258145]\n",
      "Epoch 176, loss: 0.0019430277135336037, windowed_loss: 0.0015680946296814716\n",
      "LR: [0.002621440000000001, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00351475 0.00058948]\n",
      "Epoch 177, loss: 0.001368077556517991, windowed_loss: 0.0014720193364403465\n",
      "LR: [0.002621440000000001, 0.0032768000000000007, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00034342 0.00177984 0.00128226]\n",
      "Epoch 178, loss: 0.0011351701210845601, windowed_loss: 0.001482091797045385\n",
      "Epoch 00179: reducing learning rate of group 0 to 2.6214e-03.\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.07306984e-05 4.20323319e-03 1.40448101e-03]\n",
      "Epoch 179, loss: 0.0018894816325469449, windowed_loss: 0.0014642431033831654\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00244544 0.00071097]\n",
      "Epoch 180, loss: 0.001052136222521464, windowed_loss: 0.001358929325384323\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [9.01638784e-06 2.58585760e-03 1.19341961e-03]\n",
      "Epoch 181, loss: 0.0012627645311030476, windowed_loss: 0.0014014607953904855\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.77660788e-05 2.02581066e-03 1.25068799e-03]\n",
      "Epoch 182, loss: 0.0010980882427909157, windowed_loss: 0.0011376629988051423\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.01719049e-06 2.22261318e-03 5.79678369e-04]\n",
      "Epoch 183, loss: 0.0009361029122814988, windowed_loss: 0.001098985228725154\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00368687 0.00125874]\n",
      "Epoch 184, loss: 0.0016485394514871366, windowed_loss: 0.0012275768688531838\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.69816003e-05 1.39413672e-03 2.61608511e-04]\n",
      "Epoch 185, loss: 0.0005609089452208895, windowed_loss: 0.0010485171029965084\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00208081 0.0003415 ]\n",
      "Epoch 186, loss: 0.0008074347720001683, windowed_loss: 0.0010056277229027314\n",
      "LR: [0.002621440000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [9.57179476e-06 2.71537375e-03 6.69409267e-04]\n",
      "Epoch 187, loss: 0.0011314516040411863, windowed_loss: 0.0008332651070874147\n",
      "Epoch 00188: reducing learning rate of group 0 to 2.0972e-03.\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.07696447e-06 3.88164886e-03 1.48287432e-03]\n",
      "Epoch 188, loss: 0.0017902000496784847, windowed_loss: 0.0012430288085732797\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00015651 0.00189951 0.00114344]\n",
      "Epoch 189, loss: 0.0010664848554314988, windowed_loss: 0.0013293788363837233\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.21700151e-05 1.37703680e-03 6.66578385e-04]\n",
      "Epoch 190, loss: 0.0007019283983743552, windowed_loss: 0.0011862044344947795\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00000000e+00 8.61472874e-04 6.23806634e-05]\n",
      "Epoch 191, loss: 0.00030795117896614653, windowed_loss: 0.0006921214775906667\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00895523 0.00269894]\n",
      "Epoch 192, loss: 0.0038847237486730924, windowed_loss: 0.0016315344420045313\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.91703439e-05 3.59812260e-03 1.18684667e-03]\n",
      "Epoch 193, loss: 0.0016013798727230594, windowed_loss: 0.0019313516001207662\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [4.12098386e-05 2.47125192e-03 1.57000988e-03]\n",
      "Epoch 194, loss: 0.0013608238800908578, windowed_loss: 0.0022823091671623362\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00316443 0.0007037 ]\n",
      "Epoch 195, loss: 0.0012893775534449203, windowed_loss: 0.0014171937687529458\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00342296 0.00124011]\n",
      "Epoch 196, loss: 0.0015543537270842178, windowed_loss: 0.001401518386873332\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [5.55015762e-05 5.60766289e-03 2.27031213e-03]\n",
      "Epoch 197, loss: 0.0026444922003782158, windowed_loss: 0.001829407826969118\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [3.00339677e-05 1.30745167e-03 1.42058053e-03]\n",
      "Epoch 198, loss: 0.0009193553899725279, windowed_loss: 0.0017060671058116538\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [5.25674021e-05 4.60848788e-03 2.93578123e-03]\n",
      "Epoch 199, loss: 0.0025322788366765685, windowed_loss: 0.0020320421423424373\n",
      "LR: [0.002097152000000001, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00159332 0.00119161]\n",
      "Epoch 200, loss: 0.0009283122578353592, windowed_loss: 0.0014599821614948187\n",
      "Epoch 00201: reducing learning rate of group 0 to 2.0000e-03.\n",
      "LR: [0.002, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00032655 0.00281137 0.00223048]\n",
      "Epoch 201, loss: 0.0017894674656969128, windowed_loss: 0.0017500195200696135\n",
      "LR: [0.002, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00017363 0.00396825 0.00174625]\n",
      "Epoch 202, loss: 0.0019627104541569047, windowed_loss: 0.001560163392563059\n",
      "LR: [0.002, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.00130723e-05 1.61587108e-03 1.30392391e-03]\n",
      "Epoch 203, loss: 0.0009766026879801896, windowed_loss: 0.0015762602026113358\n",
      "LR: [0.002, 0.002621440000000001, 0.004096000000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [5.52824614e-05 2.18326361e-03 4.47580422e-04]\n",
      "Epoch 204, loss: 0.0008953754987680551, windowed_loss: 0.0012782295469683832\n",
      "Epoch 00205: reducing learning rate of group 0 to 2.0972e-03.\n",
      "Epoch 00205: reducing learning rate of group 0 to 3.2768e-03.\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00016709 0.00353858 0.00162445]\n",
      "Epoch 205, loss: 0.0017767076690991719, windowed_loss: 0.0012162286186158057\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.24560499e-05 3.33277881e-03 7.66286626e-04]\n",
      "Epoch 206, loss: 0.0013738404959440231, windowed_loss: 0.001348641221270417\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00299989 0.00217904]\n",
      "Epoch 207, loss: 0.0017263088939767895, windowed_loss: 0.001625619019673328\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00010055 0.00550611 0.0018272 ]\n",
      "Epoch 208, loss: 0.0024779547908992477, windowed_loss: 0.0018593680602733535\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00153846 0.00112367]\n",
      "Epoch 209, loss: 0.0008873772440534649, windowed_loss: 0.0016972136429765008\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00012025 0.00324226 0.00020323]\n",
      "Epoch 210, loss: 0.0011885791565432694, windowed_loss: 0.0015179703971653273\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [9.59092920e-07 4.46940027e-03 9.01719725e-04]\n",
      "Epoch 211, loss: 0.0017906930297613144, windowed_loss: 0.0012888831434526829\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [8.31281597e-06 2.03566981e-03 6.46070323e-04]\n",
      "Epoch 212, loss: 0.0008966843174262482, windowed_loss: 0.0012919855012436105\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00297391 0.00133241]\n",
      "Epoch 213, loss: 0.0014354413318814652, windowed_loss: 0.0013742728930230092\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [9.73824750e-06 2.18832053e-03 4.02310355e-04]\n",
      "Epoch 214, loss: 0.0008667897094379773, windowed_loss: 0.001066305119581897\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00416551 0.00153018]\n",
      "Epoch 215, loss: 0.0018985618018742764, windowed_loss: 0.001400264281064573\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.66693194e-05 3.21814232e-03 1.46529316e-03]\n",
      "Epoch 216, loss: 0.0015667015968850164, windowed_loss: 0.0014440177027324234\n",
      "LR: [0.002, 0.002097152000000001, 0.0032768000000000007]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [3.97678126e-05 2.35192224e-03 8.45532356e-04]\n",
      "Epoch 217, loss: 0.0010790741353323967, windowed_loss: 0.0015147791780305631\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.6214e-03.\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00245977 0.00040731]\n",
      "Epoch 218, loss: 0.0009556926899787153, windowed_loss: 0.001200489474065376\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [4.38470055e-05 4.32444940e-03 7.62413510e-04]\n",
      "Epoch 219, loss: 0.001710236636978207, windowed_loss: 0.001248334487429773\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.0001014  0.00334065 0.0014794 ]\n",
      "Epoch 220, loss: 0.0016404826081160345, windowed_loss: 0.0014354706450243188\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.85043974e-05 4.08130173e-03 5.92092052e-04]\n",
      "Epoch 221, loss: 0.0015672993930903347, windowed_loss: 0.0016393395460615253\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00213556 0.00171877]\n",
      "Epoch 222, loss: 0.001284775013724963, windowed_loss: 0.0014975190049771108\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00218013 0.00058969]\n",
      "Epoch 223, loss: 0.0009232748186949528, windowed_loss: 0.0012584497418367503\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [3.37704339e-05 1.45113485e-03 1.01132200e-03]\n",
      "Epoch 224, loss: 0.0008320757611231371, windowed_loss: 0.0010133751978476842\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [4.31927090e-05 3.04124115e-03 4.70018184e-04]\n",
      "Epoch 225, loss: 0.001184817347111124, windowed_loss: 0.0009800559756430714\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.11862001e-05 2.44439326e-03 3.54297120e-04]\n",
      "Epoch 226, loss: 0.000936625525355339, windowed_loss: 0.0009845062111965334\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.001003   0.00034495]\n",
      "Epoch 227, loss: 0.00044931448770291877, windowed_loss: 0.0008569191200564607\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00256238 0.00153456]\n",
      "Epoch 228, loss: 0.0013656480745835736, windowed_loss: 0.0009171960292139438\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00211256 0.00048828]\n",
      "Epoch 229, loss: 0.0008669462845180973, windowed_loss: 0.0008939696156015299\n",
      "LR: [0.002, 0.002, 0.002621440000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00013155 0.00237501 0.00035026]\n",
      "Epoch 230, loss: 0.0009522725348219727, windowed_loss: 0.001061622297974548\n",
      "Epoch 00231: reducing learning rate of group 0 to 2.0972e-03.\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00012157 0.00461433 0.00035179]\n",
      "Epoch 231, loss: 0.001695896019086693, windowed_loss: 0.0011717049461422544\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [3.50483778e-05 2.88975882e-03 1.22459910e-03]\n",
      "Epoch 232, loss: 0.00138313543390144, windowed_loss: 0.0013437679959367019\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00180109 0.00088524]\n",
      "Epoch 233, loss: 0.000895441368673787, windowed_loss: 0.0013248242738873066\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00019356 0.0039304  0.00169549]\n",
      "Epoch 234, loss: 0.0019398177550597623, windowed_loss: 0.001406131519211663\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [7.92928040e-06 8.63275406e-04 8.83506950e-04]\n",
      "Epoch 235, loss: 0.0005849038786960371, windowed_loss: 0.0011400543341431955\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00020775 0.00144911 0.00080151]\n",
      "Epoch 236, loss: 0.0008194545904795328, windowed_loss: 0.001114725408078444\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00136079 0.00055534]\n",
      "Epoch 237, loss: 0.0006387115202166818, windowed_loss: 0.0006810233297974174\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00406571 0.00069389]\n",
      "Epoch 238, loss: 0.0015865336877830102, windowed_loss: 0.0010148999328264084\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00184941 0.00022744]\n",
      "Epoch 239, loss: 0.0006922817027026957, windowed_loss: 0.0009725089702341292\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [6.10713932e-05 1.31084245e-03 1.44836070e-04]\n",
      "Epoch 240, loss: 0.0005055833043474139, windowed_loss: 0.0009281328982777066\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00438775 0.0005902 ]\n",
      "Epoch 241, loss: 0.0016593158132199085, windowed_loss: 0.0009523936067566727\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [7.50916925e-05 2.93554840e-03 1.56138279e-03]\n",
      "Epoch 242, loss: 0.0015240076252005317, windowed_loss: 0.001229635580922618\n",
      "LR: [0.002, 0.002, 0.002097152000000001]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [1.72389502e-05 3.30744768e-03 4.07879664e-04]\n",
      "Epoch 243, loss: 0.001244188766136314, windowed_loss: 0.001475837401518918\n",
      "Epoch 00244: reducing learning rate of group 0 to 2.0000e-03.\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00064392 0.00073864]\n",
      "Epoch 244, loss: 0.00046085380017757416, windowed_loss: 0.0010763500638381399\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [2.77407129e-05 1.91988898e-03 4.01399522e-04]\n",
      "Epoch 245, loss: 0.0007830097368269254, windowed_loss: 0.0008293507677136045\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.00012963 0.00248337 0.00019061]\n",
      "Epoch 246, loss: 0.0009345344521782614, windowed_loss: 0.0007261326630609203\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.0024274  0.00072755]\n",
      "Epoch 247, loss: 0.0010516490561492517, windowed_loss: 0.0009230644150514795\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00253082 0.00076142]\n",
      "Epoch 248, loss: 0.0010974139088031018, windowed_loss: 0.0010278658057102051\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 8.333333333333334\n",
      "Unsupervised Training.. 16.666666666666668\n",
      "Unsupervised Training.. 25.0\n",
      "Unsupervised Training.. 33.333333333333336\n",
      "Unsupervised Training.. 41.666666666666664\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 58.333333333333336\n",
      "Unsupervised Training.. 66.66666666666667\n",
      "Unsupervised Training.. 75.0\n",
      "Unsupervised Training.. 83.33333333333333\n",
      "Losses: [0.         0.00174991 0.00186519]\n",
      "Epoch 249, loss: 0.0012050354571053476, windowed_loss: 0.001118032807352567\n",
      "LR: [0.002, 0.002, 0.002]\n",
      "Total Pre-training Time: 4518.981061220169\n"
     ]
    }
   ],
   "source": [
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "ensemble = Ensemble(size=3, output_size=5, lr=1e-2, weight_decay=0, new_model=True, dynamic_lr=True, manual_schedulers=True, init=\"Random\")\n",
    "if ENSEMBLE_PATH is not None:\n",
    "    ensemble.load_ensemble(ENSEMBLE_PATH, full=True)\n",
    "sampled_dataset = DATASET\n",
    "\n",
    "def pretraining(data, ensemble, data_cutoff=None, data_size=500):\n",
    "    if data_cutoff is None:\n",
    "        data_cutoff = len(data) - 1\n",
    "    # np.random.seed(0)\n",
    "    random.shuffle(triplets)\n",
    "    samples = triplets[:data_size]\n",
    "    total_loss = np.array([0.0 for i in range(len(ensemble.ensemble))])\n",
    "\n",
    "    BATCH_SIZE = 4096\n",
    "    total_updates = 0\n",
    "    total_batches = max(len(samples), data_size) // BATCH_SIZE\n",
    "\n",
    "    # Batch the data\n",
    "    for i in range(0, len(samples), BATCH_SIZE):\n",
    "        # AUGMENT_SIZE = 1\n",
    "        if i + (BATCH_SIZE) >= len(samples):\n",
    "            continue\n",
    "\n",
    "        print(f\"Unsupervised Training.. {(total_updates * 100) / total_batches}\")\n",
    "\n",
    "        temp_losses = np.array([0.0 for _ in ensemble.ensemble])\n",
    "\n",
    "        anchors = np.array([data[samples[i + j][0]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        pretraining = random.random() < 0.6\n",
    "        if pretraining:\n",
    "            positives = np.array(\n",
    "                [\n",
    "                    getRandomTransformation(data[samples[i + j][0]][0]) for j in range(BATCH_SIZE)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            positives = np.array(\n",
    "                [\n",
    "                    getRandomFlip(data[samples[i + j][1]][0]) for j in range(BATCH_SIZE)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        negatives = np.array([data[samples[i + j][2]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        anchors = np.expand_dims(anchors, axis=1)\n",
    "        positives = np.expand_dims(positives, axis=1)\n",
    "        negatives = np.expand_dims(negatives, axis=1)\n",
    "\n",
    "        losses = ensemble.train_batch(anchors, positives, negatives)\n",
    "        temp_losses += losses\n",
    "\n",
    "        total_loss += temp_losses\n",
    "        total_updates += 1\n",
    "\n",
    "    return total_loss, max(total_updates, 1)\n",
    "\n",
    "t_1 = time.time()\n",
    "epochs = 0\n",
    "loss_history = []\n",
    "while epochs < 250:\n",
    "    losses, total_updates = pretraining(sampled_dataset, ensemble, data_cutoff=9999, data_size=(4096 * 12))\n",
    "    average_loss = losses / total_updates\n",
    "    locale_loss = sum(average_loss) / len(average_loss)\n",
    "    loss_history.append(locale_loss)\n",
    "    loss = (sum(loss_history[-3:]) / 3) if len(loss_history) > 3 else 50\n",
    "    print(f\"Losses: {average_loss}\")\n",
    "    print(f\"Epoch {epochs}, loss: {locale_loss}, windowed_loss: {loss}\")\n",
    "    epochs += 1\n",
    "    ensemble.step_schedulers(losses)\n",
    "    print(f\"LR: {ensemble.get_lr()}\")\n",
    "\n",
    "print(f\"Total Pre-training Time: {time.time() - t_1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(f\"../checkpoints/ensembles/{int(time.time())}\", full=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
