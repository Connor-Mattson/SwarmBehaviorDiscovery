{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 1000\n"
     ]
    }
   ],
   "source": [
    "# For single Sensor Baseline Model\n",
    "# TRUTH_FILE = \"validation-data-two-sensor.txt\"\n",
    "# TRUTH_FILE = \"validation-data-baseline.txt\"\n",
    "\n",
    "\"\"\"\n",
    "BASELINE - Pretraining only. No HIL.\n",
    "\"\"\"\n",
    "VALIDATION_FILE = \"validation-data-baseline.txt\"\n",
    "VALIDATION_DATA = SwarmDataset(\"../data/validation-easy-model\", rank=0)\n",
    "TESTING_FILE = \"original-hand-labeled-classes.txt\"\n",
    "TESTING_DATA = SwarmDataset(\"../data/full-mini\", rank=0)\n",
    "ENSEMBLE_PATH = \"../checkpoints/ensembles/01-20-23-baseline\"\n",
    "\n",
    "\"\"\"\n",
    "BASELINE - Pretraining + HIL.\n",
    "\"\"\"\n",
    "# VALIDATION_FILE = \"validation-data-baseline.txt\"\n",
    "# VALIDATION_DATA = SwarmDataset(\"../data/validation-easy-model\", rank=0)\n",
    "# TESTING_FILE = \"original-hand-labeled-classes.txt\"\n",
    "# TESTING_DATA = SwarmDataset(\"../data/full-mini\", rank=0)\n",
    "# ENSEMBLE_PATH = \"../checkpoints/ensembles/01-24-23-baseline-HIL-B\"\n",
    "\n",
    "\n",
    "OUT = \"../data/oracle\"\n",
    "validation_classes = []\n",
    "with open(os.path.join(OUT, VALIDATION_FILE), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    validation_classes = [-1 for i in range(len(lines))]\n",
    "    for line in lines:\n",
    "        triplet = CSVLineToVec(line)\n",
    "        validation_classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "testing_classes = []\n",
    "with open(os.path.join(OUT, TESTING_FILE), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    testing_classes = [-1 for i in range(len(lines))]\n",
    "    for line in lines:\n",
    "        triplet = CSVLineToVec(line)\n",
    "        testing_classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "validation_set = []\n",
    "testing_set = []\n",
    "for i, _class in enumerate(testing_classes):\n",
    "    testing_set.append((i, _class))\n",
    "\n",
    "for i, _class in enumerate(validation_classes):\n",
    "    validation_set.append((i, _class))\n",
    "\n",
    "print(len(validation_set), len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.5000e-03.\n",
      "Adjusting learning rate of group 0 to 1.5000e-03.\n",
      "Adjusting learning rate of group 0 to 1.5000e-03.\n",
      "====================\n",
      "TRAIN results\n",
      "Ensemble 0 ~ Accuracy: 45.72514249525016\n",
      "Ensemble 1 ~ Accuracy: 55.99146695110163\n",
      "Ensemble 2 ~ Accuracy: 77.08076397453418\n",
      "Average: 59.59912447362865\n",
      "====================\n",
      "====================\n",
      "VAL results\n",
      "Ensemble 0 ~ Accuracy: 80.05399820006\n",
      "Ensemble 1 ~ Accuracy: 78.75404153194893\n",
      "Ensemble 2 ~ Accuracy: 79.02736575447486\n",
      "Average: 79.2784684954946\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ensemble = Ensemble(size=3, output_size=5, lr=15e-4, learning_decay=0.7, decay_step=1, threshold=9.0, weight_decay=1e-4, new_model=True, init=\"Random\")\n",
    "ensemble.load_ensemble(ENSEMBLE_PATH, full=True)\n",
    "ensemble.eval_mode()\n",
    "\n",
    "for metric in [\"TRAIN\", \"VAL\"]:\n",
    "    a = []\n",
    "    sampled_dataset = TESTING_DATA if metric == \"TRAIN\" else VALIDATION_DATA\n",
    "    c_set = testing_set if metric == \"TRAIN\" else validation_set\n",
    "    classes = testing_classes if metric == \"TRAIN\" else validation_classes\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"{metric} results\")\n",
    "    for i in range(len(ensemble.ensemble)):\n",
    "        embedded_positions = []\n",
    "        for j, c in enumerate(classes):\n",
    "            image, _ = sampled_dataset[j][0], sampled_dataset[j][1][0]\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            embed = ensemble.ensemble[i].forward(torch.tensor(image, device=device, dtype=torch.float))\n",
    "            embed = embed.detach().cpu().squeeze(dim=0).numpy()\n",
    "            embedded_positions.append(embed)\n",
    "\n",
    "        # Evaluate Accuracy\n",
    "        MAX_SEARCH = 30000\n",
    "        correct, total = 0, 0\n",
    "        for x, _classX in c_set:\n",
    "            for y, _classY in c_set:\n",
    "                if x == y:\n",
    "                    continue\n",
    "                for z, _classZ in c_set:\n",
    "                    if x == z or y == z:\n",
    "                        continue\n",
    "                    # If _classX and _classY are both random, ignore.\n",
    "                    if _classX == 0 and _classY == 0:\n",
    "                        continue\n",
    "                    if _classZ != _classX and _classX == _classY:\n",
    "                        positive_dist = np.linalg.norm(embedded_positions[x] - embedded_positions[y])\n",
    "                        negative_dist = np.linalg.norm(embedded_positions[x] - embedded_positions[z])\n",
    "                        if positive_dist < negative_dist:\n",
    "                            correct += 1\n",
    "                        total += 1\n",
    "                    if total > MAX_SEARCH:\n",
    "                        break\n",
    "                if total > MAX_SEARCH:\n",
    "                    break\n",
    "            if total > MAX_SEARCH:\n",
    "                break\n",
    "\n",
    "        acc = correct * 100 / total\n",
    "        a.append(acc)\n",
    "        print(f\"Ensemble {i} ~ Accuracy: {acc}\")\n",
    "\n",
    "    print(f\"Average: {sum(a) / 3}\")\n",
    "    print(\"=\" * 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
