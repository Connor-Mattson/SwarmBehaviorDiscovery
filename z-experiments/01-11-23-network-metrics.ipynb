{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getAttributeVector(controller):\n",
    "    v0_l, v0_r = controller[0],controller[1]\n",
    "    v1_l, v1_r = controller[2], controller[3]\n",
    "    v0_l, v0_r, v1_l, v1_r = round(v0_l, 1), round(v0_r, 1), round(v1_l, 1), round(v1_r, 1)\n",
    "\n",
    "    k = 0.5\n",
    "    max_elem_score = max(-min(controllers[i]), max(controllers[i]))\n",
    "    max_elem_score = -1 if max_elem_score < k else 1\n",
    "\n",
    "    k_2 = 0.75\n",
    "    magnitude_score = np.linalg.norm(controllers[i])\n",
    "    magnitude_score = -1 if magnitude_score < k_2 else 1\n",
    "\n",
    "    k_3 = 0.3\n",
    "    average_score = np.average(np.sqrt(np.power(controllers[i], 2)))\n",
    "    average_score = -1 if average_score < k_3 else 1\n",
    "\n",
    "    # Sensor off magnitude (trial i)\n",
    "    on_magnitude = (v0_l**2) + (v0_r**2)\n",
    "\n",
    "    # Sensor on magnitude (trial i)\n",
    "    off_magnitude = (v1_l**2) + (v1_r**2)\n",
    "\n",
    "    # Spinning Detection (sensor off - trial ii)\n",
    "    if v0_l == 0.0 and v0_r == 0.0:\n",
    "        off_spin_variance = 0.0\n",
    "    else:\n",
    "        denom = v0_l if v0_l != 0.0 else v0_r\n",
    "        off_spin_variance = min(abs((v0_l + v0_r)), 1.0)\n",
    "\n",
    "    # Spinning Detection (sensor on - trial ii)\n",
    "    if v1_l == 0.0 and v1_r == 0.0:\n",
    "        on_spin_variance = 0.0\n",
    "    else:\n",
    "        denom = v1_l if v1_l != 0.0 else v1_r\n",
    "        on_spin_variance = min(abs((v1_l + v1_r)), 1)\n",
    "\n",
    "    # Mirror Property\n",
    "    mirrored_controller = np.array([v0_l, v0_r, -v0_l, -v0_r])\n",
    "    mirror_score = np.linalg.norm(mirrored_controller - np.array([v0_l, v0_r, v1_l, v1_r]))\n",
    "    k_m = 0.3\n",
    "    mirror_score = -5 if mirror_score < k_m else mirror_score\n",
    "\n",
    "    # Independence Property\n",
    "    independent_controller = np.array([v0_l, v0_r, v0_l, v0_r])\n",
    "    indep = np.linalg.norm(independent_controller - np.array([v0_l, v0_r, v1_l, v1_r]))\n",
    "\n",
    "    return [indep, mirror_score, on_spin_variance, off_spin_variance, on_magnitude, off_magnitude, max_elem_score, magnitude_score, average_score]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list\n",
    "\n",
    "# sampled_dataset = SwarmDataset(\"../data/full-mini\", rank=0)\n",
    "# data = sampled_dataset\n",
    "#\n",
    "# lim = 1000\n",
    "# controllers = []\n",
    "# classes = [-1 for i in range(lim)]\n",
    "# for i in range(lim):\n",
    "#     image, genome, behavior = sampled_dataset[i][0], sampled_dataset[i][1], sampled_dataset[i][2]\n",
    "#     for j in range(len(behavior)):\n",
    "#         if behavior[j] < 0.0:\n",
    "#             behavior[j] *= -1\n",
    "#     controllers.append(genome)\n",
    "#\n",
    "# OUT = \"../data/oracle\"\n",
    "# with open(os.path.join(OUT, \"original-hand-labeled-classes.txt\"), \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         triplet = CSVLineToVec(line)\n",
    "#         classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "sampled_dataset = SwarmDataset(\"../data/gecco-two-sensor\", rank=0)\n",
    "data = sampled_dataset\n",
    "\n",
    "lim = 500\n",
    "controllers = []\n",
    "classes = [-1 for i in range(lim)]\n",
    "for i in range(lim):\n",
    "    image, genome, behavior = sampled_dataset[i][0], sampled_dataset[i][1], sampled_dataset[i][2]\n",
    "    for j in range(len(behavior)):\n",
    "        if behavior[j] < 0.0:\n",
    "            behavior[j] *= -1\n",
    "    controllers.append(genome)\n",
    "\n",
    "OUT = \"../data/oracle\"\n",
    "with open(os.path.join(OUT, \"gecco-two-sensor-classes.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        triplet = CSVLineToVec(line)\n",
    "        classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "max_randoms = 1000\n",
    "total_randoms = 0\n",
    "for i, controller in enumerate(controllers):\n",
    "    # if i % 20 == 0:\n",
    "    #     testing_data.append([controller, getAttributeVector(controller), int(classes[i] > 0)])\n",
    "    # else:\n",
    "    #     training_data.append([controller, getAttributeVector(controller), int(classes[i] > 0)])\n",
    "    if classes[i] == 0 and total_randoms >= max_randoms:\n",
    "        continue\n",
    "    elif classes[i] == 0:\n",
    "        total_randoms += 1\n",
    "    training_data.append([controller, getAttributeVector(controller), int(classes[i] > 0)])\n",
    "\n",
    "print(len(training_data), len(testing_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "class RandomDetectionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.single_layer = nn.Sequential(\n",
    "            nn.Linear(9, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            m = nn.Dropout(p=0.05)\n",
    "            x = m(x)\n",
    "        logits = self.single_layer(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "model = RandomDetectionNetwork().to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "epochs = 200\n",
    "testing_data = training_data\n",
    "for e in range(epochs):\n",
    "    # Training\n",
    "    random.shuffle(training_data)\n",
    "    model.train()\n",
    "    for i, (controller, attributes, y_truth) in enumerate(training_data):\n",
    "        X = torch.tensor(attributes).to(device)\n",
    "        y = torch.tensor([y_truth]).to(device)\n",
    "\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 300 == 0:\n",
    "            loss, current = loss.item(), i\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{len(training_data):>5d}]\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (controller, attributes, y_truth) in enumerate(testing_data):\n",
    "            X = torch.tensor(attributes).to(device)\n",
    "            y = torch.tensor([y_truth]).to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y.float()).item()\n",
    "            # if(pred.item() > 0.5):\n",
    "            #     print(pred.item(), torch.round(pred).item(), y_truth)\n",
    "            correct += (torch.round(pred) == y_truth).float().item()\n",
    "    test_loss /= len(testing_data)\n",
    "    correct /= len(testing_data)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if (100*correct) > 78:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.single_layer[0].weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
