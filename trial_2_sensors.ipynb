{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from NovelSwarmBehavior.novel_swarms.behavior.AngularMomentum import AngularMomentumBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.AverageSpeed import AverageSpeedBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.GroupRotationBehavior import GroupRotationBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.RadialVariance import RadialVarianceBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.ScatterBehavior import ScatterBehavior\n",
    "from NovelSwarmBehavior.novel_swarms.behavior.SensorOffset import GeneElementDifference\n",
    "from NovelSwarmBehavior.novel_swarms.config.AgentConfig import DiffDriveAgentConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.EvolutionaryConfig import GeneticEvolutionConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.OutputTensorConfig import OutputTensorConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.WorldConfig import RectangularWorldConfig\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.GeneRule import GeneRule\n",
    "from NovelSwarmBehavior.novel_swarms.sensors.GenomeDependentSensor import GenomeBinarySensor\n",
    "from NovelSwarmBehavior.novel_swarms.sensors.SensorSet import SensorSet\n",
    "from generation.evolution import ModifiedHaltingEvolution, ModifiedNoveltyArchieve\n",
    "from generation.halted_evolution import HaltedEvolution\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "def evolution_controller():\n",
    "    sensors = SensorSet([\n",
    "        GenomeBinarySensor(genome_id=8, draw=False),\n",
    "        GenomeBinarySensor(genome_id=9, draw=False)\n",
    "    ])\n",
    "\n",
    "    agent_config = DiffDriveAgentConfig(\n",
    "        sensors=sensors,\n",
    "        seed=None,\n",
    "    )\n",
    "\n",
    "    genotype = [\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=((1/3) * np.pi), _min=-((2/3) * np.pi), mutation_step=(np.pi/2), round_digits=4),\n",
    "        GeneRule(_max=((2/3) * np.pi), _min=-((1/3) * np.pi), mutation_step=(np.pi/2), round_digits=4),\n",
    "    ]\n",
    "\n",
    "    phenotype = [\n",
    "        AverageSpeedBehavior(),\n",
    "        AngularMomentumBehavior(),\n",
    "        RadialVarianceBehavior(),\n",
    "        ScatterBehavior(),\n",
    "        GroupRotationBehavior(),\n",
    "        # GeneElementDifference(8, 9)\n",
    "    ]\n",
    "\n",
    "    world_config = RectangularWorldConfig(\n",
    "        size=(500, 500),\n",
    "        n_agents=24,\n",
    "        seed=None,\n",
    "        behavior=phenotype,\n",
    "        agentConfig=agent_config,\n",
    "        padding=15\n",
    "    )\n",
    "\n",
    "    novelty_config = GeneticEvolutionConfig(\n",
    "        gene_rules=genotype,\n",
    "        phenotype_config=phenotype,\n",
    "        n_generations=50,\n",
    "        n_population=100,\n",
    "        crossover_rate=0.7,\n",
    "        mutation_rate=0.15,\n",
    "        world_config=world_config,\n",
    "        k_nn=15,\n",
    "        simulation_lifespan=2500,\n",
    "        display_novelty=True,\n",
    "        save_archive=True,\n",
    "        show_gui=True,\n",
    "    )\n",
    "\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"Evolutionary Novelty Search\")\n",
    "    screen = pygame.display.set_mode((world_config.w, world_config.h))\n",
    "\n",
    "    output_config = OutputTensorConfig(\n",
    "        timeless=True,\n",
    "        total_frames=80,\n",
    "        steps_between_frames=2,\n",
    "        screen=screen\n",
    "    )\n",
    "\n",
    "    halted_evolution = ModifiedHaltingEvolution(\n",
    "        world=world_config,\n",
    "        evolution_config=novelty_config,\n",
    "        output_config=output_config\n",
    "    )\n",
    "\n",
    "    return halted_evolution, screen\n",
    "\n",
    "def build_random_dataset():\n",
    "    halted_evolution, screen = evolution_controller()\n",
    "    baseline_data = DataBuilder(\"data/full-dual-sensors\", steps=1200, agents=30, ev=halted_evolution, screen=screen)\n",
    "    baseline_data.create()\n",
    "    baseline_data.evolution.close()\n",
    "\n",
    "# import time\n",
    "# t_1 = time.time()\n",
    "# build_random_dataset()\n",
    "# print(f\"TIME TO DATASET: {time.time() - t_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embed into Latent Space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from generation.evolution import ModifiedHaltingEvolution\n",
    "from scipy import ndimage\n",
    "import random\n",
    "\n",
    "def batch_update_network(network, loss_fn, optim, anchor_images, pos_images, neg_images):\n",
    "    optim.zero_grad()\n",
    "    anchor_out, pos_out, neg_out = network.batch_network_from_numpy(anchor_images, pos_images, neg_images)\n",
    "    loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "def update_network(network, loss_fn, optim, anchor_image, pos_image, neg_image, with_transforms=False):\n",
    "    optim.zero_grad()\n",
    "    if with_transforms:\n",
    "        anchor_out, pos_out, neg_out = network.network_with_transforms(anchor_image, pos_image, neg_image)\n",
    "    else:\n",
    "        anchor_out, pos_out, neg_out = network.network_from_numpy(anchor_image, pos_image, neg_image)\n",
    "    loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "def pretraining(data, network, loss_fn, optim, data_cutoff=None, data_size=500):\n",
    "    if data_cutoff is None:\n",
    "        data_cutoff = len(data) - 1\n",
    "    samples = np.random.random_integers(0, data_cutoff, (data_size, 2))\n",
    "    total_loss = 0\n",
    "    total_updates = 0\n",
    "    for i, s in enumerate(samples):\n",
    "        if i % (data_size // 20) == 0:\n",
    "            print(f\"Unsupervised Training.. {(i * 100) / data_size}\")\n",
    "        anchor_image = data[s[0]][0]\n",
    "        pos_image = data[s[0]][0]\n",
    "\n",
    "        pos_images = np.stack([\n",
    "            [ndimage.rotate(pos_image, 90)],\n",
    "            [ndimage.rotate(pos_image, 180)],\n",
    "            [ndimage.rotate(pos_image, 270)],\n",
    "            # ndimage.gaussian_filter(pos_image, 1),\n",
    "            # ndimage.gaussian_filter(pos_image, 3),\n",
    "            # ndimage.gaussian_filter(pos_image, 5),\n",
    "        ])\n",
    "\n",
    "        anchor_images = np.stack([[anchor_image] for _ in pos_images])\n",
    "        neg_images = np.stack([ [data[random.randint(0, data_cutoff)][0]] for _ in pos_images])\n",
    "\n",
    "        loss = batch_update_network(network, loss_fn, optim, anchor_images, pos_images, neg_images)\n",
    "        total_loss += loss.item()\n",
    "        total_updates += 1\n",
    "\n",
    "    return total_loss / total_updates\n",
    "\n",
    "\n",
    "def train(network, loss_fn, optim, anchor_img, pos_img, neg_img):\n",
    "\n",
    "    pos_images = np.stack([\n",
    "        [ndimage.rotate(pos_img, 90)],\n",
    "        [ndimage.rotate(pos_img, 180)],\n",
    "        [ndimage.rotate(pos_img, 270)],\n",
    "        # ndimage.gaussian_filter(pos_img, 1),\n",
    "        # ndimage.gaussian_filter(pos_img, 3),\n",
    "        # ndimage.gaussian_filter(pos_img, 5),\n",
    "    ])\n",
    "\n",
    "    anchor_images = np.stack([[anchor_img] for _ in pos_images])\n",
    "    neg_images = np.stack([[neg_img] for _ in pos_images])\n",
    "\n",
    "    loss = batch_update_network(network, loss_fn, optim, anchor_images, pos_images, neg_images)\n",
    "    return loss\n",
    "\n",
    "def human_in_the_loop(anchor_dataset, network, optimizer, loss_fn, HIL_archive, random_archive, stop_at):\n",
    "    print(\"HIL TIME!\")\n",
    "    improvements, human_loss, triplet_helpfulness, embedded_archive = hil.humanInput(anchor_dataset, network, optimizer, loss_fn, HIL_archive, random_archive, stop_at)\n",
    "    print(f\"Improvement Count: {improvements}, loss: {human_loss}\")\n",
    "\n",
    "    HIL_archive.save_to_file(f\"data/queries/{trial_name}_hil.csv\")\n",
    "    random_archive.save_to_file(f\"data/queries/{trial_name}_rand.csv\")\n",
    "    return improvements, human_loss, triplet_helpfulness, embedded_archive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from hil.HIL import HIL\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "trial_name = f\"{str(int(time.time()))}\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding(out_size=15).to(device)\n",
    "\n",
    "TRAIN = True\n",
    "SAVE_CLUSTER_IMAGES = True\n",
    "SAVE_CLUSTER_MEDOIDS = True\n",
    "PRETRAINING = True\n",
    "HUMAN_IN_LOOP = True\n",
    "SYNTHETIC_HIL = False\n",
    "EVOLUTION = True\n",
    "CLUSTER_AND_DISPLAY = True\n",
    "CONCAT_BVEC_AND_EVEC = True\n",
    "PASS_THROUGHS = 1\n",
    "RESULTS_DATA_SIZE = 10000\n",
    "EVOLUTIONS_PER_EPOCH = 50\n",
    "K_CLUSTERS = 8\n",
    "INITIAL_EVOLVES = 3\n",
    "\n",
    "anchor_dataset = ContinuingDataset(\"data\")\n",
    "sampled_dataset = SwarmDataset(\"data/full-dual-sensors\", rank=0)\n",
    "# evolution, _ = ModifiedHaltingEvolution.defaultEvolver(steps=180, n_agents=12, evolve_population=50, seed=None)\n",
    "evolution, _ = evolution_controller()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.8)\n",
    "\n",
    "loss_fn = torch.nn.TripletMarginLoss(margin=15)\n",
    "hil = HIL(name=trial_name, synthetic=SYNTHETIC_HIL, data_limiter=RESULTS_DATA_SIZE, clusters=K_CLUSTERS)\n",
    "HIL_archive = DataAggregationArchive()\n",
    "random_archive = DataAggregationArchive(scalar=True)\n",
    "EPSILON = 0.5\n",
    "\n",
    "simulation_time = 0\n",
    "evolution_time = 0\n",
    "training_time = 0\n",
    "hil_time = 0\n",
    "\n",
    "\n",
    "# TODO: Add Randomly-sampled Contrastive/Triplet Loss\n",
    "if PRETRAINING:\n",
    "    network.load_model(\"two_sensor_subz1\")\n",
    "    print(\"Pretrained model loaded!\")\n",
    "\n",
    "dataset = anchor_dataset if EVOLUTION else sampled_dataset\n",
    "\n",
    "if TRAIN:\n",
    "    writer = SummaryWriter()\n",
    "    STOP_FLAG = False\n",
    "    for epoch in range(PASS_THROUGHS + 1):\n",
    "        if EVOLUTION:\n",
    "            start_time = time.time()\n",
    "            evolution.restart_screen()\n",
    "            for gen in range(EVOLUTIONS_PER_EPOCH if epoch > 0 else INITIAL_EVOLVES):\n",
    "                # Simulate current population + Save Data\n",
    "                for i in range(len(evolution.getPopulation())):\n",
    "                    # The collection of the original behavior vector below is only used to collect data to compare with the baseline\n",
    "                    visual_behavior, genome, baseline_behavior = evolution.next()\n",
    "                    dataset.new_entry(visual_behavior, genome, baseline_behavior)\n",
    "                simulation_time += (time.time() - start_time)\n",
    "\n",
    "                # Then, evolve\n",
    "                start_time = time.time()\n",
    "                embedded_archive = hil.getEmbeddedArchive(dataset, network, concat_behavior=CONCAT_BVEC_AND_EVEC)\n",
    "                evolution.overwriteArchive(embedded_archive, random_archive)\n",
    "                embedded_behavior = embedded_archive.archive[-evolution.evolve_config.population:]\n",
    "                evolution.overwriteBehavior(embedded_behavior)\n",
    "                evolution.evolve()\n",
    "                evolution.restart_screen()\n",
    "                evolution_time += (time.time() - start_time)\n",
    "\n",
    "                print(f\"Evolution complete for e{epoch} and gen{gen}\")\n",
    "                print(f\"Size of Evolved Archive: {len(evolution.behavior_discovery.archive.archive)}\")\n",
    "\n",
    "                # Record the accuracy of the medoids with respect to the synthetic policy\n",
    "                medoid_acc, cluster_acc = 0, 0\n",
    "                if len(dataset) > 0 and SAVE_CLUSTER_MEDOIDS:\n",
    "                    if SYNTHETIC_HIL and EVOLUTION:\n",
    "                        hil.synthetic_knowledge = hil.syntheticBehaviorSpace(dataset)\n",
    "                        print(f\"Synthetic Human Knowledge Size: {len(hil.synthetic_knowledge.labels_)}\")\n",
    "                    medoid_acc, cluster_acc = hil.record_medoids(network, dataset)\n",
    "\n",
    "                # Cluster current dataset, display clusters, and save for analysis\n",
    "                if len(dataset) > 0 and SAVE_CLUSTER_IMAGES:\n",
    "                    hil.embed_and_cluster(network, dataset, auto_quit=True)\n",
    "                    evolution.restart_screen()\n",
    "\n",
    "                evolution.saveArchive(trial_name)\n",
    "\n",
    "        # Human in the Loop determines behavior embedding\n",
    "        if HUMAN_IN_LOOP and epoch < PASS_THROUGHS:\n",
    "            start_time = time.time()\n",
    "            improvements, human_loss, triplet_helpfulness, _ =   human_in_the_loop(\n",
    "                                                                dataset,\n",
    "                                                                network,\n",
    "                                                                optimizer,\n",
    "                                                                loss_fn,\n",
    "                                                                HIL_archive,\n",
    "                                                                random_archive,\n",
    "                                                                len(dataset)\n",
    "                                                            )\n",
    "            hil_time += (time.time() - start_time)\n",
    "\n",
    "        # Train on past user information\n",
    "        if TRAIN:\n",
    "            start_time = time.time()\n",
    "            loss = None\n",
    "\n",
    "            average_loss = 50\n",
    "            loop = 0\n",
    "            BATCH_SIZE = 5\n",
    "            network.train()\n",
    "            while (average_loss > 0.01) and loop < 20:\n",
    "                loss_sum = 0\n",
    "                total_loss = 0\n",
    "                for i, (anchor, pos, neg) in enumerate(HIL_archive):\n",
    "                    anchor_encoding = dataset[anchor][0]\n",
    "                    similar_encoding = dataset[pos][0]\n",
    "                    anti_encoding = dataset[neg][0]\n",
    "                    loss = train(network, loss_fn, optimizer, anchor_encoding, similar_encoding, anti_encoding)\n",
    "                    loss_sum += loss.item()\n",
    "                    total_loss += 1\n",
    "\n",
    "                    if loss is not None and i > 0 and i % 20 == 0:\n",
    "                        print(f\"Epoch Progress: {(i*100) / len(HIL_archive)}%, Immediate Loss: {loss.item()}\")\n",
    "\n",
    "                average_loss = loss_sum / (total_loss + 1)\n",
    "                print(f\"Loop: {loop}, Average Loss: {average_loss}\")\n",
    "                loop += 1\n",
    "\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            training_time += (time.time() - start_time)\n",
    "\n",
    "            writer.add_scalar(\"Loss/Average\", loss_sum / (total_loss + 1), epoch)\n",
    "\n",
    "        if SAVE_CLUSTER_MEDOIDS:\n",
    "            writer.add_scalar(\"Accuracy/MedoidClassification\", medoid_acc, epoch)\n",
    "            writer.add_scalar(\"Accuracy/RandomSampleClassification\", cluster_acc, epoch)\n",
    "\n",
    "        if HUMAN_IN_LOOP:\n",
    "            writer.add_scalar(\"Loss/TripletQuality\", triplet_helpfulness, epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Human_Queries\", epoch*8*9, epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Triplets_Generated\", len(HIL_archive), epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Random_Classes\", len(random_archive), epoch)\n",
    "\n",
    "        if EVOLUTION:\n",
    "            writer.add_scalar(\"Novelty/Highest\", evolution.behavior_discovery.getBestScore(), epoch)\n",
    "            writer.add_scalar(\"Novelty/Average\", evolution.behavior_discovery.getAverageScore(), epoch)\n",
    "\n",
    "        writer.add_scalar(\"Time/Simulation\", simulation_time, epoch)\n",
    "        writer.add_scalar(\"Time/HIL\", hil_time, epoch)\n",
    "        writer.add_scalar(\"Time/Evolution\", evolution_time, epoch)\n",
    "        writer.add_scalar(\"Time/Training\", training_time, epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "evolution.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evolve and Display?\n",
    "print(len(evolution.archive.archive))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save Model\n",
    "import torch\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "\n",
    "PRETRAINING = True\n",
    "target = 0.01\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding(out_size=15).to(device)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "sampled_dataset = SwarmDataset(\"data/full-dual-sensors\", rank=0)\n",
    "\n",
    "t_1 = time.time()\n",
    "if PRETRAINING:\n",
    "    epochs = 0\n",
    "    while loss > target:\n",
    "        loss = pretraining(sampled_dataset, network, loss_fn, optimizer, data_cutoff=None, data_size=1000)\n",
    "        print(f\"Epoch {epochs}, loss: {loss}\")\n",
    "        epochs += 1\n",
    "        scheduler.step()\n",
    "\n",
    "print(f\"Total Pre-training Time: {time.time() - t_1}\")\n",
    "\n",
    "network.save_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.save_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(optimizer.lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from NovelSwarmBehavior.novel_swarms.config.ResultsConfig import ResultsConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from data.swarmset import DataBuilder\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding().to(device)\n",
    "network.load_model(\"trialA-10-18-2022\")\n",
    "# anchor_dataset = SwarmDataset(\"data/full\", rank=0)\n",
    "\n",
    "WRITE_OUT = True\n",
    "if WRITE_OUT:\n",
    "    network.eval()\n",
    "    test_archive = NoveltyArchive()\n",
    "    for i in range(len(anchor_dataset)):\n",
    "        anchor_encoding, genome, _ = anchor_dataset[i]\n",
    "        anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "        embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "        test_archive.addToArchive(embedding, genome)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "import pygame\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=test_archive, k_clusters=6, world_config=world_config, tsne_perplexity=16, tsne_early_exaggeration=12, skip_tsne=False)\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()\n",
    "# results(config)\n",
    "pygame.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering + Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "\n",
    "# Cluster over saved behaviors\n",
    "archive = NoveltyArchive()\n",
    "for i, (_, genome, behavior, _, _, _, _) in enumerate(anchor_dataset):\n",
    "    archive.addToArchive(vec=behavior, genome=genome)\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=archive, k_clusters=7, world_config=world_config, tsne_perplexity=20, tsne_early_exaggeration=2, skip_tsne=False)\n",
    "\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(human_l_hist, \"b\", label='Human Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Time (Epochs)\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
