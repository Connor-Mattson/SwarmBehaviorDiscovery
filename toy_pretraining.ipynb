{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% Complete\n",
      "0.2% Complete\n",
      "0.4% Complete\n",
      "0.6% Complete\n",
      "0.8% Complete\n",
      "1.0% Complete\n",
      "1.2% Complete\n",
      "1.4% Complete\n",
      "1.6% Complete\n",
      "1.8% Complete\n",
      "2.0% Complete\n",
      "2.2% Complete\n",
      "2.4% Complete\n",
      "2.6% Complete\n",
      "2.8% Complete\n",
      "3.0% Complete\n",
      "3.2% Complete\n",
      "3.4% Complete\n",
      "3.6% Complete\n",
      "3.8% Complete\n",
      "4.0% Complete\n",
      "4.2% Complete\n",
      "4.4% Complete\n",
      "4.6% Complete\n",
      "4.8% Complete\n",
      "5.0% Complete\n",
      "5.2% Complete\n",
      "5.4% Complete\n",
      "5.6% Complete\n",
      "5.8% Complete\n",
      "6.0% Complete\n",
      "6.2% Complete\n",
      "6.4% Complete\n",
      "6.6% Complete\n",
      "6.8% Complete\n",
      "7.0% Complete\n",
      "7.2% Complete\n",
      "7.4% Complete\n",
      "7.6% Complete\n",
      "7.8% Complete\n",
      "8.0% Complete\n",
      "8.2% Complete\n",
      "8.4% Complete\n",
      "8.6% Complete\n",
      "8.8% Complete\n",
      "9.0% Complete\n",
      "9.2% Complete\n",
      "9.4% Complete\n",
      "9.6% Complete\n",
      "9.8% Complete\n",
      "10.0% Complete\n",
      "10.2% Complete\n",
      "10.4% Complete\n",
      "10.6% Complete\n",
      "10.8% Complete\n",
      "11.0% Complete\n",
      "11.2% Complete\n",
      "11.4% Complete\n",
      "11.6% Complete\n",
      "11.8% Complete\n",
      "12.0% Complete\n",
      "12.2% Complete\n",
      "12.4% Complete\n",
      "12.6% Complete\n",
      "12.8% Complete\n",
      "13.0% Complete\n",
      "13.2% Complete\n",
      "13.4% Complete\n",
      "13.6% Complete\n",
      "13.8% Complete\n",
      "14.0% Complete\n",
      "14.2% Complete\n",
      "14.4% Complete\n",
      "14.6% Complete\n",
      "14.8% Complete\n",
      "15.0% Complete\n",
      "15.2% Complete\n",
      "15.4% Complete\n",
      "15.6% Complete\n",
      "15.8% Complete\n",
      "16.0% Complete\n",
      "16.2% Complete\n",
      "16.4% Complete\n",
      "16.6% Complete\n",
      "16.8% Complete\n",
      "17.0% Complete\n",
      "17.2% Complete\n",
      "17.4% Complete\n",
      "17.6% Complete\n",
      "17.8% Complete\n",
      "18.0% Complete\n",
      "18.2% Complete\n",
      "18.4% Complete\n",
      "18.6% Complete\n",
      "18.8% Complete\n",
      "19.0% Complete\n",
      "19.2% Complete\n",
      "19.4% Complete\n",
      "19.6% Complete\n",
      "19.8% Complete\n",
      "20.0% Complete\n",
      "20.2% Complete\n",
      "20.4% Complete\n",
      "20.6% Complete\n",
      "20.8% Complete\n",
      "21.0% Complete\n",
      "21.2% Complete\n",
      "21.4% Complete\n",
      "21.6% Complete\n",
      "21.8% Complete\n",
      "22.0% Complete\n",
      "22.2% Complete\n",
      "22.4% Complete\n",
      "22.6% Complete\n",
      "22.8% Complete\n",
      "23.0% Complete\n",
      "23.2% Complete\n",
      "23.4% Complete\n",
      "23.6% Complete\n",
      "23.8% Complete\n",
      "24.0% Complete\n",
      "24.2% Complete\n",
      "24.4% Complete\n",
      "24.6% Complete\n",
      "24.8% Complete\n",
      "25.0% Complete\n",
      "25.2% Complete\n",
      "25.4% Complete\n",
      "25.6% Complete\n",
      "25.8% Complete\n",
      "26.0% Complete\n",
      "26.2% Complete\n",
      "26.4% Complete\n",
      "26.6% Complete\n",
      "26.8% Complete\n",
      "27.0% Complete\n",
      "27.2% Complete\n",
      "27.4% Complete\n",
      "27.6% Complete\n",
      "27.8% Complete\n",
      "28.0% Complete\n",
      "28.2% Complete\n",
      "28.4% Complete\n",
      "28.6% Complete\n",
      "28.8% Complete\n",
      "29.0% Complete\n",
      "29.2% Complete\n",
      "29.4% Complete\n",
      "29.6% Complete\n",
      "29.8% Complete\n",
      "30.0% Complete\n",
      "30.2% Complete\n",
      "30.4% Complete\n",
      "30.6% Complete\n",
      "30.8% Complete\n",
      "31.0% Complete\n",
      "31.2% Complete\n",
      "31.4% Complete\n",
      "31.6% Complete\n",
      "31.8% Complete\n",
      "32.0% Complete\n",
      "32.2% Complete\n",
      "32.4% Complete\n",
      "32.6% Complete\n",
      "32.8% Complete\n",
      "33.0% Complete\n",
      "33.2% Complete\n",
      "33.4% Complete\n",
      "33.6% Complete\n",
      "33.8% Complete\n",
      "34.0% Complete\n",
      "34.2% Complete\n",
      "34.4% Complete\n",
      "34.6% Complete\n",
      "34.8% Complete\n",
      "35.0% Complete\n",
      "35.2% Complete\n",
      "35.4% Complete\n",
      "35.6% Complete\n",
      "35.8% Complete\n",
      "36.0% Complete\n",
      "36.2% Complete\n",
      "36.4% Complete\n",
      "36.6% Complete\n",
      "36.8% Complete\n",
      "37.0% Complete\n",
      "37.2% Complete\n",
      "37.4% Complete\n",
      "37.6% Complete\n",
      "37.8% Complete\n",
      "38.0% Complete\n",
      "38.2% Complete\n",
      "38.4% Complete\n",
      "38.6% Complete\n",
      "38.8% Complete\n",
      "39.0% Complete\n",
      "39.2% Complete\n",
      "39.4% Complete\n",
      "39.6% Complete\n",
      "39.8% Complete\n",
      "40.0% Complete\n",
      "40.2% Complete\n",
      "40.4% Complete\n",
      "40.6% Complete\n",
      "40.8% Complete\n",
      "41.0% Complete\n",
      "41.2% Complete\n",
      "41.4% Complete\n",
      "41.6% Complete\n",
      "41.8% Complete\n",
      "42.0% Complete\n",
      "42.2% Complete\n",
      "42.4% Complete\n",
      "42.6% Complete\n",
      "42.8% Complete\n",
      "43.0% Complete\n",
      "43.2% Complete\n",
      "43.4% Complete\n",
      "43.6% Complete\n",
      "43.8% Complete\n",
      "44.0% Complete\n",
      "44.2% Complete\n",
      "44.4% Complete\n",
      "44.6% Complete\n",
      "44.8% Complete\n",
      "45.0% Complete\n",
      "45.2% Complete\n",
      "45.4% Complete\n",
      "45.6% Complete\n",
      "45.8% Complete\n",
      "46.0% Complete\n",
      "46.2% Complete\n",
      "46.4% Complete\n",
      "46.6% Complete\n",
      "46.8% Complete\n",
      "47.0% Complete\n",
      "47.2% Complete\n",
      "47.4% Complete\n",
      "47.6% Complete\n",
      "47.8% Complete\n",
      "48.0% Complete\n",
      "48.2% Complete\n",
      "48.4% Complete\n",
      "48.6% Complete\n",
      "48.8% Complete\n",
      "49.0% Complete\n",
      "49.2% Complete\n",
      "49.4% Complete\n",
      "49.6% Complete\n",
      "49.8% Complete\n",
      "50.0% Complete\n",
      "50.2% Complete\n",
      "50.4% Complete\n",
      "50.6% Complete\n",
      "50.8% Complete\n",
      "51.0% Complete\n",
      "51.2% Complete\n",
      "51.4% Complete\n",
      "51.6% Complete\n",
      "51.8% Complete\n",
      "52.0% Complete\n",
      "52.2% Complete\n",
      "52.4% Complete\n",
      "52.6% Complete\n",
      "52.8% Complete\n",
      "53.0% Complete\n",
      "53.2% Complete\n",
      "53.4% Complete\n",
      "53.6% Complete\n",
      "53.8% Complete\n",
      "54.0% Complete\n",
      "54.2% Complete\n",
      "54.4% Complete\n",
      "54.6% Complete\n",
      "54.8% Complete\n",
      "55.0% Complete\n",
      "55.2% Complete\n",
      "55.4% Complete\n",
      "55.6% Complete\n",
      "55.8% Complete\n",
      "56.0% Complete\n",
      "56.2% Complete\n",
      "56.4% Complete\n",
      "56.6% Complete\n",
      "56.8% Complete\n",
      "57.0% Complete\n",
      "57.2% Complete\n",
      "57.4% Complete\n",
      "57.6% Complete\n",
      "57.8% Complete\n",
      "58.0% Complete\n",
      "58.2% Complete\n",
      "58.4% Complete\n",
      "58.6% Complete\n",
      "58.8% Complete\n",
      "59.0% Complete\n",
      "59.2% Complete\n",
      "59.4% Complete\n",
      "59.6% Complete\n",
      "59.8% Complete\n",
      "60.0% Complete\n",
      "60.2% Complete\n",
      "60.4% Complete\n",
      "60.6% Complete\n",
      "60.8% Complete\n",
      "61.0% Complete\n",
      "61.2% Complete\n",
      "61.4% Complete\n",
      "61.6% Complete\n",
      "61.8% Complete\n",
      "62.0% Complete\n",
      "62.2% Complete\n",
      "62.4% Complete\n",
      "62.6% Complete\n",
      "62.8% Complete\n",
      "63.0% Complete\n",
      "63.2% Complete\n",
      "63.4% Complete\n",
      "63.6% Complete\n",
      "63.8% Complete\n",
      "64.0% Complete\n",
      "64.2% Complete\n",
      "64.4% Complete\n",
      "64.6% Complete\n",
      "64.8% Complete\n",
      "65.0% Complete\n",
      "65.2% Complete\n",
      "65.4% Complete\n",
      "65.6% Complete\n",
      "65.8% Complete\n",
      "66.0% Complete\n",
      "66.2% Complete\n",
      "66.4% Complete\n",
      "66.6% Complete\n",
      "66.8% Complete\n",
      "67.0% Complete\n",
      "67.2% Complete\n",
      "67.4% Complete\n",
      "67.6% Complete\n",
      "67.8% Complete\n",
      "68.0% Complete\n",
      "68.2% Complete\n",
      "68.4% Complete\n",
      "68.6% Complete\n",
      "68.8% Complete\n",
      "69.0% Complete\n",
      "69.2% Complete\n",
      "69.4% Complete\n",
      "69.6% Complete\n",
      "69.8% Complete\n",
      "70.0% Complete\n",
      "70.2% Complete\n",
      "70.4% Complete\n",
      "70.6% Complete\n",
      "70.8% Complete\n",
      "71.0% Complete\n",
      "71.2% Complete\n",
      "71.4% Complete\n",
      "71.6% Complete\n",
      "71.8% Complete\n",
      "72.0% Complete\n",
      "72.2% Complete\n",
      "72.4% Complete\n",
      "72.6% Complete\n",
      "72.8% Complete\n",
      "73.0% Complete\n",
      "73.2% Complete\n",
      "73.4% Complete\n",
      "73.6% Complete\n",
      "73.8% Complete\n",
      "74.0% Complete\n",
      "74.2% Complete\n",
      "74.4% Complete\n",
      "74.6% Complete\n",
      "74.8% Complete\n",
      "75.0% Complete\n",
      "75.2% Complete\n",
      "75.4% Complete\n",
      "75.6% Complete\n",
      "75.8% Complete\n",
      "76.0% Complete\n",
      "76.2% Complete\n",
      "76.4% Complete\n",
      "76.6% Complete\n",
      "76.8% Complete\n",
      "77.0% Complete\n",
      "77.2% Complete\n",
      "77.4% Complete\n",
      "77.6% Complete\n",
      "77.8% Complete\n",
      "78.0% Complete\n",
      "78.2% Complete\n",
      "78.4% Complete\n",
      "78.6% Complete\n",
      "78.8% Complete\n",
      "79.0% Complete\n",
      "79.2% Complete\n",
      "79.4% Complete\n",
      "79.6% Complete\n",
      "79.8% Complete\n",
      "80.0% Complete\n",
      "80.2% Complete\n",
      "80.4% Complete\n",
      "80.6% Complete\n",
      "80.8% Complete\n",
      "81.0% Complete\n",
      "81.2% Complete\n",
      "81.4% Complete\n",
      "81.6% Complete\n",
      "81.8% Complete\n",
      "82.0% Complete\n",
      "82.2% Complete\n",
      "82.4% Complete\n",
      "82.6% Complete\n",
      "82.8% Complete\n",
      "83.0% Complete\n",
      "83.2% Complete\n",
      "83.4% Complete\n",
      "83.6% Complete\n",
      "83.8% Complete\n",
      "84.0% Complete\n",
      "84.2% Complete\n",
      "84.4% Complete\n",
      "84.6% Complete\n",
      "84.8% Complete\n",
      "85.0% Complete\n",
      "85.2% Complete\n",
      "85.4% Complete\n",
      "85.6% Complete\n",
      "85.8% Complete\n",
      "86.0% Complete\n",
      "86.2% Complete\n",
      "86.4% Complete\n",
      "86.6% Complete\n",
      "86.8% Complete\n",
      "87.0% Complete\n",
      "87.2% Complete\n",
      "87.4% Complete\n",
      "87.6% Complete\n",
      "87.8% Complete\n",
      "88.0% Complete\n",
      "88.2% Complete\n",
      "88.4% Complete\n",
      "88.6% Complete\n",
      "88.8% Complete\n",
      "89.0% Complete\n",
      "89.2% Complete\n",
      "89.4% Complete\n",
      "89.6% Complete\n",
      "89.8% Complete\n",
      "90.0% Complete\n",
      "90.2% Complete\n",
      "90.4% Complete\n",
      "90.6% Complete\n",
      "90.8% Complete\n",
      "91.0% Complete\n",
      "91.2% Complete\n",
      "91.4% Complete\n",
      "91.6% Complete\n",
      "91.8% Complete\n",
      "92.0% Complete\n",
      "92.2% Complete\n",
      "92.4% Complete\n",
      "92.6% Complete\n",
      "92.8% Complete\n",
      "93.0% Complete\n",
      "93.2% Complete\n",
      "93.4% Complete\n",
      "93.6% Complete\n",
      "93.8% Complete\n",
      "94.0% Complete\n",
      "94.2% Complete\n",
      "94.4% Complete\n",
      "94.6% Complete\n",
      "94.8% Complete\n",
      "95.0% Complete\n",
      "95.2% Complete\n",
      "95.4% Complete\n",
      "95.6% Complete\n",
      "95.8% Complete\n",
      "96.0% Complete\n",
      "96.2% Complete\n",
      "96.4% Complete\n",
      "96.6% Complete\n",
      "96.8% Complete\n",
      "97.0% Complete\n",
      "97.2% Complete\n",
      "97.4% Complete\n",
      "97.6% Complete\n",
      "97.8% Complete\n",
      "98.0% Complete\n",
      "98.2% Complete\n",
      "98.4% Complete\n",
      "98.6% Complete\n",
      "98.8% Complete\n",
      "99.0% Complete\n",
      "99.2% Complete\n",
      "99.4% Complete\n",
      "99.6% Complete\n",
      "99.8% Complete\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "\n",
    "def rand_shape(scr):\n",
    "    scr.fill((0, 0, 0))\n",
    "    choices = [\"Cir\", \"Tri\", \"Sqr\"]\n",
    "    choice = random.choice(choices)\n",
    "    if choice == \"Cir\":\n",
    "        x, y = random.randint(50, 450), random.randint(50, 450)\n",
    "        r = random.randint(50, 200)\n",
    "        pygame.draw.circle(screen, (255, 255, 255), [x, y], r, 3)\n",
    "    elif choice == \"Tri\":\n",
    "        q = 500 // 6\n",
    "        vec = [random.randint(q * i, q * (i + 1)) for i in range(6)]\n",
    "        random.shuffle(vec)\n",
    "        pygame.draw.polygon(screen, (255, 255, 255), [[vec[0], vec[1]], [vec[2], vec[3]], [vec[4], vec[5]]], 3)\n",
    "    elif choice == \"Sqr\":\n",
    "        x1, y1 = random.randint(0, 500), random.randint(0, 500)\n",
    "        x2, y2 = random.randint(0, 500), random.randint(0, 500)\n",
    "        rect = pygame.Rect(min(x1, x2), min(y1, y2), 500 - max(x1, x2), 500 - max(y1, y2))\n",
    "        pygame.draw.rect(scr, (255, 255, 255), rect, 3)\n",
    "    else:\n",
    "        raise Exception(\"Bad Shape!\")\n",
    "\n",
    "    pygame.display.flip()\n",
    "    out = pygame.surfarray.array2d(scr)\n",
    "    _class = choices.index(choice)\n",
    "    return out, _class\n",
    "\n",
    "SAMPLES = 500\n",
    "\n",
    "pygame.init()\n",
    "pygame.display.set_caption(\"Evolutionary Novelty Search\")\n",
    "screen = pygame.display.set_mode((500, 500))\n",
    "\n",
    "dataset = SwarmDataset(\"data/testtoy\")\n",
    "\n",
    "import time\n",
    "for i in range(SAMPLES):\n",
    "    output, shape = rand_shape(screen)\n",
    "    dataset.new_sample(output, [shape], [shape])\n",
    "    print(f\"{(i*100) / SAMPLES}% Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# Create some OOD Examples for Testing\n",
    "import math\n",
    "import pygame\n",
    "\n",
    "def draw_regular_polygon(surface, color, vertex_count, radius, position, width=0):\n",
    "    n, r = vertex_count, radius\n",
    "    x, y = position\n",
    "    pygame.draw.polygon(surface, color, [\n",
    "        (x + r * math.cos(2 * math.pi * i / n), y + r * math.sin(2 * math.pi * i / n))\n",
    "        for i in range(n)\n",
    "    ], width)\n",
    "    pygame.display.flip()\n",
    "    out = pygame.surfarray.array2d(surface)\n",
    "    return out, 4\n",
    "\n",
    "SAMPLES = 1\n",
    "pygame.init()\n",
    "pygame.display.set_caption(\"Evolutionary Novelty Search\")\n",
    "screen = pygame.display.set_mode((500, 500))\n",
    "dataset = SwarmDataset(\"data/testtoy\")\n",
    "\n",
    "import time\n",
    "for i in range(SAMPLES):\n",
    "    output, shape = draw_regular_polygon(screen, (255, 255, 255), 5, 60, (250, 250), width=3)\n",
    "    dataset.new_sample(output, [shape], [shape])\n",
    "    print(f\"{(i*100) / SAMPLES}% Complete\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1011737/1376081311.py:79: DeprecationWarning: This function is deprecated. Please call randint(0, 499 + 1) instead\n",
      "  samples = np.random.random_integers(0, data_cutoff, (data_size, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.001], [0.001], [0.001]]\n",
      "Losses: [0.38963442 0.35096583 0.19497264]\n",
      "Epoch 0, loss: 0.3118576314075884, windowed_loss: 50\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.001], [0.001], [0.001]]\n",
      "Losses: [0.2048214  0.29293824 0.35208603]\n",
      "Epoch 1, loss: 0.2832818902639979, windowed_loss: 50\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.001], [0.001], [0.00095]]\n",
      "Losses: [0.4116378  0.29454883 0.36799184]\n",
      "Epoch 2, loss: 0.35805949069056897, windowed_loss: 50\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.001], [0.001], [0.00095]]\n",
      "Losses: [0.16069549 0.2518031  0.22110625]\n",
      "Epoch 3, loss: 0.21120161215464273, windowed_loss: 0.28418099770306987\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00095], [0.001], [0.00095]]\n",
      "Losses: [0.20552286 0.15721953 0.15613718]\n",
      "Epoch 4, loss: 0.1729598558179999, windowed_loss: 0.24740698622107052\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00095], [0.00095], [0.00095]]\n",
      "Losses: [0.35347234 0.16226083 0.16627027]\n",
      "Epoch 5, loss: 0.22733447865477863, windowed_loss: 0.20383198220914042\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00095], [0.00095], [0.0009025]]\n",
      "Losses: [0.22992423 0.18802019 0.20516068]\n",
      "Epoch 6, loss: 0.20770170311172884, windowed_loss: 0.20266534586150245\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00095], [0.00095], [0.0009025]]\n",
      "Losses: [0.1086654  0.05460014 0.03858508]\n",
      "Epoch 7, loss: 0.06728354114401988, windowed_loss: 0.16743990763684247\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0009025], [0.0009025], [0.0009025]]\n",
      "Losses: [0.22100507 0.19024304 0.13543991]\n",
      "Epoch 8, loss: 0.18222934019589473, windowed_loss: 0.15240486148388113\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0009025], [0.0009025], [0.0009025]]\n",
      "Losses: [0.06911295 0.09506614 0.05058612]\n",
      "Epoch 9, loss: 0.07158840497334798, windowed_loss: 0.10703376210442088\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0009025], [0.0009025], [0.000857375]]\n",
      "Losses: [0.09559807 0.06745471 0.07459842]\n",
      "Epoch 10, loss: 0.07921706602903071, windowed_loss: 0.11101160373275781\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000857375], [0.0009025], [0.000857375]]\n",
      "Losses: [0.16917307 0.11285721 0.18976541]\n",
      "Epoch 11, loss: 0.1572652276357015, windowed_loss: 0.10269023287936006\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000857375], [0.000857375], [0.000857375]]\n",
      "Losses: [0.09738552 0.12391175 0.16089189]\n",
      "Epoch 12, loss: 0.12739638751341464, windowed_loss: 0.12129289372604896\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000857375], [0.000857375], [0.000857375]]\n",
      "Losses: [0.10359352 0.04196935 0.07130636]\n",
      "Epoch 13, loss: 0.07228974509287169, windowed_loss: 0.11898378674732928\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0008145062499999999], [0.000857375], [0.0008145062499999999]]\n",
      "Losses: [0.1455613  0.13954447 0.13306496]\n",
      "Epoch 14, loss: 0.13939024448394777, windowed_loss: 0.11302545903007803\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0008145062499999999], [0.0008145062499999999], [0.0008145062499999999]]\n",
      "Losses: [0.141344   0.15145268 0.16947603]\n",
      "Epoch 15, loss: 0.15409090077789453, windowed_loss: 0.121923630118238\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0008145062499999999], [0.0008145062499999999], [0.0007737809374999998]]\n",
      "Losses: [0.23667146 0.14212975 0.18684725]\n",
      "Epoch 16, loss: 0.1885494871139526, windowed_loss: 0.1606768774585983\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0008145062499999999], [0.0008145062499999999], [0.0007737809374999998]]\n",
      "Losses: [0.15091319 0.11449273 0.12644708]\n",
      "Epoch 17, loss: 0.13061766624450685, windowed_loss: 0.157752684712118\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0007737809374999998], [0.0008145062499999999], [0.0007737809374999998]]\n",
      "Losses: [0.15922968 0.1516093  0.15291607]\n",
      "Epoch 18, loss: 0.15458501512615733, windowed_loss: 0.15791738949487225\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0007737809374999998], [0.0008145062499999999], [0.0007737809374999998]]\n",
      "Losses: [0.08349431 0.04928683 0.05223264]\n",
      "Epoch 19, loss: 0.06167126020432792, windowed_loss: 0.11562464719166403\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0007737809374999998], [0.0007737809374999998], [0.0007350918906249997]]\n",
      "Losses: [0.12372401 0.14544228 0.16146063]\n",
      "Epoch 20, loss: 0.14354230690002442, windowed_loss: 0.11993286074350322\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0007350918906249997], [0.0007737809374999998], [0.0007350918906249997]]\n",
      "Losses: [0.16940925 0.1649522  0.11440707]\n",
      "Epoch 21, loss: 0.14958950640284066, windowed_loss: 0.11826769116906433\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0007350918906249997], [0.0007737809374999998], [0.0007350918906249997]]\n",
      "Losses: [0.07883286 0.08047464 0.08575552]\n",
      "Epoch 22, loss: 0.08168767223077286, windowed_loss: 0.12493982851121266\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0007350918906249997], [0.0007737809374999998], [0.0007350918906249997]]\n",
      "Losses: [0.09875801 0.05255196 0.09076882]\n",
      "Epoch 23, loss: 0.08069292736181134, windowed_loss: 0.10399003533180828\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006983372960937497], [0.0007350918906249997], [0.0007350918906249997]]\n",
      "Losses: [0.10661279 0.09694616 0.07385051]\n",
      "Epoch 24, loss: 0.09246981938680014, windowed_loss: 0.0849501396597948\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006983372960937497], [0.0007350918906249997], [0.0007350918906249997]]\n",
      "Losses: [0.08768348 0.07181065 0.06288962]\n",
      "Epoch 25, loss: 0.07412791730888398, windowed_loss: 0.08243022135249849\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006983372960937497], [0.0007350918906249997], [0.0006983372960937497]]\n",
      "Losses: [0.10624493 0.13663901 0.15114839]\n",
      "Epoch 26, loss: 0.13134411144256591, windowed_loss: 0.09931394937941668\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006983372960937497], [0.0007350918906249997], [0.0006983372960937497]]\n",
      "Losses: [0.06307384 0.07604524 0.10310948]\n",
      "Epoch 27, loss: 0.08074285482945372, windowed_loss: 0.09540496119363452\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006983372960937497], [0.0007350918906249997], [0.0006983372960937497]]\n",
      "Losses: [0.05256775 0.01090462 0.01663241]\n",
      "Epoch 28, loss: 0.026701593081156414, windowed_loss: 0.07959618645105868\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006983372960937497], [0.0006983372960937497], [0.0006983372960937497]]\n",
      "Losses: [0.04445276 0.02220846 0.03461391]\n",
      "Epoch 29, loss: 0.03375837622918363, windowed_loss: 0.047067608046597924\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006634204312890621], [0.0006983372960937497], [0.0006634204312890621]]\n",
      "Losses: [0.15551819 0.13507341 0.15980122]\n",
      "Epoch 30, loss: 0.15013094152631848, windowed_loss: 0.07019697027888618\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006634204312890621], [0.0006983372960937497], [0.0006634204312890621]]\n",
      "Losses: [0.04566693 0.02736591 0.0054783 ]\n",
      "Epoch 31, loss: 0.026170377661246972, windowed_loss: 0.0700198984722497\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006634204312890621], [0.0006634204312890621], [0.0006634204312890621]]\n",
      "Losses: [0.1031827  0.12063816 0.08920677]\n",
      "Epoch 32, loss: 0.1043425453609041, windowed_loss: 0.09354795484948986\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006634204312890621], [0.0006634204312890621], [0.000630249409724609]]\n",
      "Losses: [0.07615823 0.07362812 0.09224636]\n",
      "Epoch 33, loss: 0.08067757066090903, windowed_loss: 0.0703968312276867\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006634204312890621], [0.0006634204312890621], [0.000630249409724609]]\n",
      "Losses: [0.03731997 0.0214623  0.02651053]\n",
      "Epoch 34, loss: 0.028430935853946664, windowed_loss: 0.07115035062525325\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0006634204312890621], [0.0006634204312890621], [0.000630249409724609]]\n",
      "Losses: [0.01040235 0.00468331 0.01695187]\n",
      "Epoch 35, loss: 0.010679177920023602, windowed_loss: 0.039929228144959764\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000630249409724609], [0.0006634204312890621], [0.000630249409724609]]\n",
      "Losses: [0.23623444 0.25246674 0.1847851 ]\n",
      "Epoch 36, loss: 0.22449542458852134, windowed_loss: 0.0878685127874972\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000630249409724609], [0.0006634204312890621], [0.000630249409724609]]\n",
      "Losses: [0.06320795 0.0594251  0.05549578]\n",
      "Epoch 37, loss: 0.059376278395952796, windowed_loss: 0.0981836269681659\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000630249409724609], [0.0006634204312890621], [0.000630249409724609]]\n",
      "Losses: [0.02138765 0.0171377  0.00659254]\n",
      "Epoch 38, loss: 0.01503929583231608, windowed_loss: 0.09963699960559673\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000630249409724609], [0.0006634204312890621], [0.0005987369392383785]]\n",
      "Losses: [0.02658553 0.00463961 0.0275489 ]\n",
      "Epoch 39, loss: 0.01959134772688688, windowed_loss: 0.03133564065171859\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000630249409724609], [0.000630249409724609], [0.0005987369392383785]]\n",
      "Losses: [0.02139574 0.00501665 0.02086923]\n",
      "Epoch 40, loss: 0.015760541519644105, windowed_loss: 0.01679706169294902\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.000630249409724609], [0.000630249409724609], [0.0005987369392383785]]\n",
      "Losses: [0.01335363 0.00735073 0.01088361]\n",
      "Epoch 41, loss: 0.010529322869473485, windowed_loss: 0.015293737372001492\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005987369392383785], [0.0005987369392383785], [0.0005987369392383785]]\n",
      "Losses: [0.09337415 0.0714494  0.05327612]\n",
      "Epoch 42, loss: 0.07269988604681286, windowed_loss: 0.03299658347864348\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005987369392383785], [0.0005987369392383785], [0.0005987369392383785]]\n",
      "Losses: [0.01890955 0.01955119 0.00790303]\n",
      "Epoch 43, loss: 0.015454591115315755, windowed_loss: 0.032894600010534035\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005987369392383785], [0.0005987369392383785], [0.0005688000922764595]]\n",
      "Losses: [0.08123076 0.09727633 0.09475394]\n",
      "Epoch 44, loss: 0.09108700974782308, windowed_loss: 0.05974716230331723\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005987369392383785], [0.0005987369392383785], [0.0005688000922764595]]\n",
      "Losses: [0.02119632 0.01907477 0.00749873]\n",
      "Epoch 45, loss: 0.015923272768656415, windowed_loss: 0.040821624543931755\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005688000922764595], [0.0005688000922764595], [0.0005688000922764595]]\n",
      "Losses: [0.06802336 0.05987725 0.07371347]\n",
      "Epoch 46, loss: 0.06720469543231884, windowed_loss: 0.058071659316266115\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005688000922764595], [0.0005688000922764595], [0.0005688000922764595]]\n",
      "Losses: [0.12091613 0.08449152 0.0650006 ]\n",
      "Epoch 47, loss: 0.09013608519236248, windowed_loss: 0.057754684464445906\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005688000922764595], [0.0005688000922764595], [0.0005403600876626365]]\n",
      "Losses: [0.04830878 0.04277524 0.0747228 ]\n",
      "Epoch 48, loss: 0.055268939080806455, windowed_loss: 0.07086990656849593\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005403600876626365], [0.0005403600876626365]]\n",
      "Losses: [0.1008231  0.11349713 0.10315111]\n",
      "Epoch 49, loss: 0.10582378430858035, windowed_loss: 0.08374293619391643\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005403600876626365], [0.0005403600876626365]]\n",
      "Losses: [0.03677197 0.0490018  0.02974388]\n",
      "Epoch 50, loss: 0.038505884170532224, windowed_loss: 0.06653286918663967\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005403600876626365], [0.0005133420832795047]]\n",
      "Losses: [0.12157775 0.1379231  0.12674434]\n",
      "Epoch 51, loss: 0.12874839719136558, windowed_loss: 0.09102602189015939\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005403600876626365], [0.0005133420832795047]]\n",
      "Losses: [0.06988867 0.04890563 0.05381556]\n",
      "Epoch 52, loss: 0.05753661794668846, windowed_loss: 0.07493029976952875\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005133420832795047], [0.0005133420832795047]]\n",
      "Losses: [0.05207088 0.05256255 0.04518126]\n",
      "Epoch 53, loss: 0.049938229088172435, windowed_loss: 0.07874108140874216\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005133420832795047], [0.0005133420832795047]]\n",
      "Losses: [0.04139616 0.0467126  0.03938943]\n",
      "Epoch 54, loss: 0.042499397595723465, windowed_loss: 0.049991414876861456\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005133420832795047], [0.0005133420832795047]]\n",
      "Losses: [0.01195361 0.0078444  0.0016342 ]\n",
      "Epoch 55, loss: 0.007144071331305038, windowed_loss: 0.033193899338400314\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005403600876626365], [0.0005133420832795047], [0.0005133420832795047]]\n",
      "Losses: [0.00581877 0.00182044 0.        ]\n",
      "Epoch 56, loss: 0.002546405217733728, windowed_loss: 0.017396624714920744\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005133420832795047], [0.0005133420832795047], [0.0005133420832795047]]\n",
      "Losses: [0.076336   0.06991564 0.0602596 ]\n",
      "Epoch 57, loss: 0.06883708269658213, windowed_loss: 0.026175853081873632\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005133420832795047], [0.0005133420832795047], [0.0005133420832795047]]\n",
      "Losses: [0.07405539 0.04835822 0.04745223]\n",
      "Epoch 58, loss: 0.05662194871870613, windowed_loss: 0.042668478877674\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005133420832795047], [0.0005133420832795047], [0.0005133420832795047]]\n",
      "Losses: [0.05067796 0.04276105 0.04650885]\n",
      "Epoch 59, loss: 0.0466492887633827, windowed_loss: 0.057369440059556985\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0005133420832795047], [0.00048767497911552944], [0.00048767497911552944]]\n",
      "Losses: [0.10868407 0.09635701 0.09398727]\n",
      "Epoch 60, loss: 0.09967611789703369, windowed_loss: 0.0676491184597075\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00048767497911552944], [0.00048767497911552944], [0.00048767497911552944]]\n",
      "Losses: [0.13505057 0.13866257 0.15830454]\n",
      "Epoch 61, loss: 0.14400589298867508, windowed_loss: 0.09677709988303047\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00048767497911552944], [0.00048767497911552944], [0.00048767497911552944]]\n",
      "Losses: [0.00312996 0.01043055 0.01386168]\n",
      "Epoch 62, loss: 0.009140727678934734, windowed_loss: 0.08427424618821451\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00048767497911552944], [0.00048767497911552944], [0.00048767497911552944]]\n",
      "Losses: [0.00804327 0.00302631 0.00551387]\n",
      "Epoch 63, loss: 0.005527815183003743, windowed_loss: 0.052891478616871185\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00046329123015975297], [0.00046329123015975297], [0.00046329123015975297]]\n",
      "Losses: [0.05942442 0.05611801 0.06331698]\n",
      "Epoch 64, loss: 0.05961980120340984, windowed_loss: 0.024762781355116104\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00046329123015975297], [0.00046329123015975297], [0.00046329123015975297]]\n",
      "Losses: [0.08826529 0.12233657 0.09274845]\n",
      "Epoch 65, loss: 0.10111677001616759, windowed_loss: 0.05542146213419372\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00046329123015975297], [0.00046329123015975297], [0.00046329123015975297]]\n",
      "Losses: [0.08710461 0.08085576 0.07913922]\n",
      "Epoch 66, loss: 0.08236653168996176, windowed_loss: 0.08103436763651306\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00046329123015975297], [0.0004401266686517653], [0.00046329123015975297]]\n",
      "Losses: [0.07673527 0.08814601 0.06801387]\n",
      "Epoch 67, loss: 0.07763171657850204, windowed_loss: 0.08703833942821047\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00046329123015975297], [0.0004401266686517653], [0.00046329123015975297]]\n",
      "Losses: [0.00390449 0.00242133 0.00599334]\n",
      "Epoch 68, loss: 0.004106385959998033, windowed_loss: 0.05470154474282062\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0004401266686517653], [0.0004401266686517653], [0.0004401266686517653]]\n",
      "Losses: [0.04022952 0.03765532 0.0327    ]\n",
      "Epoch 69, loss: 0.036861611366271974, windowed_loss: 0.03953323796825735\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0004401266686517653], [0.0004401266686517653], [0.0004401266686517653]]\n",
      "Losses: [0.00612977 0.00614778 0.        ]\n",
      "Epoch 70, loss: 0.004092516545542256, windowed_loss: 0.015020171290604086\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0004401266686517653], [0.00041812033521917703], [0.0004401266686517653]]\n",
      "Losses: [0.02214388 0.02097123 0.03083317]\n",
      "Epoch 71, loss: 0.024649427346412386, windowed_loss: 0.021867851752742206\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0004401266686517653], [0.00041812033521917703], [0.0004401266686517653]]\n",
      "Losses: [0.         0.00331176 0.00364144]\n",
      "Epoch 72, loss: 0.002317733075245317, windowed_loss: 0.010353225655733319\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00041812033521917703], [0.00041812033521917703]]\n",
      "Losses: [0.07077254 0.08223983 0.08241542]\n",
      "Epoch 73, loss: 0.07847593363668705, windowed_loss: 0.03514769801944825\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00041812033521917703], [0.00041812033521917703]]\n",
      "Losses: [0.06600949 0.05820993 0.07254571]\n",
      "Epoch 74, loss: 0.0655883783966044, windowed_loss: 0.04879401503617892\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00041812033521917703], [0.00041812033521917703]]\n",
      "Losses: [0.00860315 0.01033218 0.00520249]\n",
      "Epoch 75, loss: 0.008045942526940275, windowed_loss: 0.050703418186743905\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00039721431845821814], [0.00041812033521917703]]\n",
      "Losses: [0.09544355 0.10251565 0.10213902]\n",
      "Epoch 76, loss: 0.10003274091084798, windowed_loss: 0.05788902061146422\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00039721431845821814], [0.00041812033521917703]]\n",
      "Losses: [0.07055985 0.05974263 0.05701293]\n",
      "Epoch 77, loss: 0.0624384708404541, windowed_loss: 0.05683905142608078\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00039721431845821814], [0.00039721431845821814]]\n",
      "Losses: [0.05636084 0.06464729 0.06287077]\n",
      "Epoch 78, loss: 0.06129296779632568, windowed_loss: 0.07458805984920926\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00039721431845821814], [0.00039721431845821814]]\n",
      "Losses: [0.03528211 0.04716866 0.03811956]\n",
      "Epoch 79, loss: 0.040190113277001234, windowed_loss: 0.054640517304593676\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00041812033521917703], [0.00039721431845821814], [0.00039721431845821814]]\n",
      "Losses: [0.00081275 0.00496779 0.00878354]\n",
      "Epoch 80, loss: 0.00485469404856364, windowed_loss: 0.03544592504063019\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00039721431845821814], [0.0003773536025353072], [0.00039721431845821814]]\n",
      "Losses: [0.03328609 0.03266787 0.03177182]\n",
      "Epoch 81, loss: 0.03257525882963029, windowed_loss: 0.025873355385065053\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00039721431845821814], [0.0003773536025353072], [0.0003773536025353072]]\n",
      "Losses: [0.06779586 0.03382019 0.04459487]\n",
      "Epoch 82, loss: 0.048736972018887215, windowed_loss: 0.028722308299027045\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00039721431845821814], [0.0003773536025353072], [0.0003773536025353072]]\n",
      "Losses: [0.00155649 0.00701325 0.01098283]\n",
      "Epoch 83, loss: 0.006517523263831694, windowed_loss: 0.0292765847041164\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003773536025353072], [0.0003584859224085418], [0.0003773536025353072]]\n",
      "Losses: [0.03625766 0.04745613 0.03623857]\n",
      "Epoch 84, loss: 0.039984118245646565, windowed_loss: 0.03174620450945516\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003773536025353072], [0.0003584859224085418], [0.0003773536025353072]]\n",
      "Losses: [0.0052309  0.00124094 0.00130693]\n",
      "Epoch 85, loss: 0.002592922846476237, windowed_loss: 0.016364854785318166\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003773536025353072], [0.0003584859224085418], [0.0003584859224085418]]\n",
      "Losses: [0.06334454 0.05703481 0.06124654]\n",
      "Epoch 86, loss: 0.06054196044942301, windowed_loss: 0.0343730005138486\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003773536025353072], [0.0003584859224085418], [0.0003584859224085418]]\n",
      "Losses: [0.00140846 0.01438215 0.00109301]\n",
      "Epoch 87, loss: 0.005627872630127605, windowed_loss: 0.022920918642008955\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003584859224085418], [0.0003405616262881147], [0.0003584859224085418]]\n",
      "Losses: [0.04137583 0.03684694 0.03666678]\n",
      "Epoch 88, loss: 0.03829651791727048, windowed_loss: 0.03482211699894036\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003584859224085418], [0.0003405616262881147], [0.0003405616262881147]]\n",
      "Losses: [0.0750187  0.04810853 0.04809699]\n",
      "Epoch 89, loss: 0.05707474009195964, windowed_loss: 0.03366637687978591\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003584859224085418], [0.00032353354497370894], [0.0003405616262881147]]\n",
      "Losses: [0.06894563 0.05881241 0.07433669]\n",
      "Epoch 90, loss: 0.06736491310970737, windowed_loss: 0.05424539037297916\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003584859224085418], [0.00032353354497370894], [0.0003405616262881147]]\n",
      "Losses: [0.00087397 0.0009926  0.00264004]\n",
      "Epoch 91, loss: 0.0015022045943286315, windowed_loss: 0.04198061926533187\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003405616262881147], [0.00032353354497370894], [0.00032353354497370894]]\n",
      "Losses: [0.05197632 0.05897235 0.05534521]\n",
      "Epoch 92, loss: 0.05543129447313996, windowed_loss: 0.04143280405905866\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003405616262881147], [0.00032353354497370894], [0.00032353354497370894]]\n",
      "Losses: [0.03690604 0.03680078 0.02930354]\n",
      "Epoch 93, loss: 0.03433678639118243, windowed_loss: 0.030423428486217003\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003405616262881147], [0.00030735686772502346], [0.00032353354497370894]]\n",
      "Losses: [0.05546931 0.05291628 0.05838084]\n",
      "Epoch 94, loss: 0.05558880805969238, windowed_loss: 0.048452296308004926\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0003405616262881147], [0.00030735686772502346], [0.00032353354497370894]]\n",
      "Losses: [0.         0.00490928 0.00215132]\n",
      "Epoch 95, loss: 0.0023535344309223915, windowed_loss: 0.030759709627265732\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00032353354497370894], [0.00030735686772502346], [0.00030735686772502346]]\n",
      "Losses: [0.02519829 0.03386875 0.03462859]\n",
      "Epoch 96, loss: 0.03123187700907389, windowed_loss: 0.029724739833229555\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00032353354497370894], [0.00030735686772502346], [0.00030735686772502346]]\n",
      "Losses: [0.04013009 0.03177176 0.03545493]\n",
      "Epoch 97, loss: 0.03578559318700471, windowed_loss: 0.02312366820900033\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00032353354497370894], [0.00030735686772502346], [0.00030735686772502346]]\n",
      "Losses: [0.02403186 0.02284405 0.02223261]\n",
      "Epoch 98, loss: 0.023036173207009722, windowed_loss: 0.03001788113436277\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00030735686772502346], [0.00029198902433877225], [0.00029198902433877225]]\n",
      "Losses: [0.06577271 0.05315411 0.06107947]\n",
      "Epoch 99, loss: 0.06000209603217258, windowed_loss: 0.039607954142062335\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00030735686772502346], [0.00029198902433877225], [0.00029198902433877225]]\n",
      "Losses: [0.00019093 0.00162626 0.00018567]\n",
      "Epoch 100, loss: 0.0006676223489867104, windowed_loss: 0.027901963862723006\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00030735686772502346], [0.00029198902433877225], [0.00029198902433877225]]\n",
      "Losses: [0.00601622 0.00096953 0.00100651]\n",
      "Epoch 101, loss: 0.0026640901584663464, windowed_loss: 0.021111269513208547\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00029198902433877225], [0.00029198902433877225], [0.00027738957312183364]]\n",
      "Losses: [0.10545468 0.10683921 0.09870619]\n",
      "Epoch 102, loss: 0.10366669382822856, windowed_loss: 0.035666135445227204\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00029198902433877225], [0.00029198902433877225], [0.00027738957312183364]]\n",
      "Losses: [0.03215456 0.03738187 0.03514698]\n",
      "Epoch 103, loss: 0.034894473741013045, windowed_loss: 0.04707508590923598\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00029198902433877225], [0.00029198902433877225], [0.00027738957312183364]]\n",
      "Losses: [0.02339111 0.02125282 0.02047502]\n",
      "Epoch 104, loss: 0.021706314086914064, windowed_loss: 0.053422493885385225\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00029198902433877225], [0.00027738957312183364], [0.00027738957312183364]]\n",
      "Losses: [0.03866667 0.03669596 0.03472314]\n",
      "Epoch 105, loss: 0.036695254191447674, windowed_loss: 0.031098680673124924\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00027738957312183364], [0.00027738957312183364], [0.0002635200944657419]]\n",
      "Losses: [0.07804135 0.07068464 0.06592036]\n",
      "Epoch 106, loss: 0.07154878332341459, windowed_loss: 0.04331678386725877\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00027738957312183364], [0.00027738957312183364], [0.0002635200944657419]]\n",
      "Losses: [0.05651557 0.05017929 0.05142992]\n",
      "Epoch 107, loss: 0.05270825779262476, windowed_loss: 0.053650765102495675\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00027738957312183364], [0.00027738957312183364], [0.0002635200944657419]]\n",
      "Losses: [0.00087271 0.00013214 0.00203555]\n",
      "Epoch 108, loss: 0.0010134665171305337, windowed_loss: 0.041756835877723296\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00027738957312183364], [0.0002635200944657419], [0.0002635200944657419]]\n",
      "Losses: [0.03389643 0.03867409 0.0328704 ]\n",
      "Epoch 109, loss: 0.03514697360992431, windowed_loss: 0.02962289930655987\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00027738957312183364], [0.0002635200944657419], [0.0002635200944657419]]\n",
      "Losses: [0.03175944 0.03045427 0.02600738]\n",
      "Epoch 110, loss: 0.02940702978769938, windowed_loss: 0.021855823304918073\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00027738957312183364], [0.0002635200944657419], [0.0002503440897424548]]\n",
      "Losses: [0.030586   0.03285968 0.03070831]\n",
      "Epoch 111, loss: 0.03138466181365681, windowed_loss: 0.03197955507042683\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00027738957312183364], [0.0002635200944657419], [0.0002503440897424548]]\n",
      "Losses: [2.88558150e-05 1.36701664e-03 1.64406381e-03]\n",
      "Epoch 112, loss: 0.0010133120881452033, windowed_loss: 0.020601667896500465\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002635200944657419], [0.0002503440897424548], [0.0002503440897424548]]\n",
      "Losses: [0.05086669 0.02706818 0.03363399]\n",
      "Epoch 113, loss: 0.03718961869866034, windowed_loss: 0.023195864200154116\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002635200944657419], [0.0002503440897424548], [0.0002503440897424548]]\n",
      "Losses: [0.03843593 0.03915555 0.03298162]\n",
      "Epoch 114, loss: 0.03685769894637501, windowed_loss: 0.025020209911060184\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002635200944657419], [0.0002503440897424548], [0.00023782688525533205]]\n",
      "Losses: [0.037798   0.03499292 0.04443067]\n",
      "Epoch 115, loss: 0.03907386541525842, windowed_loss: 0.037707061020097916\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002635200944657419], [0.0002503440897424548], [0.00023782688525533205]]\n",
      "Losses: [0.         0.00142152 0.00157853]\n",
      "Epoch 116, loss: 0.0010000170272592712, windowed_loss: 0.02564386046296423\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002635200944657419], [0.00023782688525533205], [0.00023782688525533205]]\n",
      "Losses: [0.06805668 0.06444253 0.0707502 ]\n",
      "Epoch 117, loss: 0.06774980476103633, windowed_loss: 0.035941229067851334\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002635200944657419], [0.00023782688525533205], [0.00023782688525533205]]\n",
      "Losses: [0.00197548 0.         0.        ]\n",
      "Epoch 118, loss: 0.0006584942420642857, windowed_loss: 0.023136105343453294\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002503440897424548], [0.00023782688525533205], [0.00022593554099256544]]\n",
      "Losses: [0.08962016 0.09014132 0.0876229 ]\n",
      "Epoch 119, loss: 0.08912812519073486, windowed_loss: 0.052512141397945154\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002503440897424548], [0.00023782688525533205], [0.00022593554099256544]]\n",
      "Losses: [0.05208605 0.05441848 0.05351187]\n",
      "Epoch 120, loss: 0.05333879938425027, windowed_loss: 0.04770847293901647\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002503440897424548], [0.00023782688525533205], [0.00022593554099256544]]\n",
      "Losses: [0.02819582 0.03772087 0.03042057]\n",
      "Epoch 121, loss: 0.03211241912841797, windowed_loss: 0.05819311456780104\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002503440897424548], [0.00023782688525533205], [0.00022593554099256544]]\n",
      "Losses: [0.00072678 0.00423862 0.00042986]\n",
      "Epoch 122, loss: 0.0017984223986914256, windowed_loss: 0.029083213637119885\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002503440897424548], [0.00023782688525533205], [0.00022593554099256544]]\n",
      "Losses: [0.0006387  0.00044589 0.        ]\n",
      "Epoch 123, loss: 0.00036152998606363935, windowed_loss: 0.011424123837724344\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002503440897424548], [0.00022593554099256544], [0.00022593554099256544]]\n",
      "Losses: [0.03056656 0.03305183 0.02822262]\n",
      "Epoch 124, loss: 0.030613669253383064, windowed_loss: 0.010924540546046043\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002503440897424548], [0.00022593554099256544], [0.00022593554099256544]]\n",
      "Losses: [0.02229409 0.02057441 0.02137936]\n",
      "Epoch 125, loss: 0.021415954123518034, windowed_loss: 0.017463717787654914\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00023782688525533205], [0.00022593554099256544], [0.00021463876394293716]]\n",
      "Losses: [0.03282343 0.02912803 0.03167711]\n",
      "Epoch 126, loss: 0.031209524154663087, windowed_loss: 0.027746382510521393\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00023782688525533205], [0.00022593554099256544], [0.00021463876394293716]]\n",
      "Losses: [0.00043313 0.         0.00150421]\n",
      "Epoch 127, loss: 0.00064577794617598, windowed_loss: 0.017757085408119034\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00023782688525533205], [0.00021463876394293716], [0.00021463876394293716]]\n",
      "Losses: [0.02617999 0.03089312 0.02574408]\n",
      "Epoch 128, loss: 0.02760572885782144, windowed_loss: 0.019820343652886836\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00022593554099256544], [0.00021463876394293716], [0.0002039068257457903]]\n",
      "Losses: [0.05199622 0.0462606  0.04655857]\n",
      "Epoch 129, loss: 0.048271794637044264, windowed_loss: 0.025507767147013898\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00022593554099256544], [0.0002039068257457903], [0.0002039068257457903]]\n",
      "Losses: [0.10479688 0.08333636 0.08052512]\n",
      "Epoch 130, loss: 0.08955278778076171, windowed_loss: 0.0551434370918758\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00022593554099256544], [0.0002039068257457903], [0.0002039068257457903]]\n",
      "Losses: [0.06791242 0.05900838 0.07396524]\n",
      "Epoch 131, loss: 0.06696201342300485, windowed_loss: 0.06826219861360361\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00022593554099256544], [0.0002039068257457903], [0.0002039068257457903]]\n",
      "Losses: [0.00012537 0.00167755 0.00155134]\n",
      "Epoch 132, loss: 0.001118084832208064, windowed_loss: 0.05254429534532488\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00021463876394293716], [0.0002039068257457903], [0.00019371148445850077]]\n",
      "Losses: [0.0613978  0.06530175 0.05738073]\n",
      "Epoch 133, loss: 0.061360092367353535, windowed_loss: 0.04314673020752215\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00021463876394293716], [0.0002039068257457903], [0.00019371148445850077]]\n",
      "Losses: [0.02564392 0.02       0.02276405]\n",
      "Epoch 134, loss: 0.02280265935262044, windowed_loss: 0.02842694551739401\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00021463876394293716], [0.00019371148445850077], [0.00019371148445850077]]\n",
      "Losses: [0.05155836 0.0510753  0.04587952]\n",
      "Epoch 135, loss: 0.049504396353233, windowed_loss: 0.04455571602440233\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002039068257457903], [0.00019371148445850077], [0.00018402591023557573]]\n",
      "Losses: [0.06155315 0.062417   0.05290547]\n",
      "Epoch 136, loss: 0.05895854061893726, windowed_loss: 0.04375519877493023\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002039068257457903], [0.00019371148445850077], [0.00018402591023557573]]\n",
      "Losses: [0.         0.00216165 0.00176013]\n",
      "Epoch 137, loss: 0.0013072618359383171, windowed_loss: 0.036590066269369524\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002039068257457903], [0.00018402591023557573], [0.00018402591023557573]]\n",
      "Losses: [0.05200511 0.05629886 0.05976379]\n",
      "Epoch 138, loss: 0.05602258659270872, windowed_loss: 0.038762796349194766\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002039068257457903], [0.00018402591023557573], [0.00018402591023557573]]\n",
      "Losses: [0.03165566 0.03180664 0.03232203]\n",
      "Epoch 139, loss: 0.031928108307068236, windowed_loss: 0.02975265224523842\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0002039068257457903], [0.00018402591023557573], [0.00018402591023557573]]\n",
      "Losses: [1.21450424e-05 5.93119621e-04 9.32245255e-04]\n",
      "Epoch 140, loss: 0.0005125033060709635, windowed_loss: 0.02948773273528264\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00019371148445850077], [0.00018402591023557573], [0.00017482461472379692]]\n",
      "Losses: [0.03671806 0.02898758 0.02443155]\n",
      "Epoch 141, loss: 0.030045731671268967, windowed_loss: 0.020828781094802722\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00019371148445850077], [0.00017482461472379692], [0.00017482461472379692]]\n",
      "Losses: [0.05232801 0.05479096 0.05725966]\n",
      "Epoch 142, loss: 0.054792875586104996, windowed_loss: 0.028450370187814975\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00019371148445850077], [0.00017482461472379692], [0.00017482461472379692]]\n",
      "Losses: [0.02       0.02122361 0.02019893]\n",
      "Epoch 143, loss: 0.020474177678426107, windowed_loss: 0.03510426164526669\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00018402591023557573], [0.00017482461472379692], [0.00016608338398760707]]\n",
      "Losses: [0.04729415 0.04850384 0.05027129]\n",
      "Epoch 144, loss: 0.04868975813259821, windowed_loss: 0.041318937132376436\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00018402591023557573], [0.00017482461472379692], [0.00016608338398760707]]\n",
      "Losses: [0.05097212 0.04621091 0.04577875]\n",
      "Epoch 145, loss: 0.04765392589569092, windowed_loss: 0.038939287235571746\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00018402591023557573], [0.00017482461472379692], [0.00016608338398760707]]\n",
      "Losses: [0.02637332 0.02674746 0.02507122]\n",
      "Epoch 146, loss: 0.026063999018035258, windowed_loss: 0.04080256101544146\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00018402591023557573], [0.00017482461472379692], [0.00016608338398760707]]\n",
      "Losses: [0.02078624 0.02018843 0.02086497]\n",
      "Epoch 147, loss: 0.02061321166808715, windowed_loss: 0.031443712193937774\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00017482461472379692], [0.00016608338398760707], [0.00016608338398760707]]\n",
      "Losses: [0.05447947 0.05088537 0.04804832]\n",
      "Epoch 148, loss: 0.05113771992674474, windowed_loss: 0.03260497687095571\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00017482461472379692], [0.00016608338398760707], [0.0001577792147882267]]\n",
      "Losses: [0.04955498 0.04617836 0.05074252]\n",
      "Epoch 149, loss: 0.04882528759549101, windowed_loss: 0.04019207306344097\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00017482461472379692], [0.00016608338398760707], [0.0001577792147882267]]\n",
      "Losses: [0.02908278 0.03086309 0.02651937]\n",
      "Epoch 150, loss: 0.02882174897687628, windowed_loss: 0.042928252166370674\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00017482461472379692], [0.00016608338398760707], [0.0001577792147882267]]\n",
      "Losses: [0.         0.         0.00056117]\n",
      "Epoch 151, loss: 0.00018705622355143228, windowed_loss: 0.025944697598639575\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00017482461472379692], [0.00016608338398760707], [0.0001577792147882267]]\n",
      "Losses: [0. 0. 0.]\n",
      "Epoch 152, loss: 0.0, windowed_loss: 0.009669601733475903\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00016608338398760707], [0.0001577792147882267], [0.0001577792147882267]]\n",
      "Losses: [0.08316641 0.08766296 0.08903992]\n",
      "Epoch 153, loss: 0.08662309754906133, windowed_loss: 0.028936717924204255\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00016608338398760707], [0.0001577792147882267], [0.0001577792147882267]]\n",
      "Losses: [0.02622445 0.02908595 0.02594131]\n",
      "Epoch 154, loss: 0.027083904646043388, windowed_loss: 0.03790233406503491\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00016608338398760707], [0.0001577792147882267], [0.0001577792147882267]]\n",
      "Losses: [0.02071701 0.02164783 0.02061235]\n",
      "Epoch 155, loss: 0.0209923985009203, windowed_loss: 0.04489980023200835\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00016608338398760707], [0.0001577792147882267], [0.00014989025404881537]]\n",
      "Losses: [0.02338633 0.03127136 0.02472079]\n",
      "Epoch 156, loss: 0.026459492598682067, windowed_loss: 0.024845265248548586\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0001577792147882267], [0.0001577792147882267], [0.00014989025404881537]]\n",
      "Losses: [0.02820925 0.02951627 0.02947313]\n",
      "Epoch 157, loss: 0.02906621704916995, windowed_loss: 0.025506036049590772\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0001577792147882267], [0.0001577792147882267], [0.00014989025404881537]]\n",
      "Losses: [0.02150064 0.02004008 0.02035233]\n",
      "Epoch 158, loss: 0.020631013907188564, windowed_loss: 0.02538557451834686\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0001577792147882267], [0.0001577792147882267], [0.00014989025404881537]]\n",
      "Losses: [0.02 0.02 0.02]\n",
      "Epoch 159, loss: 0.02, windowed_loss: 0.023232410318786173\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.0001577792147882267], [0.00014989025404881537], [0.00014239574134637458]]\n",
      "Losses: [0.02962193 0.02658554 0.02977054]\n",
      "Epoch 160, loss: 0.028659335119213036, windowed_loss: 0.023096783008800536\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014989025404881537], [0.00014989025404881537], [0.00014239574134637458]]\n",
      "Losses: [0.04082846 0.02437035 0.02461791]\n",
      "Epoch 161, loss: 0.02993890453993518, windowed_loss: 0.026199413219716073\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014989025404881537], [0.00014989025404881537], [0.00014239574134637458]]\n",
      "Losses: [0.00104827 0.         0.00011518]\n",
      "Epoch 162, loss: 0.0003878192569676492, windowed_loss: 0.01966201963870529\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014989025404881537], [0.00014989025404881537], [0.00014239574134637458]]\n",
      "Losses: [0.02528052 0.02796188 0.0288465 ]\n",
      "Epoch 163, loss: 0.027362964558633296, windowed_loss: 0.01922989611851204\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014239574134637458], [0.00014239574134637458], [0.00013527595427905584]]\n",
      "Losses: [0.07828293 0.07829972 0.07348771]\n",
      "Epoch 164, loss: 0.07669011895148532, windowed_loss: 0.034813634255695426\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014239574134637458], [0.00014239574134637458], [0.00013527595427905584]]\n",
      "Losses: [0.0287417  0.02692095 0.02905689]\n",
      "Epoch 165, loss: 0.028239847105506588, windowed_loss: 0.04409764353854173\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014239574134637458], [0.00014239574134637458], [0.00013527595427905584]]\n",
      "Losses: [0.00167188 0.00215076 0.        ]\n",
      "Epoch 166, loss: 0.0012742101787804124, windowed_loss: 0.035401392078590777\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014239574134637458], [0.00014239574134637458], [0.00013527595427905584]]\n",
      "Losses: [0.02640011 0.03349547 0.02541882]\n",
      "Epoch 167, loss: 0.028438132603963218, windowed_loss: 0.019317396629416738\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00014239574134637458], [0.00014239574134637458], [0.00013527595427905584]]\n",
      "Losses: [0.00000000e+00 0.00000000e+00 2.59399414e-07]\n",
      "Epoch 168, loss: 8.646647135416667e-08, windowed_loss: 0.009904143083071661\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00013527595427905584], [0.00013527595427905584], [0.00012851215656510304]]\n",
      "Losses: [0.07558679 0.08078351 0.07191352]\n",
      "Epoch 169, loss: 0.07609460858718985, windowed_loss: 0.034844275885874805\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00013527595427905584], [0.00013527595427905584], [0.00012851215656510304]]\n",
      "Losses: [0.00079975 0.00120572 0.00080061]\n",
      "Epoch 170, loss: 0.0009353617946545761, windowed_loss: 0.025676685616105258\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00013527595427905584], [0.00013527595427905584], [0.00012851215656510304]]\n",
      "Losses: [0.        0.0002194 0.0042965]\n",
      "Epoch 171, loss: 0.0015053003926472246, windowed_loss: 0.026178423591497215\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00013527595427905584], [0.00013527595427905584], [0.00012851215656510304]]\n",
      "Losses: [0.00022809 0.00088414 0.        ]\n",
      "Epoch 172, loss: 0.00037074248634348596, windowed_loss: 0.0009371348912150954\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00012851215656510304], [0.00012851215656510304], [0.00012208654873684788]]\n",
      "Losses: [0.02759755 0.03597699 0.02786776]\n",
      "Epoch 173, loss: 0.030480765149684133, windowed_loss: 0.010785602676224948\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00012851215656510304], [0.00012851215656510304], [0.00012208654873684788]]\n",
      "Losses: [0.05137869 0.05481297 0.04892228]\n",
      "Epoch 174, loss: 0.051704647064208985, windowed_loss: 0.027518718233412204\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00012851215656510304], [0.00012851215656510304], [0.00012208654873684788]]\n",
      "Losses: [0.00010005 0.00073557 0.        ]\n",
      "Epoch 175, loss: 0.0002785373723490969, windowed_loss: 0.027487983195414067\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00012208654873684788], [0.00012208654873684788], [0.00011598222130000548]]\n",
      "Losses: [0.02004008 0.0203424  0.02013698]\n",
      "Epoch 176, loss: 0.020173155632350313, windowed_loss: 0.024052113356302796\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00012208654873684788], [0.00012208654873684788], [0.00011598222130000548]]\n",
      "Losses: [0.02079794 0.02117358 0.02004008]\n",
      "Epoch 177, loss: 0.020670535648832658, windowed_loss: 0.013707409551177357\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011598222130000548], [0.00011598222130000548], [0.00011598222130000548]]\n",
      "Losses: [0.0462038  0.05317776 0.0464837 ]\n",
      "Epoch 178, loss: 0.04862175441011687, windowed_loss: 0.029821815230433275\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011598222130000548], [0.00011598222130000548], [0.00011598222130000548]]\n",
      "Losses: [0.04494268 0.04837877 0.04578607]\n",
      "Epoch 179, loss: 0.04636917547456567, windowed_loss: 0.038553821844505065\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011598222130000548], [0.00011598222130000548], [0.00011598222130000548]]\n",
      "Losses: [0.0247268  0.02507319 0.02490375]\n",
      "Epoch 180, loss: 0.024901247024536135, windowed_loss: 0.039964058969739556\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011598222130000548], [0.00011598222130000548], [0.00011598222130000548]]\n",
      "Losses: [0.00059576 0.         0.00028775]\n",
      "Epoch 181, loss: 0.0002945048014322917, windowed_loss: 0.0238549757668447\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011598222130000548], [0.00011598222130000548], [0.00011598222130000548]]\n",
      "Losses: [0.        0.0016591 0.       ]\n",
      "Epoch 182, loss: 0.0005530326019430385, windowed_loss: 0.008582928142637156\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011598222130000548], [0.00011018311023500519], [0.00011018311023500519]]\n",
      "Losses: [0.02376458 0.02380451 0.02285468]\n",
      "Epoch 183, loss: 0.023474588820993222, windowed_loss: 0.00810737540812285\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011018311023500519], [0.00011018311023500519], [0.00011018311023500519]]\n",
      "Losses: [0.07355779 0.07419134 0.07692836]\n",
      "Epoch 184, loss: 0.07489249637784684, windowed_loss: 0.032973372600261036\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011018311023500519], [0.00011018311023500519], [0.00011018311023500519]]\n",
      "Losses: [0.05130932 0.051483   0.05182231]\n",
      "Epoch 185, loss: 0.05153820856188834, windowed_loss: 0.04996843125357614\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011018311023500519], [0.00010467395472325493], [0.00011018311023500519]]\n",
      "Losses: [0.05841719 0.05715184 0.05157092]\n",
      "Epoch 186, loss: 0.05571331532796225, windowed_loss: 0.06071467342256581\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00011018311023500519], [0.00010467395472325493], [0.00011018311023500519]]\n",
      "Losses: [0.02359218 0.02494951 0.02250165]\n",
      "Epoch 187, loss: 0.023681113141810966, windowed_loss: 0.043644212343887184\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00010467395472325493], [0.00010467395472325493], [0.00010467395472325493]]\n",
      "Losses: [0.03727456 0.0276615  0.02837371]\n",
      "Epoch 188, loss: 0.031103259086608884, windowed_loss: 0.03683256251879403\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00010467395472325493], [0.00010467395472325493], [0.00010467395472325493]]\n",
      "Losses: [0.00191056 0.0003986  0.        ]\n",
      "Epoch 189, loss: 0.000769720390928743, windowed_loss: 0.0185180308731162\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00010467395472325493], [9.944025698709218e-05], [0.00010467395472325493]]\n",
      "Losses: [0.02708844 0.02344208 0.02535058]\n",
      "Epoch 190, loss: 0.025293701942985154, windowed_loss: 0.019055560473507593\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[0.00010467395472325493], [9.944025698709218e-05], [0.00010467395472325493]]\n",
      "Losses: [0.         0.0007951  0.00098443]\n",
      "Epoch 191, loss: 0.0005931745948358306, windowed_loss: 0.008885532309583243\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[9.944025698709218e-05], [9.944025698709218e-05], [0.00010467395472325493]]\n",
      "Losses: [5.95672607e-04 4.05998230e-05 3.83465767e-04]\n",
      "Epoch 192, loss: 0.00033991273244222004, windowed_loss: 0.008742263090087734\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 4.0\n",
      "Unsupervised Training.. 8.0\n",
      "Unsupervised Training.. 12.0\n",
      "Unsupervised Training.. 16.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 24.0\n",
      "Unsupervised Training.. 28.0\n",
      "Unsupervised Training.. 32.0\n",
      "Unsupervised Training.. 36.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 44.0\n",
      "Unsupervised Training.. 48.0\n",
      "Unsupervised Training.. 52.0\n",
      "Unsupervised Training.. 56.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 64.0\n",
      "Unsupervised Training.. 68.0\n",
      "Unsupervised Training.. 72.0\n",
      "Unsupervised Training.. 76.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 84.0\n",
      "Unsupervised Training.. 88.0\n",
      "Unsupervised Training.. 92.0\n",
      "Unsupervised Training.. 96.0\n",
      "LR: [[9.944025698709218e-05], [9.944025698709218e-05], [0.00010467395472325493]]\n",
      "Losses: [0.         0.         0.00023576]\n",
      "Epoch 193, loss: 7.858543931123967e-05, windowed_loss: 0.00033722425552976343\n",
      "Total Pre-training Time: 889.1168539524078\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "\n",
    "PRETRAINING = True\n",
    "target = 0.0005\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ensemble = Ensemble(size=3, output_size=5, lr_series=[10e-4, 10e-4, 10e-4], learning_decay=0.95, decay_step=2, threshold=9.0, weight_decay=1e-4, new_model=True)\n",
    "ensemble.load_ensemble(\"full-mini-CLR-2\")\n",
    "sampled_dataset = SwarmDataset(\"data/tinytoy\", rank=0)\n",
    "\n",
    "def resizeInput(X, w=200):\n",
    "    frame = X.astype(np.uint8)\n",
    "    resized = cv2.resize(frame, dsize=(w, w), interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def translate(img, offset=(10, 10)):\n",
    "    h, w = img.shape\n",
    "    xoff, yoff = offset\n",
    "    if xoff < 0: xpadding = (0, -xoff)\n",
    "    else: xpadding = (xoff, 0)\n",
    "    if yoff < 0: ypadding = (0, -yoff)\n",
    "    else: ypadding = (yoff, 0)\n",
    "    img = np.pad(img, (xpadding, ypadding))\n",
    "\n",
    "    if xoff >= 0 and yoff >= 0:\n",
    "        return img[:w, :w]\n",
    "    elif xoff < 0 and yoff >= 0:\n",
    "        return img[-w:, :w]\n",
    "    elif xoff >= 0 and yoff < 0:\n",
    "        return img[:w, -w:]\n",
    "    return img[-w:, -w:]\n",
    "\n",
    "def zoom_at(img, zoom, coord=None):\n",
    "    # Adapted from https://stackoverflow.com/questions/69050464/zoom-into-image-with-opencv\n",
    "    h, w = [ zoom * i for i in img.shape ]\n",
    "    if coord is None: cx, cy = w/2, h/2\n",
    "    else: cx, cy = [ zoom*c for c in coord ]\n",
    "    img = cv2.resize( img, (0, 0), fx=zoom, fy=zoom)\n",
    "    img = img[ int(round(cy - h/zoom * .5)) : int(round(cy + h/zoom * .5)),\n",
    "               int(round(cx - w/zoom * .5)) : int(round(cx + w/zoom * .5))]\n",
    "    return img\n",
    "\n",
    "def getRandomTransformation(image):\n",
    "    transformation_choices = [\"Rotation\", \"Blur\", \"Zoom\", \"Translate\"]\n",
    "    weights = [0.4, 0.3, 0.0, 0.3]\n",
    "    # weights = [1.0, 0.0, 0.0, 0.0]\n",
    "    choice = random.choices(transformation_choices, weights, k=1)[0]\n",
    "    if choice == \"Rotation\":\n",
    "        theta = random.choice([90, 180, 270])\n",
    "        return ndimage.rotate(image, theta)\n",
    "    elif choice == \"Blur\":\n",
    "        blur = random.choice([0.5, 1.0, 1.5])\n",
    "        return ndimage.gaussian_filter(image, sigma=blur)\n",
    "    elif choice == \"Zoom\":\n",
    "        # zoom = random.choice([1.06, 1.12, 1.18])\n",
    "        padding = random.choice([10])\n",
    "        padded = np.pad(image, padding, mode='constant')\n",
    "        return resizeInput(padded, 50)\n",
    "    elif choice == \"Translate\":\n",
    "        # offsets = [i for i in range(-10, 10, 2)]\n",
    "        # offset = (random.choice(offsets), random.choice(offsets))\n",
    "        offset = (2, 2)\n",
    "        return translate(image, offset)\n",
    "\n",
    "def pretraining(data, ensemble, data_cutoff=None, data_size=500):\n",
    "    if data_cutoff is None:\n",
    "        data_cutoff = len(data) - 1\n",
    "    np.random.seed(0)\n",
    "    samples = np.random.random_integers(0, data_cutoff, (data_size, 2))\n",
    "    total_loss = np.array([0.0 for i in range(len(ensemble.ensemble))])\n",
    "    total_updates = 0\n",
    "    BATCH_SIZE = 4\n",
    "\n",
    "    pull_set = [k for k in range(len(samples))]\n",
    "    random.shuffle(pull_set)\n",
    "    for index in range(0, len(pull_set), BATCH_SIZE):\n",
    "        i = pull_set[index]\n",
    "        if total_updates % 20 == 0:\n",
    "            print(f\"Unsupervised Training.. {(total_updates * BATCH_SIZE * 100) / data_size}\")\n",
    "\n",
    "        AUGMENT_SIZE = 1\n",
    "        if i + (BATCH_SIZE * AUGMENT_SIZE) >= len(pull_set):\n",
    "            continue\n",
    "\n",
    "        temp_losses = np.array([0.0 for _ in ensemble.ensemble])\n",
    "\n",
    "        anchors = np.array([data[samples[i + (j % AUGMENT_SIZE)][0]][0] for j in range(AUGMENT_SIZE * BATCH_SIZE)])\n",
    "        positives = np.array([getRandomTransformation(data[samples[i + (j % AUGMENT_SIZE)][0]][0]) for j in range(AUGMENT_SIZE * BATCH_SIZE)])\n",
    "        negatives = np.array([data[samples[i + (j % AUGMENT_SIZE)][1]][0] for j in range(AUGMENT_SIZE * BATCH_SIZE)])\n",
    "\n",
    "        anchors = np.expand_dims(anchors, axis=1)\n",
    "        positives = np.expand_dims(positives, axis=1)\n",
    "        negatives = np.expand_dims(negatives, axis=1)\n",
    "\n",
    "        losses = ensemble.train_batch(anchors, positives, negatives)\n",
    "        temp_losses += losses\n",
    "\n",
    "        total_loss += temp_losses\n",
    "        total_updates += 1\n",
    "\n",
    "    return total_loss, total_updates\n",
    "\n",
    "t_1 = time.time()\n",
    "if PRETRAINING:\n",
    "    epochs = 0\n",
    "    loss_history = []\n",
    "    while loss > target:\n",
    "        losses, total_updates = pretraining(sampled_dataset, ensemble, data_cutoff=None, data_size=2000)\n",
    "        average_loss = losses / total_updates\n",
    "        lr = ensemble.evaluate_lr(average_loss)\n",
    "        locale_loss = sum(average_loss) / len(average_loss)\n",
    "        loss_history.append(locale_loss)\n",
    "        loss = (sum(loss_history[-3:]) / 3) if len(loss_history) > 3 else 50\n",
    "        print(f\"LR: {lr}\")\n",
    "        print(f\"Losses: {average_loss}\")\n",
    "        print(f\"Epoch {epochs}, loss: {locale_loss}, windowed_loss: {loss}\")\n",
    "        epochs += 1\n",
    "\n",
    "print(f\"Total Pre-training Time: {time.time() - t_1}\")\n",
    "ensemble.save_ensemble(f\"{int(time.time())}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(f\"{int(time.time())}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [11.54286151 11.83442778 12.2440053 ], LR: [[0.0015], [0.0015], [0.0015]], Loss: 11.873764862219494\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [10.03131055 10.08217909 10.07937417], LR: [[0.0015], [0.0015], [0.0015]], Loss: 10.064287937482199\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [9.55167725 9.97975469 9.74495038], LR: [[0.0015], [0.0015], [0.0015]], Loss: 9.75879410425822\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [9.71152709 9.95000814 9.47615086], LR: [[0.0015], [0.0015], [0.0015]], Loss: 9.712562029361726\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [ 9.33350379 10.01450524  9.46293617], LR: [[0.0015], [0.0015], [0.0015]], Loss: 9.603648402690887\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [9.09710578 9.9947085  8.80587229], LR: [[0.0015], [0.0015], [0.0015]], Loss: 9.299228858153027\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [8.91026346 9.93509552 9.16031048], LR: [[0.0015], [0.0015], [0.0015]], Loss: 9.335223155021668\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [8.40066546 9.92950411 8.87925193], LR: [[0.0015], [0.0015], [0.0015]], Loss: 9.06980716784795\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [8.64950223 9.92057007 8.2323114 ], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.934127903779348\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [8.25686075 9.97247553 8.71675787], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.982031385103863\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [ 7.70013315 10.00120161  8.18092563], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.627420132160188\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [8.12822819 9.97619547 8.60479373], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.903072460889817\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [8.05902464 9.87593992 7.47488219], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.469948918819426\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [7.43787046 9.8785648  6.96576973], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.094068327744802\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [7.50198532 9.94169431 7.00908328], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.150920967062314\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [7.75968551 9.94537359 6.90227356], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.20244422107935\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [8.0155704  9.97623401 7.03152498], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.341109794538157\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [7.45088198 9.97293859 6.59897108], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.00759721716245\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [ 7.41986846 10.03413103  6.89673871], LR: [[0.0015], [0.0015], [0.0015]], Loss: 8.116912733217081\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [7.01900979 9.87267232 6.77253819], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.888073434829711\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [6.75211152 9.90436475 6.10641381], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.587630023901972\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [6.84182326 9.96503257 6.53013845], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.778998093927901\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [6.60221964 9.87453487 6.25555417], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.57743622769912\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [ 6.61524423 10.0143615   5.67933995], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.436315227349599\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [6.05512802 9.92812652 5.64197248], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.208409004497031\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [6.46452467 9.92842177 5.63036863], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.3411050231009725\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [6.04868072 9.88512778 6.05260224], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.32880357970794\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.85153017 9.97711331 5.66931113], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.1659848711888\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.71714169 9.90998143 5.93541486], LR: [[0.0015], [0.0015], [0.0015]], Loss: 7.187512661119302\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.17634177 9.91243326 5.13218629], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.740320438724011\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.47215611 9.98466002 5.06872254], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.8418462212321645\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.18910736 9.66770233 5.42388403], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.760231236442923\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [6.00550227 9.68973694 5.19079848], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.962012565607826\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.9230442  9.61075416 5.17789245], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.903896935159961\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.19140284 8.62065623 4.46104732], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.091035462617874\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.04267646 9.3334226  5.33882548], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.571641515096029\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.11620723 8.75962962 4.77230551], LR: [[0.0015], [0.0015], [0.0015]], Loss: 6.2160474531725045\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.12174833 8.64686213 5.26316168], LR: [[0.0015], [0.0015], [0.00135]], Loss: 6.343924048667152\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.37080814 8.39948611 4.79166438], LR: [[0.00135], [0.0015], [0.00135]], Loss: 6.187319543361664\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.42898299 8.1489195  5.14903633], LR: [[0.00135], [0.0015], [0.00135]], Loss: 5.90897960462297\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.58388237 7.9007268  4.17938498], LR: [[0.00135], [0.0015], [0.00135]], Loss: 5.5546647169627255\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [5.15503706 7.68190894 4.7954903 ], LR: [[0.00135], [0.0015], [0.00135]], Loss: 5.877478765261671\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.39680599 6.87344503 4.78334437], LR: [[0.00135], [0.0015], [0.00135]], Loss: 5.351198460410039\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.45101414 6.80579298 4.67555413], LR: [[0.00135], [0.0015], [0.00135]], Loss: 5.31078708315889\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.60101609 7.17436835 4.01219123], LR: [[0.0012150000000000002], [0.0015], [0.00135]], Loss: 5.262525220972797\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.55802324 7.44657398 4.35004481], LR: [[0.0012150000000000002], [0.0015], [0.00135]], Loss: 5.45154734581088\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.32874249 6.62375756 5.93277549], LR: [[0.0012150000000000002], [0.0015], [0.0012150000000000002]], Loss: 5.628425180216631\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.69763012 6.70571937 4.23862952], LR: [[0.0012150000000000002], [0.0015], [0.0012150000000000002]], Loss: 4.880659667326448\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.09107905 6.63479654 4.0612635 ], LR: [[0.0012150000000000002], [0.0015], [0.0012150000000000002]], Loss: 4.92904636570563\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.74272164 6.45683231 4.85752161], LR: [[0.0012150000000000002], [0.0015], [0.0012150000000000002]], Loss: 5.352358520080646\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.38370491 5.83035251 4.41289438], LR: [[0.0012150000000000002], [0.0015], [0.0012150000000000002]], Loss: 4.542317263775815\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.64369799 6.76714124 4.58408549], LR: [[0.0012150000000000002], [0.0015], [0.0012150000000000002]], Loss: 4.998308237751286\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [4.25461501 6.06023707 3.74138995], LR: [[0.0010935], [0.0015], [0.0012150000000000002]], Loss: 4.6854140070453285\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.4551205  5.73239618 4.14445882], LR: [[0.0010935], [0.0015], [0.0012150000000000002]], Loss: 4.44399183334317\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.88480191 5.8405744  3.99842857], LR: [[0.0010935], [0.0015], [0.0012150000000000002]], Loss: 4.574601625222713\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.27053207 5.42149493 3.75576358], LR: [[0.0010935], [0.0015], [0.0012150000000000002]], Loss: 4.149263526110444\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.70948059 5.68573786 4.14493096], LR: [[0.0010935], [0.0015], [0.0010935]], Loss: 4.51338313623642\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.626628   5.30912435 4.07287153], LR: [[0.0010935], [0.0015], [0.0010935]], Loss: 4.336207961241404\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.42357413 5.02469477 4.35700566], LR: [[0.0010935], [0.0015], [0.0010935]], Loss: 4.268424851885065\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.91291634 5.31530852 4.00206083], LR: [[0.0010935], [0.0015], [0.0010935]], Loss: 4.076761898212134\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.30076015 5.17062823 3.76513443], LR: [[0.0010935], [0.0015], [0.0010935]], Loss: 4.0788409344106915\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.17238566 5.39437147 3.23948217], LR: [[0.0010935], [0.00135], [0.0010935]], Loss: 3.9354130992169183\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.33012556 4.98889438 2.82368205], LR: [[0.00098415], [0.00135], [0.0010935]], Loss: 3.7142339929255344\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.20146038 4.87352396 3.56130013], LR: [[0.00098415], [0.00135], [0.0010935]], Loss: 3.878761489043633\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.84350486 4.78864308 3.28751671], LR: [[0.00098415], [0.00135], [0.0010935]], Loss: 3.639888216747592\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.80057346 4.1224886  3.35556485], LR: [[0.00098415], [0.00135], [0.0010935]], Loss: 3.4262089713041974\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.03077868 4.62520623 3.568857  ], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 3.7416139719883597\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.30067201 4.3782106  3.50032171], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 3.72640144108329\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.06879189 4.48028517 3.28604113], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 3.6117060637349887\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.94382322 6.38195228 3.47630174], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 4.267359081249064\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.92426746 4.65289866 3.12784415], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 3.568336756260445\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.92097321 4.93649213 3.13845053], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 3.665305287422767\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.69004983 4.61068381 3.55357352], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 3.6181023877051977\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.59651831 4.30163208 3.00846003], LR: [[0.00098415], [0.00135], [0.00098415]], Loss: 3.302203474417329\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.63017923 4.47565468 2.65194955], LR: [[0.00098415], [0.0012150000000000002], [0.00098415]], Loss: 3.2525944861521325\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.34478259 4.03771047 2.64804261], LR: [[0.00098415], [0.0012150000000000002], [0.00098415]], Loss: 3.010178556796163\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.5242128  4.23873296 2.94962981], LR: [[0.000885735], [0.0012150000000000002], [0.000885735]], Loss: 3.2375251938092213\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.06283649 4.1937265  2.95461918], LR: [[0.000885735], [0.0012150000000000002], [0.000885735]], Loss: 3.4037273921072484\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.58745978 3.44915217 1.9972883 ], LR: [[0.000885735], [0.0012150000000000002], [0.000885735]], Loss: 2.677966749928892\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.16306699 4.24984894 2.837409  ], LR: [[0.000885735], [0.0012150000000000002], [0.000885735]], Loss: 3.083441641703248\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.37921894 4.20192327 2.79589123], LR: [[0.000885735], [0.0012150000000000002], [0.000885735]], Loss: 3.1256778121801716\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [3.03481497 3.89136136 2.67348453], LR: [[0.000885735], [0.0012150000000000002], [0.000885735]], Loss: 3.199886952359229\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.32244248 3.28978539 2.70312349], LR: [[0.000885735], [0.0012150000000000002], [0.000885735]], Loss: 2.7717837889461467\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.4766448  3.36718237 2.42341238], LR: [[0.0007971615000000001], [0.0012150000000000002], [0.000885735]], Loss: 2.755746517398705\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.25402189 3.37939122 2.37361153], LR: [[0.0007971615000000001], [0.0010935], [0.000885735]], Loss: 2.6690082130332784\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.00158565 3.49268187 2.11618589], LR: [[0.0007971615000000001], [0.0010935], [0.000885735]], Loss: 2.536817804076709\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.4753462  3.16330879 2.96356701], LR: [[0.0007971615000000001], [0.0010935], [0.0007971615000000001]], Loss: 2.867407337681701\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.36821252 3.60475715 2.59079988], LR: [[0.0007971615000000001], [0.0010935], [0.0007971615000000001]], Loss: 2.854589849673211\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.33415858 3.03660118 3.12512448], LR: [[0.0007971615000000001], [0.0010935], [0.0007971615000000001]], Loss: 2.8319614144911376\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.42265097 3.85803213 2.50294599], LR: [[0.0007971615000000001], [0.0010935], [0.0007971615000000001]], Loss: 2.9278763614781202\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.09021344 3.56760898 2.64065548], LR: [[0.0007971615000000001], [0.0010935], [0.0007971615000000001]], Loss: 2.7661593012015024\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.3684244  3.54469069 2.74874265], LR: [[0.0007971615000000001], [0.0010935], [0.0007971615000000001]], Loss: 2.8872859172088408\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.55687193 2.96307101 2.66299362], LR: [[0.00071744535], [0.0010935], [0.0007971615000000001]], Loss: 2.727645520654818\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.90266024 3.50346654 1.97704125], LR: [[0.00071744535], [0.00098415], [0.0007971615000000001]], Loss: 2.4610560109862125\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.89871858 3.2671367  2.54458144], LR: [[0.00071744535], [0.00098415], [0.00071744535]], Loss: 2.5701455713482573\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.0351099  2.59596841 2.07584516], LR: [[0.00071744535], [0.00098415], [0.00071744535]], Loss: 2.235641157180071\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.26974385 3.00184445 2.83311837], LR: [[0.00071744535], [0.00098415], [0.00071744535]], Loss: 2.7015688936797475\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.12692432 3.42303717 1.68626431], LR: [[0.00071744535], [0.00098415], [0.00071744535]], Loss: 2.4120752674154935\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.07437557 2.89708109 2.60796225], LR: [[0.00071744535], [0.00098415], [0.00071744535]], Loss: 2.5264729719081274\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.15768942 3.11163545 2.47374877], LR: [[0.00071744535], [0.00098415], [0.00071744535]], Loss: 2.581024547166501\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.79240529 2.82045231 1.99722674], LR: [[0.00071744535], [0.00098415], [0.00071744535]], Loss: 2.2033614470716567\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.91794256 2.84497732 2.15952952], LR: [[0.000645700815], [0.000885735], [0.00071744535]], Loss: 2.307483134184343\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.88871912 2.96803254 2.61135567], LR: [[0.000645700815], [0.000885735], [0.000645700815]], Loss: 2.4893691077285136\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.07032304 2.62166202 2.03771012], LR: [[0.000645700815], [0.000885735], [0.000645700815]], Loss: 2.243231727362533\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.32047134 2.78517985 2.15371129], LR: [[0.000645700815], [0.000885735], [0.000645700815]], Loss: 2.086454159468412\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.94499674 3.22982274 2.55015228], LR: [[0.000645700815], [0.000885735], [0.000645700815]], Loss: 2.574990586829372\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.49842109 2.38206149 1.7583593 ], LR: [[0.000645700815], [0.000885735], [0.000645700815]], Loss: 1.8796139591559768\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.62245005 2.40396761 2.33642098], LR: [[0.000645700815], [0.0007971615000000001], [0.000645700815]], Loss: 2.1209462143046163\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.62672251 2.590907   1.83218592], LR: [[0.0005811307335], [0.0007971615000000001], [0.000645700815]], Loss: 2.016605141106993\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.35750334 2.80242343 1.59791403], LR: [[0.0005811307335], [0.0007971615000000001], [0.000645700815]], Loss: 1.9192802668424944\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.36403119 2.09281765 1.7188988 ], LR: [[0.0005811307335], [0.0007971615000000001], [0.0005811307335]], Loss: 1.725249216283361\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.98851867 2.07250864 2.11149799], LR: [[0.0005811307335], [0.0007971615000000001], [0.0005811307335]], Loss: 2.0575084295940664\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.39431165 2.76572861 2.12879844], LR: [[0.0005811307335], [0.0007971615000000001], [0.0005811307335]], Loss: 2.0962795643415304\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.80119682 2.20569364 2.18169115], LR: [[0.0005811307335], [0.0007971615000000001], [0.0005811307335]], Loss: 2.0628605362648766\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.93276563 2.03203989 1.89329348], LR: [[0.0005230176601500001], [0.0007971615000000001], [0.0005811307335]], Loss: 1.9526996680845816\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.65606067 2.61049319 1.6773993 ], LR: [[0.0005230176601500001], [0.00071744535], [0.0005811307335]], Loss: 1.9813177191310871\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.59062243 2.52077847 2.23164716], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 2.11434935218965\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.53422565 2.751429   1.68482592], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 1.990160188861191\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.80058371 2.49560208 1.7080004 ], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 2.0013953971366085\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [2.08631592 2.30140253 1.6834305 ], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 2.0237163161455345\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.68565202 2.09827977 1.91019165], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 1.898041147496551\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.37796267 1.78944829 1.58634154], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 1.5845841658208517\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.42201934 2.38100474 1.43994515], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 1.7476564074804386\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.97580019 2.29060153 1.75450818], LR: [[0.0005230176601500001], [0.00071744535], [0.0005230176601500001]], Loss: 1.6736366288426023\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.59216743 2.29754242 1.86989413], LR: [[0.00047071589413500006], [0.00071744535], [0.00047071589413500006]], Loss: 1.9198679975420234\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.49846895 2.34171928 1.15172239], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.6639702045125888\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.22770532 1.8492935  1.0903212 ], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.3891066715404063\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.19129908 1.54429423 1.88219864], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.5392639843126137\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.29897244 2.01292002 1.73485473], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.682249060397347\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.94809748 2.34467536 1.61516618], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.6359796712175012\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.31103255 2.14995185 1.72262353], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.7278693088221673\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.1728214  2.202728   1.26349637], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.5463485895221432\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.38241837 1.8087298  2.03020638], LR: [[0.00047071589413500006], [0.000645700815], [0.00047071589413500006]], Loss: 1.7404515154426916\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.54475314 2.46011974 1.63097222], LR: [[0.0004236443047215001], [0.0005811307335], [0.00047071589413500006]], Loss: 1.87861503199907\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.86395169 1.59167679 1.40799494], LR: [[0.0004236443047215001], [0.0005811307335], [0.00047071589413500006]], Loss: 1.2878744697809452\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.97297165 1.92916169 1.51947443], LR: [[0.0004236443047215001], [0.0005811307335], [0.0004236443047215001]], Loss: 1.4738692548777907\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.13235301 1.93116933 1.29181288], LR: [[0.0004236443047215001], [0.0005811307335], [0.0004236443047215001]], Loss: 1.4517784076960136\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.33107423 2.19179514 1.50919459], LR: [[0.0004236443047215001], [0.0005811307335], [0.0004236443047215001]], Loss: 1.6773546558451684\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.45524484 2.2845374  1.31511881], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0004236443047215001]], Loss: 1.6849670179498693\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.88075144 1.88847186 1.20425748], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0004236443047215001]], Loss: 1.3244935917885352\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.05072736 1.56687122 1.21148804], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0004236443047215001]], Loss: 1.2763622078920405\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.03086842 1.56721137 1.39555502], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0004236443047215001]], Loss: 1.3312116047429543\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.77919786 1.62051361 1.31828344], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0004236443047215001]], Loss: 1.2393316374719143\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.23581674 1.60732067 1.57931507], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0003812798742493501]], Loss: 1.4741508247563615\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.03432371 1.55281051 1.2095124 ], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0003812798742493501]], Loss: 1.2655488732503726\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.81400273 1.26099069 1.46793318], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0003812798742493501]], Loss: 1.1809755326419449\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.37518281 1.66773938 1.41492531], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0003812798742493501]], Loss: 1.4859491658746264\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.11295788 1.51352183 1.17653997], LR: [[0.0003812798742493501], [0.0005230176601500001], [0.0003812798742493501]], Loss: 1.267673225357818\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.97432469 1.84207943 1.33782262], LR: [[0.0003812798742493501], [0.00047071589413500006], [0.0003812798742493501]], Loss: 1.3847422475319278\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.82496528 1.3606115  1.06751211], LR: [[0.0003812798742493501], [0.00047071589413500006], [0.0003812798742493501]], Loss: 1.0843629641396304\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.69100436 1.70774029 1.28569457], LR: [[0.0003812798742493501], [0.00047071589413500006], [0.0003812798742493501]], Loss: 1.22814640643696\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.820386   1.28098999 0.87536741], LR: [[0.0003431518868244151], [0.00047071589413500006], [0.0003812798742493501]], Loss: 0.9922477959825969\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.29063674 1.26771597 1.19452794], LR: [[0.0003431518868244151], [0.00047071589413500006], [0.0003431518868244151]], Loss: 1.2509602155536415\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.95812014 1.27893046 1.11542689], LR: [[0.0003431518868244151], [0.00047071589413500006], [0.0003431518868244151]], Loss: 1.1174924945427727\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.12593993 1.41549711 1.55663268], LR: [[0.0003431518868244151], [0.00047071589413500006], [0.0003431518868244151]], Loss: 1.3660232417269922\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.98804573 1.48708135 1.06215592], LR: [[0.0003431518868244151], [0.0004236443047215001], [0.0003431518868244151]], Loss: 1.1790943349866818\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.76391489 1.21324786 1.5362973 ], LR: [[0.0003431518868244151], [0.0004236443047215001], [0.0003431518868244151]], Loss: 1.1711533499734166\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.62496227 1.43973829 0.68735583], LR: [[0.0003431518868244151], [0.0004236443047215001], [0.0003431518868244151]], Loss: 0.9173521322570742\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.8778737  1.77857811 1.0505413 ], LR: [[0.0003431518868244151], [0.0004236443047215001], [0.0003431518868244151]], Loss: 1.2356643680731456\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.96835706 0.91287311 0.92667129], LR: [[0.0003088366981419736], [0.0004236443047215001], [0.0003431518868244151]], Loss: 0.9359671541505182\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.626725   1.24790175 1.12716001], LR: [[0.0003088366981419736], [0.0004236443047215001], [0.0003088366981419736]], Loss: 1.0005955888408546\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.36935089 1.59974499 0.66276867], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0003088366981419736]], Loss: 1.2106215176459711\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.95802132 1.3395832  0.74943064], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0003088366981419736]], Loss: 1.0156783849086302\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.92532541 1.4065413  0.95857099], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0003088366981419736]], Loss: 1.0968125656805934\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.57250451 1.0713329  0.79034549], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0003088366981419736]], Loss: 0.8113943009714907\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.55138256 1.05672206 0.75765832], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0003088366981419736]], Loss: 0.7885876487568021\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.85051012 1.33184018 1.19601457], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0003088366981419736]], Loss: 1.1261216243379748\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.43778237 1.05392828 1.22654993], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0002779530283277762]], Loss: 0.9060868591908365\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.51533942 1.42748606 0.71538371], LR: [[0.0003088366981419736], [0.0003812798742493501], [0.0002779530283277762]], Loss: 0.8860697313894829\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.87071148 1.52492326 0.86569379], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002779530283277762]], Loss: 1.0871095120025953\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.45118448 1.44598824 1.35973987], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002779530283277762]], Loss: 1.085637529088029\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.52589611 1.23819952 1.01072599], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002779530283277762]], Loss: 0.9249405405546228\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.73135601 1.36201286 1.39941765], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002779530283277762]], Loss: 1.164262174135074\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.61833152 1.42127805 0.81758659], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002779530283277762]], Loss: 0.9523987185272077\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.00179777 1.41625906 0.63813037], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002779530283277762]], Loss: 1.0187290673951308\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.91953906 1.33824392 0.94688382], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002501577254949986]], Loss: 1.0682222685044205\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.57078457 0.97306363 0.79983635], LR: [[0.0002779530283277762], [0.0003431518868244151], [0.0002501577254949986]], Loss: 0.7812281855319937\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.6752726  1.21749077 0.79547446], LR: [[0.0002501577254949986], [0.0003431518868244151], [0.0002501577254949986]], Loss: 0.8960792763593295\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.49643053 1.12154599 0.65115192], LR: [[0.0002501577254949986], [0.0003431518868244151], [0.0002501577254949986]], Loss: 0.7563761457738778\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.73427806 0.87678155 0.7767692 ], LR: [[0.0002501577254949986], [0.0003431518868244151], [0.0002501577254949986]], Loss: 0.7959429354344806\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.5061904  0.97187401 1.02105795], LR: [[0.0002501577254949986], [0.0003088366981419736], [0.0002501577254949986]], Loss: 0.8330407835155104\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.5932032  1.06859195 0.80858414], LR: [[0.0002501577254949986], [0.0003088366981419736], [0.0002501577254949986]], Loss: 0.8234597662122299\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.64591654 0.93001403 0.90830149], LR: [[0.0002501577254949986], [0.0003088366981419736], [0.0002501577254949986]], Loss: 0.8280773510923609\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.63171672 0.69480159 0.50890517], LR: [[0.0002501577254949986], [0.0003088366981419736], [0.0002501577254949986]], Loss: 0.611807826096192\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.59644296 1.06362556 0.43832942], LR: [[0.0002501577254949986], [0.0003088366981419736], [0.0002501577254949986]], Loss: 0.6994659771335622\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.57883949 1.03249747 0.7326754 ], LR: [[0.0002501577254949986], [0.0003088366981419736], [0.00022514195294549873]], Loss: 0.7813374540147683\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.58294992 1.25537614 0.71474046], LR: [[0.00022514195294549873], [0.0003088366981419736], [0.00022514195294549873]], Loss: 0.8510221733103389\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.74304484 0.89178502 0.73811059], LR: [[0.00022514195294549873], [0.0003088366981419736], [0.00022514195294549873]], Loss: 0.7909801490193544\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.70990268 0.77827824 0.56384688], LR: [[0.00022514195294549873], [0.0003088366981419736], [0.00022514195294549873]], Loss: 0.6840092686105829\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.52453354 0.91033056 0.81734794], LR: [[0.00022514195294549873], [0.0002779530283277762], [0.00022514195294549873]], Loss: 0.7507373452434938\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.50897214 0.84664686 0.78566562], LR: [[0.00022514195294549873], [0.0002779530283277762], [0.00022514195294549873]], Loss: 0.7137615401033933\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.56313467 0.6306685  0.52616754], LR: [[0.00022514195294549873], [0.0002779530283277762], [0.00022514195294549873]], Loss: 0.5733235702989623\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.37466626 0.82773404 0.88298877], LR: [[0.00022514195294549873], [0.0002779530283277762], [0.00022514195294549873]], Loss: 0.6951296921571096\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.39727828 0.67806322 0.41793133], LR: [[0.00022514195294549873], [0.0002779530283277762], [0.00022514195294549873]], Loss: 0.49775761050793027\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.5302191  0.89765231 0.54314314], LR: [[0.00020262775765094885], [0.0002779530283277762], [0.00020262775765094885]], Loss: 0.6570048468622068\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.51238713 0.99880634 0.58985232], LR: [[0.00020262775765094885], [0.0002779530283277762], [0.00020262775765094885]], Loss: 0.7003485972713679\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.54897205 0.94702383 0.47048544], LR: [[0.00020262775765094885], [0.0002779530283277762], [0.00020262775765094885]], Loss: 0.6554937717070183\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.67954917 0.84853838 0.86319819], LR: [[0.00020262775765094885], [0.0002779530283277762], [0.00020262775765094885]], Loss: 0.7970952446099061\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.29443023 0.68613019 0.90763575], LR: [[0.00020262775765094885], [0.0002779530283277762], [0.00020262775765094885]], Loss: 0.6293987216831495\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.38116526 0.60555624 0.59319271], LR: [[0.00020262775765094885], [0.0002779530283277762], [0.00020262775765094885]], Loss: 0.5266380706988275\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.43845921 1.02938007 0.60755837], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.6917992147368689\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.43813789 0.87261246 0.45633366], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.5890280028820659\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.82797116 0.73060832 0.65632272], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.7383007311541587\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.60850489 0.68266121 0.52752581], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.6062306386484609\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.59596274 0.66473212 0.73154776], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.6640808724425732\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.51516189 1.14545256 0.94797281], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.869529086908636\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.76012626 0.85372382 0.86160479], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.8251516207059225\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.45876396 0.76183924 0.68032126], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00018236498188585397]], Loss: 0.6336414898062747\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.52655948 0.52845776 0.74779102], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00016412848369726858]], Loss: 0.6009360861902436\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.50184047 0.86476812 0.59240888], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00016412848369726858]], Loss: 0.6530058243364328\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.32871064 0.65106779 0.42942344], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00016412848369726858]], Loss: 0.4697339524476168\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22274256 0.41645261 0.45245868], LR: [[0.00018236498188585397], [0.0002501577254949986], [0.00016412848369726858]], Loss: 0.36388461588164017\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.4907559  0.62919574 0.61861731], LR: [[0.00016412848369726858], [0.0002501577254949986], [0.00016412848369726858]], Loss: 0.5795229824321966\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.54874067 0.61688752 0.48481245], LR: [[0.00016412848369726858], [0.0002501577254949986], [0.00016412848369726858]], Loss: 0.5501468792743981\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.46226645 0.528048   0.58795387], LR: [[0.00016412848369726858], [0.0002501577254949986], [0.00016412848369726858]], Loss: 0.5260894375139227\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.57190347 0.57959581 0.42647967], LR: [[0.00016412848369726858], [0.00022514195294549873], [0.00016412848369726858]], Loss: 0.5259929808058466\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.23483987 0.4817597  0.36730159], LR: [[0.00016412848369726858], [0.00022514195294549873], [0.00016412848369726858]], Loss: 0.36130038546149684\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18250362 0.56102317 0.15179276], LR: [[0.00016412848369726858], [0.00022514195294549873], [0.00016412848369726858]], Loss: 0.29843984806211665\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20777516 0.77526499 0.30193093], LR: [[0.00016412848369726858], [0.00022514195294549873], [0.00014771563532754172]], Loss: 0.42832369294638434\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.47844019 0.55618538 0.7588335 ], LR: [[0.00014771563532754172], [0.00022514195294549873], [0.00014771563532754172]], Loss: 0.5978196903007725\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.697957   0.60614994 0.71466763], LR: [[0.00014771563532754172], [0.00022514195294549873], [0.00014771563532754172]], Loss: 0.672924856475244\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19145205 0.56566519 0.65341204], LR: [[0.00014771563532754172], [0.00022514195294549873], [0.00014771563532754172]], Loss: 0.47017642417301736\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.36611161 0.61629185 0.75185091], LR: [[0.00014771563532754172], [0.00020262775765094885], [0.00014771563532754172]], Loss: 0.5780847880554696\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18440874 0.75476617 0.48772078], LR: [[0.00014771563532754172], [0.00020262775765094885], [0.00014771563532754172]], Loss: 0.47563189937733114\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19950526 0.68903784 0.43401215], LR: [[0.00014771563532754172], [0.00020262775765094885], [0.00014771563532754172]], Loss: 0.44085174739360805\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.53562438 0.88417137 0.25577111], LR: [[0.00013294407179478756], [0.00020262775765094885], [0.00014771563532754172]], Loss: 0.5585222878058751\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.32336155 0.51437925 0.57049731], LR: [[0.00013294407179478756], [0.00020262775765094885], [0.00014771563532754172]], Loss: 0.46941270134101315\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.48974881 0.80414517 0.35363246], LR: [[0.00013294407179478756], [0.00020262775765094885], [0.00014771563532754172]], Loss: 0.5491754795207332\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.58674818 0.44458158 0.58527416], LR: [[0.00013294407179478756], [0.00020262775765094885], [0.00013294407179478756]], Loss: 0.5388679733530929\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26364786 0.44569767 0.39334353], LR: [[0.00013294407179478756], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.36756302297115323\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.34851881 0.64345165 0.43056689], LR: [[0.00013294407179478756], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.47417911774013194\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17433115 0.39148327 0.61999251], LR: [[0.00013294407179478756], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.3952689764128688\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26252093 0.38677828 0.49324124], LR: [[0.0001196496646153088], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.3808468166366219\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.24638203 0.64138838 0.36388025], LR: [[0.0001196496646153088], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.41721688601110757\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20149843 0.59096921 0.24066355], LR: [[0.0001196496646153088], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.34437706699284415\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.34088567 0.24466125 0.49037962], LR: [[0.0001196496646153088], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.3586421797300378\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.33544576 0.63802285 0.237044  ], LR: [[0.0001196496646153088], [0.00018236498188585397], [0.00013294407179478756]], Loss: 0.40350420107133694\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.40620984 0.60972897 0.29427273], LR: [[0.0001196496646153088], [0.00018236498188585397], [0.0001196496646153088]], Loss: 0.43673717910656706\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.29265143 0.68211728 0.53281155], LR: [[0.0001196496646153088], [0.00016412848369726858], [0.0001196496646153088]], Loss: 0.5025267547306915\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.232959   0.57077571 0.64722495], LR: [[0.0001196496646153088], [0.00016412848369726858], [0.0001196496646153088]], Loss: 0.48365322154015306\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26869797 0.71680942 0.1263296 ], LR: [[0.0001196496646153088], [0.00016412848369726858], [0.0001196496646153088]], Loss: 0.3706123321596533\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.29588403 0.3644359  0.23913298], LR: [[0.00010768469815377792], [0.00016412848369726858], [0.0001196496646153088]], Loss: 0.29981763871386646\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.30451909 0.2443541  0.33140811], LR: [[0.00010768469815377792], [0.00016412848369726858], [0.00010768469815377792]], Loss: 0.29342709813577433\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.31583848 0.66929496 0.5174139 ], LR: [[0.00010768469815377792], [0.00016412848369726858], [0.00010768469815377792]], Loss: 0.5008491119245688\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.55479328 0.50924972 0.78389185], LR: [[0.00010768469815377792], [0.00016412848369726858], [0.00010768469815377792]], Loss: 0.6159782800730317\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.44647436 0.24497363 0.28590512], LR: [[0.00010768469815377792], [0.00016412848369726858], [0.00010768469815377792]], Loss: 0.325784370303154\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1868538  0.55221886 0.63016125], LR: [[0.00010768469815377792], [0.00016412848369726858], [0.00010768469815377792]], Loss: 0.45641130485339093\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.41601213 0.50209419 0.2975129 ], LR: [[9.691622833840013e-05], [0.00016412848369726858], [0.00010768469815377792]], Loss: 0.40520640363295873\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.34111917 0.29149375 0.30935915], LR: [[9.691622833840013e-05], [0.00016412848369726858], [9.691622833840013e-05]], Loss: 0.313990687354235\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13684331 0.22107818 0.14544108], LR: [[9.691622833840013e-05], [0.00016412848369726858], [9.691622833840013e-05]], Loss: 0.167787523806716\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10119149 0.35516767 0.36318472], LR: [[9.691622833840013e-05], [0.00014771563532754172], [9.691622833840013e-05]], Loss: 0.27318129195366053\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.37867751 0.3709283  0.39604211], LR: [[9.691622833840013e-05], [0.00014771563532754172], [9.691622833840013e-05]], Loss: 0.3818826401163824\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12511534 0.42257083 0.28038488], LR: [[9.691622833840013e-05], [0.00014771563532754172], [9.691622833840013e-05]], Loss: 0.2760236844719232\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.31911369 0.21465445 0.42621758], LR: [[9.691622833840013e-05], [0.00014771563532754172], [9.691622833840013e-05]], Loss: 0.31999524171774585\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.36332899 0.25312645 0.16573363], LR: [[9.691622833840013e-05], [0.00014771563532754172], [9.691622833840013e-05]], Loss: 0.2607296904424826\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.45375322 0.45982422 0.50765623], LR: [[8.722460550456011e-05], [0.00013294407179478756], [8.722460550456011e-05]], Loss: 0.4737445573105166\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.57744017 0.5099448  0.4179614 ], LR: [[8.722460550456011e-05], [0.00013294407179478756], [8.722460550456011e-05]], Loss: 0.5017821216645341\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18267836 0.38705018 0.28262772], LR: [[8.722460550456011e-05], [0.00013294407179478756], [8.722460550456011e-05]], Loss: 0.28411875523626806\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.33381836 0.41801172 0.24337438], LR: [[8.722460550456011e-05], [0.00013294407179478756], [8.722460550456011e-05]], Loss: 0.33173482105446356\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.32025487 0.52826967 0.33925593], LR: [[8.722460550456011e-05], [0.00013294407179478756], [8.722460550456011e-05]], Loss: 0.39592682537894386\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.28840236 0.42926795 0.26086273], LR: [[8.722460550456011e-05], [0.00013294407179478756], [8.722460550456011e-05]], Loss: 0.32617767883464693\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.42056654 0.33049885 0.38762383], LR: [[8.722460550456011e-05], [0.00013294407179478756], [8.722460550456011e-05]], Loss: 0.3795630729633073\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26626977 0.40189371 0.40997601], LR: [[8.722460550456011e-05], [0.0001196496646153088], [8.722460550456011e-05]], Loss: 0.35937983156957976\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.42773227 0.31263609 0.55567577], LR: [[7.85021449541041e-05], [0.0001196496646153088], [7.85021449541041e-05]], Loss: 0.43201470864781494\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18714579 0.52736385 0.25120379], LR: [[7.85021449541041e-05], [0.0001196496646153088], [7.85021449541041e-05]], Loss: 0.32190447380145387\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10057723 0.26519462 0.05924478], LR: [[7.85021449541041e-05], [0.0001196496646153088], [7.85021449541041e-05]], Loss: 0.14167220899214347\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20903756 0.28893507 0.24433779], LR: [[7.85021449541041e-05], [0.0001196496646153088], [7.85021449541041e-05]], Loss: 0.24743680727357664\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.34381067 0.45529761 0.32041649], LR: [[7.85021449541041e-05], [0.0001196496646153088], [7.85021449541041e-05]], Loss: 0.37317492319115747\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08890281 0.23013119 0.4013731 ], LR: [[7.85021449541041e-05], [0.0001196496646153088], [7.85021449541041e-05]], Loss: 0.24013570089514058\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.35117653 0.29375266 0.49487118], LR: [[7.85021449541041e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.37993345575009396\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22874248 0.23905724 0.33100667], LR: [[7.85021449541041e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.26626879412525645\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.33976981 0.40152595 0.32949843], LR: [[7.06519304586937e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.3569313991721719\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.28336872 0.35282925 0.26020229], LR: [[7.06519304586937e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.29880008685247356\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.32452633 0.27907016 0.3447152 ], LR: [[7.06519304586937e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.31610389722821614\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.57182429 0.25102645 0.37999942], LR: [[7.06519304586937e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.40095005308898785\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.28164115 0.41855292 0.11892207], LR: [[7.06519304586937e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.2730387134461974\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18239126 0.15871784 0.4343896 ], LR: [[7.06519304586937e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.25849956656495726\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.24495539 0.25182982 0.28166307], LR: [[7.06519304586937e-05], [0.00010768469815377792], [7.06519304586937e-05]], Loss: 0.25948276094316197\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08851592 0.17959036 0.31705458], LR: [[7.06519304586937e-05], [0.00010768469815377792], [6.358673741282432e-05]], Loss: 0.195053618621702\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.29846539 0.23074068 0.19443325], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.24121310781377056\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18876097 0.19170811 0.12362815], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.16803240888984874\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.33608774 0.19953048 0.29279354], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.27613725290323293\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21412565 0.34143859 0.15929185], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.23828536551756163\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1474632  0.21897827 0.36752669], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.2446560526608179\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14761416 0.13109456 0.05930502], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.11267124529462309\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1612809  0.2445587  0.27000575], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.22528178380957495\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03940216 0.08378739 0.08698135], LR: [[6.358673741282432e-05], [9.691622833840013e-05], [6.358673741282432e-05]], Loss: 0.0700569689863672\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13399419 0.32108865 0.24807606], LR: [[5.722806367154189e-05], [8.722460550456011e-05], [5.722806367154189e-05]], Loss: 0.23438629780119904\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.246204   0.20784169 0.08631361], LR: [[5.722806367154189e-05], [8.722460550456011e-05], [5.722806367154189e-05]], Loss: 0.18011976626391213\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.27469425 0.15261288 0.14611836], LR: [[5.722806367154189e-05], [8.722460550456011e-05], [5.722806367154189e-05]], Loss: 0.19114183101492624\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.27275202 0.24228588 0.32773406], LR: [[5.722806367154189e-05], [8.722460550456011e-05], [5.722806367154189e-05]], Loss: 0.2809239852024863\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22826626 0.29600375 0.14199624], LR: [[5.722806367154189e-05], [8.722460550456011e-05], [5.722806367154189e-05]], Loss: 0.2220887504890561\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13388894 0.1670141  0.06360527], LR: [[5.722806367154189e-05], [8.722460550456011e-05], [5.722806367154189e-05]], Loss: 0.12150276974842807\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19037973 0.42387276 0.42557003], LR: [[5.722806367154189e-05], [8.722460550456011e-05], [5.722806367154189e-05]], Loss: 0.3466075061820448\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22342633 0.44539062 0.66065962], LR: [[5.1505257304387705e-05], [7.85021449541041e-05], [5.1505257304387705e-05]], Loss: 0.44315885900209345\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1583989  0.26147129 0.1927894 ], LR: [[5.1505257304387705e-05], [7.85021449541041e-05], [5.1505257304387705e-05]], Loss: 0.20421986093744635\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.24601403 0.20543125 0.31902547], LR: [[5.1505257304387705e-05], [7.85021449541041e-05], [5.1505257304387705e-05]], Loss: 0.2568235853128135\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06529449 0.27911244 0.55052809], LR: [[5.1505257304387705e-05], [7.85021449541041e-05], [5.1505257304387705e-05]], Loss: 0.29831167286261917\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19754495 0.2881807  0.2019288 ], LR: [[5.1505257304387705e-05], [7.85021449541041e-05], [5.1505257304387705e-05]], Loss: 0.2292181512709552\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.31021123 0.09615657 0.34297742], LR: [[5.1505257304387705e-05], [7.85021449541041e-05], [5.1505257304387705e-05]], Loss: 0.24978173995700978\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.40097117 0.14130722 0.3155822 ], LR: [[4.635473157394894e-05], [7.85021449541041e-05], [5.1505257304387705e-05]], Loss: 0.28595352889504283\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.37604301 0.28423242 0.25759275], LR: [[4.635473157394894e-05], [7.06519304586937e-05], [5.1505257304387705e-05]], Loss: 0.30595606188910707\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13227129 0.24999304 0.15748981], LR: [[4.635473157394894e-05], [7.06519304586937e-05], [5.1505257304387705e-05]], Loss: 0.17991804697628444\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10375676 0.2563823  0.22542726], LR: [[4.635473157394894e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.19518877544595548\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0355114  0.13627675 0.23723615], LR: [[4.635473157394894e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.13634143370358895\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11010559 0.06357982 0.2111017 ], LR: [[4.635473157394894e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.1282623705267906\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1507346  0.36550863 0.14460158], LR: [[4.635473157394894e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.22028160264521532\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17636455 0.18344063 0.30711294], LR: [[4.635473157394894e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.2223060389383075\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21299449 0.11372902 0.25109141], LR: [[4.171925841655404e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.19260497553894917\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.23929846 0.29941329 0.07317246], LR: [[4.171925841655404e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.20396140022513765\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.32910988 0.25746852 0.24963366], LR: [[4.171925841655404e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.27873735459521415\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09794718 0.109461   0.04460729], LR: [[4.171925841655404e-05], [7.06519304586937e-05], [4.635473157394894e-05]], Loss: 0.08400515681016259\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07783416 0.24561177 0.22658233], LR: [[4.171925841655404e-05], [6.358673741282432e-05], [4.171925841655404e-05]], Loss: 0.18334275469028702\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07152316 0.20510154 0.30705465], LR: [[4.171925841655404e-05], [6.358673741282432e-05], [4.171925841655404e-05]], Loss: 0.19455978425219655\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21954853 0.16817911 0.1151836 ], LR: [[4.171925841655404e-05], [6.358673741282432e-05], [4.171925841655404e-05]], Loss: 0.16763708089788754\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04267468 0.13082089 0.15828545], LR: [[4.171925841655404e-05], [6.358673741282432e-05], [4.171925841655404e-05]], Loss: 0.11059367642078238\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10293059 0.0838177  0.49374861], LR: [[3.754733257489864e-05], [6.358673741282432e-05], [4.171925841655404e-05]], Loss: 0.2268322996996964\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11972843 0.14285637 0.30133775], LR: [[3.754733257489864e-05], [6.358673741282432e-05], [4.171925841655404e-05]], Loss: 0.18797418531030416\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20234401 0.24549517 0.18599183], LR: [[3.754733257489864e-05], [6.358673741282432e-05], [4.171925841655404e-05]], Loss: 0.21127700408920647\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17281299 0.21124524 0.19968052], LR: [[3.754733257489864e-05], [6.358673741282432e-05], [3.754733257489864e-05]], Loss: 0.19457958474444847\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18002954 0.14590109 0.16549401], LR: [[3.754733257489864e-05], [6.358673741282432e-05], [3.754733257489864e-05]], Loss: 0.16380821564545234\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26178502 0.26574995 0.09049832], LR: [[3.379259931740878e-05], [6.358673741282432e-05], [3.754733257489864e-05]], Loss: 0.20601109633843104\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11246227 0.11755659 0.16760729], LR: [[3.379259931740878e-05], [6.358673741282432e-05], [3.754733257489864e-05]], Loss: 0.1325420497613959\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20256489 0.16526221 0.1086162 ], LR: [[3.379259931740878e-05], [5.722806367154189e-05], [3.754733257489864e-05]], Loss: 0.1588144361025964\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.089945   0.09652075 0.38207993], LR: [[3.379259931740878e-05], [5.722806367154189e-05], [3.754733257489864e-05]], Loss: 0.18951522582171795\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20813693 0.29753848 0.27696205], LR: [[3.379259931740878e-05], [5.722806367154189e-05], [3.754733257489864e-05]], Loss: 0.26087915261305167\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.25577332 0.19661408 0.29305152], LR: [[3.379259931740878e-05], [5.722806367154189e-05], [3.754733257489864e-05]], Loss: 0.2484796389037122\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.33124069 0.15442197 0.23532069], LR: [[3.0413339385667903e-05], [5.722806367154189e-05], [3.754733257489864e-05]], Loss: 0.24032778319669887\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21032469 0.25557473 0.12297582], LR: [[3.0413339385667903e-05], [5.722806367154189e-05], [3.754733257489864e-05]], Loss: 0.19629174609920785\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12501293 0.03758945 0.09917764], LR: [[3.0413339385667903e-05], [5.722806367154189e-05], [3.754733257489864e-05]], Loss: 0.08726000622535746\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1939372  0.31440627 0.269213  ], LR: [[3.0413339385667903e-05], [5.722806367154189e-05], [3.379259931740878e-05]], Loss: 0.2591854895008752\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13362092 0.14612389 0.15097927], LR: [[3.0413339385667903e-05], [5.722806367154189e-05], [3.379259931740878e-05]], Loss: 0.14357469451303284\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10809806 0.11030359 0.37823584], LR: [[3.0413339385667903e-05], [5.722806367154189e-05], [3.379259931740878e-05]], Loss: 0.19887916751826804\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.32073033 0.13611276 0.30768181], LR: [[3.0413339385667903e-05], [5.1505257304387705e-05], [3.379259931740878e-05]], Loss: 0.2548416317999363\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06498739 0.0706036  0.19742091], LR: [[3.0413339385667903e-05], [5.1505257304387705e-05], [3.379259931740878e-05]], Loss: 0.11100396555305149\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12233262 0.07927681 0.17539287], LR: [[3.0413339385667903e-05], [5.1505257304387705e-05], [3.379259931740878e-05]], Loss: 0.12566743593042096\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08168681 0.26086994 0.13207068], LR: [[3.0413339385667903e-05], [5.1505257304387705e-05], [3.379259931740878e-05]], Loss: 0.15820914344241221\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09454102 0.0164753  0.33912271], LR: [[2.7372005447101112e-05], [5.1505257304387705e-05], [3.379259931740878e-05]], Loss: 0.15004634210325699\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1553446  0.29543637 0.15604699], LR: [[2.7372005447101112e-05], [5.1505257304387705e-05], [3.379259931740878e-05]], Loss: 0.2022759866838654\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13978892 0.07904962 0.02311844], LR: [[2.7372005447101112e-05], [5.1505257304387705e-05], [3.379259931740878e-05]], Loss: 0.08065232581148545\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2146196  0.14718125 0.09847705], LR: [[2.7372005447101112e-05], [4.635473157394894e-05], [3.379259931740878e-05]], Loss: 0.153425963994038\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14291112 0.29635805 0.22974058], LR: [[2.7372005447101112e-05], [4.635473157394894e-05], [3.0413339385667903e-05]], Loss: 0.2230032521113753\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11895274 0.12794819 0.10057586], LR: [[2.7372005447101112e-05], [4.635473157394894e-05], [3.0413339385667903e-05]], Loss: 0.11582559384716053\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03346322 0.16020973 0.04496489], LR: [[2.7372005447101112e-05], [4.635473157394894e-05], [3.0413339385667903e-05]], Loss: 0.07954594887793064\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13717954 0.21471613 0.25892553], LR: [[2.7372005447101112e-05], [4.635473157394894e-05], [3.0413339385667903e-05]], Loss: 0.2036070683998211\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15303933 0.10232761 0.19372343], LR: [[2.4634804902391e-05], [4.635473157394894e-05], [3.0413339385667903e-05]], Loss: 0.1496967897266101\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07645652 0.20845573 0.08747461], LR: [[2.4634804902391e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.12412895495382448\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16228177 0.17291899 0.42256264], LR: [[2.4634804902391e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.2525877983619769\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.29199073 0.22198029 0.24017955], LR: [[2.4634804902391e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.251383521471483\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11361193 0.07378655 0.08063266], LR: [[2.4634804902391e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.08934371593718728\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18379282 0.03652494 0.23005894], LR: [[2.4634804902391e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.15012556940938035\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0835438  0.10234129 0.16065055], LR: [[2.4634804902391e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.11551187808935841\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.24982781 0.11756776 0.06611385], LR: [[2.21713244121519e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.14450313886006674\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05246394 0.02398743 0.04604616], LR: [[2.21713244121519e-05], [4.171925841655404e-05], [3.0413339385667903e-05]], Loss: 0.04083250975701958\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13069418 0.12885431 0.12732838], LR: [[2.21713244121519e-05], [3.754733257489864e-05], [2.7372005447101112e-05]], Loss: 0.12895895744829128\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.25094597 0.04930153 0.14715418], LR: [[2.21713244121519e-05], [3.754733257489864e-05], [2.7372005447101112e-05]], Loss: 0.14913389276713132\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16639525 0.06238872 0.1507304 ], LR: [[2.21713244121519e-05], [3.754733257489864e-05], [2.7372005447101112e-05]], Loss: 0.12650479060597716\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12402281 0.03354501 0.14101397], LR: [[2.21713244121519e-05], [3.754733257489864e-05], [2.7372005447101112e-05]], Loss: 0.09952726365687947\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14355732 0.14519139 0.31443479], LR: [[2.21713244121519e-05], [3.754733257489864e-05], [2.7372005447101112e-05]], Loss: 0.20106116792187093\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09868729 0.07893953 0.09310772], LR: [[2.21713244121519e-05], [3.754733257489864e-05], [2.7372005447101112e-05]], Loss: 0.09024484824699659\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.047367   0.10904264 0.05098198], LR: [[2.21713244121519e-05], [3.754733257489864e-05], [2.7372005447101112e-05]], Loss: 0.06913054024179777\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08946335 0.21849068 0.20897327], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.1723091005037228\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11022383 0.12233941 0.04644552], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.09300292247595887\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.51937607 0.15501696 0.21900215], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.2977983954075413\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.28225589 0.08489277 0.05312061], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.14008975686583047\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16128456 0.07847205 0.13640144], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.12538601734704571\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.29310525 0.09925736 0.05519833], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.14918698015933238\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15417794 0.06589721 0.04202967], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.08736827524825157\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05775577 0.06148164 0.10706025], LR: [[1.995419197093671e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.07543255145971973\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16091675 0.15953628 0.03181909], LR: [[1.795877277384304e-05], [3.379259931740878e-05], [2.4634804902391e-05]], Loss: 0.11742403996603873\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10408846 0.07830691 0.14056104], LR: [[1.795877277384304e-05], [3.379259931740878e-05], [2.21713244121519e-05]], Loss: 0.10765213804785162\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.30478685 0.03536448 0.21650054], LR: [[1.795877277384304e-05], [3.379259931740878e-05], [2.21713244121519e-05]], Loss: 0.18555062290591495\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03020502 0.05774878 0.04540965], LR: [[1.795877277384304e-05], [3.0413339385667903e-05], [2.21713244121519e-05]], Loss: 0.04445448313296462\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26616069 0.06758511 0.06549403], LR: [[1.795877277384304e-05], [3.0413339385667903e-05], [2.21713244121519e-05]], Loss: 0.1330799441660444\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17381283 0.08367677 0.18184905], LR: [[1.795877277384304e-05], [3.0413339385667903e-05], [2.21713244121519e-05]], Loss: 0.14644621361221652\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20893848 0.0635314  0.01225452], LR: [[1.795877277384304e-05], [3.0413339385667903e-05], [2.21713244121519e-05]], Loss: 0.0949081338228037\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10883692 0.01490472 0.3448592 ], LR: [[1.795877277384304e-05], [3.0413339385667903e-05], [1.995419197093671e-05]], Loss: 0.15620028261405725\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.23472662 0.24169822 0.2604864 ], LR: [[1.6162895496458737e-05], [3.0413339385667903e-05], [1.995419197093671e-05]], Loss: 0.24563707884090644\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12586245 0.31099212 0.22768898], LR: [[1.6162895496458737e-05], [2.7372005447101112e-05], [1.995419197093671e-05]], Loss: 0.22151451606613892\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04750991 0.04951961 0.10664498], LR: [[1.6162895496458737e-05], [2.7372005447101112e-05], [1.995419197093671e-05]], Loss: 0.067891500464951\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16058514 0.06156457 0.16097234], LR: [[1.6162895496458737e-05], [2.7372005447101112e-05], [1.995419197093671e-05]], Loss: 0.12770735058312613\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06846491 0.1204263  0.01992636], LR: [[1.6162895496458737e-05], [2.7372005447101112e-05], [1.995419197093671e-05]], Loss: 0.06960585965774953\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14859028 0.20955973 0.19215726], LR: [[1.6162895496458737e-05], [2.7372005447101112e-05], [1.995419197093671e-05]], Loss: 0.18343575498710077\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05604224 0.14972091 0.3212722 ], LR: [[1.6162895496458737e-05], [2.7372005447101112e-05], [1.995419197093671e-05]], Loss: 0.17567845307290555\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07070282 0.08168988 0.05911018], LR: [[1.6162895496458737e-05], [2.7372005447101112e-05], [1.995419197093671e-05]], Loss: 0.0705009587481618\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11255312 0.0979211  0.14157417], LR: [[1.4546605946812864e-05], [2.4634804902391e-05], [1.795877277384304e-05]], Loss: 0.11734946390924354\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15126779 0.20309165 0.21883657], LR: [[1.4546605946812864e-05], [2.4634804902391e-05], [1.795877277384304e-05]], Loss: 0.1910653368383646\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15156647 0.06310524 0.09730928], LR: [[1.4546605946812864e-05], [2.4634804902391e-05], [1.795877277384304e-05]], Loss: 0.10399366390464516\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09713913 0.14102481 0.22223393], LR: [[1.4546605946812864e-05], [2.4634804902391e-05], [1.795877277384304e-05]], Loss: 0.15346595855701403\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04728688 0.13654737 0.08543274], LR: [[1.4546605946812864e-05], [2.4634804902391e-05], [1.795877277384304e-05]], Loss: 0.08975566438787307\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22816607 0.11097624 0.10052297], LR: [[1.4546605946812864e-05], [2.4634804902391e-05], [1.795877277384304e-05]], Loss: 0.14655509674921632\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07415048 0.08670911 0.15500946], LR: [[1.4546605946812864e-05], [2.4634804902391e-05], [1.6162895496458737e-05]], Loss: 0.10528968416076774\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18609006 0.11285515 0.06599592], LR: [[1.3091945352131578e-05], [2.4634804902391e-05], [1.6162895496458737e-05]], Loss: 0.12164704070426524\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20001053 0.07750528 0.18220743], LR: [[1.3091945352131578e-05], [2.4634804902391e-05], [1.6162895496458737e-05]], Loss: 0.15324108152339858\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11125416 0.06508719 0.03320976], LR: [[1.3091945352131578e-05], [2.4634804902391e-05], [1.6162895496458737e-05]], Loss: 0.06985037185251712\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14367402 0.08056817 0.17521799], LR: [[1.3091945352131578e-05], [2.21713244121519e-05], [1.6162895496458737e-05]], Loss: 0.13315339188401898\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18377315 0.08174783 0.42842438], LR: [[1.3091945352131578e-05], [2.21713244121519e-05], [1.6162895496458737e-05]], Loss: 0.23131511965300888\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18851549 0.12174875 0.03484431], LR: [[1.178275081691842e-05], [2.21713244121519e-05], [1.6162895496458737e-05]], Loss: 0.1150361814328547\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09595904 0.04816009 0.23661175], LR: [[1.178275081691842e-05], [2.21713244121519e-05], [1.4546605946812864e-05]], Loss: 0.12691029345151036\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09699855 0.13496719 0.20260654], LR: [[1.178275081691842e-05], [2.21713244121519e-05], [1.4546605946812864e-05]], Loss: 0.14485743058825998\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11424401 0.12074661 0.09831157], LR: [[1.178275081691842e-05], [2.21713244121519e-05], [1.4546605946812864e-05]], Loss: 0.11110073167830707\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.27937012 0.16530273 0.17374676], LR: [[1.178275081691842e-05], [1.995419197093671e-05], [1.4546605946812864e-05]], Loss: 0.2061398695127961\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07338394 0.04129808 0.04171534], LR: [[1.178275081691842e-05], [1.995419197093671e-05], [1.4546605946812864e-05]], Loss: 0.05213245542409519\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03583949 0.03323544 0.00950532], LR: [[1.178275081691842e-05], [1.995419197093671e-05], [1.4546605946812864e-05]], Loss: 0.026193416598252955\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14977527 0.1965     0.22548562], LR: [[1.0604475735226578e-05], [1.995419197093671e-05], [1.4546605946812864e-05]], Loss: 0.19058696702122688\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21817101 0.02536757 0.07157025], LR: [[1.0604475735226578e-05], [1.995419197093671e-05], [1.4546605946812864e-05]], Loss: 0.1050362746306928\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08674407 0.03690079 0.06024626], LR: [[1.0604475735226578e-05], [1.995419197093671e-05], [1.4546605946812864e-05]], Loss: 0.061297040622060504\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.23514144 0.10414908 0.30159299], LR: [[1.0604475735226578e-05], [1.995419197093671e-05], [1.4546605946812864e-05]], Loss: 0.21362783609113345\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16200462 0.11728979 0.12933676], LR: [[1.0604475735226578e-05], [1.795877277384304e-05], [1.4546605946812864e-05]], Loss: 0.1362103868213793\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21098498 0.07433306 0.05769793], LR: [[1.0604475735226578e-05], [1.795877277384304e-05], [1.4546605946812864e-05]], Loss: 0.11433865631852919\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04103008 0.06880856 0.10528673], LR: [[1.0604475735226578e-05], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.07170845359719048\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09293465 0.06461877 0.09259194], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.08338178836274891\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17944296 0.03567686 0.23513444], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.15008475397946314\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13736221 0.01186484 0.01756839], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.05559847847558558\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07554115 0.04463381 0.2176067 ], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.11259388657364373\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20267266 0.26131647 0.14722222], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.20373711553091803\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.036045   0.11015189 0.15027167], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.09882285371383963\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0433786  0.05880837 0.06278926], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.3091945352131578e-05]], Loss: 0.054992077536880964\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.01307968 0.16681172 0.13347528], LR: [[9.544028161703921e-06], [1.795877277384304e-05], [1.178275081691842e-05]], Loss: 0.1044555598770603\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.27221256 0.10583118 0.11649182], LR: [[8.58962534553353e-06], [1.795877277384304e-05], [1.178275081691842e-05]], Loss: 0.16484518595815947\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05341725 0.1167421  0.14152628], LR: [[8.58962534553353e-06], [1.6162895496458737e-05], [1.178275081691842e-05]], Loss: 0.10389520883230337\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1029677  0.10864073 0.05350252], LR: [[8.58962534553353e-06], [1.6162895496458737e-05], [1.178275081691842e-05]], Loss: 0.08837031637415445\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03173648 0.15303982 0.03783228], LR: [[8.58962534553353e-06], [1.6162895496458737e-05], [1.178275081691842e-05]], Loss: 0.07420286024532591\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19158116 0.09454223 0.0619348 ], LR: [[8.58962534553353e-06], [1.6162895496458737e-05], [1.178275081691842e-05]], Loss: 0.11601939817424863\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03663705 0.02271957 0.13743079], LR: [[8.58962534553353e-06], [1.6162895496458737e-05], [1.178275081691842e-05]], Loss: 0.06559580226118365\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13244957 0.09291446 0.16463505], LR: [[8.58962534553353e-06], [1.6162895496458737e-05], [1.0604475735226578e-05]], Loss: 0.1299996922692905\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18158652 0.15852436 0.14360641], LR: [[7.730662810980177e-06], [1.6162895496458737e-05], [1.0604475735226578e-05]], Loss: 0.16123909418005497\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.096301   0.02757175 0.00245393], LR: [[7.730662810980177e-06], [1.6162895496458737e-05], [1.0604475735226578e-05]], Loss: 0.042108893493811285\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.01666872 0.089512   0.25882184], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [1.0604475735226578e-05]], Loss: 0.12166752009962996\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16965071 0.06753241 0.00512749], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [1.0604475735226578e-05]], Loss: 0.08077020451736946\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07392738 0.08668544 0.27012305], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [1.0604475735226578e-05]], Loss: 0.1435786264940786\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21852006 0.12409424 0.08920844], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [1.0604475735226578e-05]], Loss: 0.1439409148755173\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04098027 0.04498306 0.08984117], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [1.0604475735226578e-05]], Loss: 0.05860150222724769\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.00434541 0.0245359  0.00161938], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [1.0604475735226578e-05]], Loss: 0.010166896851733326\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12838367 0.01985864 0.08027658], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [9.544028161703921e-06]], Loss: 0.07617296252089241\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02720293 0.09838099 0.00580008], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [9.544028161703921e-06]], Loss: 0.043794662570580845\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02149778 0.07826016 0.02204353], LR: [[7.730662810980177e-06], [1.4546605946812864e-05], [9.544028161703921e-06]], Loss: 0.040600490607321266\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12558997 0.03532809 0.07065884], LR: [[6.95759652988216e-06], [1.4546605946812864e-05], [9.544028161703921e-06]], Loss: 0.07719230360739554\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05889402 0.10434981 0.04147746], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [9.544028161703921e-06]], Loss: 0.06824043042104071\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04112033 0.03247081 0.0223987 ], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [9.544028161703921e-06]], Loss: 0.03199661282318023\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11245157 0.05103342 0.04078169], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [9.544028161703921e-06]], Loss: 0.06808889236296332\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1489131  0.03671223 0.02410188], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [9.544028161703921e-06]], Loss: 0.06990907362662256\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07332696 0.02416964 0.00165996], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [9.544028161703921e-06]], Loss: 0.033052187144445876\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06534838 0.12833045 0.13159499], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [8.58962534553353e-06]], Loss: 0.10842460553239412\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22082003 0.11309053 0.04527482], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [8.58962534553353e-06]], Loss: 0.1263951268179032\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17205593 0.04893356 0.24377199], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [8.58962534553353e-06]], Loss: 0.1549204916848491\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.130358   0.12616093 0.21547766], LR: [[6.95759652988216e-06], [1.3091945352131578e-05], [8.58962534553353e-06]], Loss: 0.15733219524379818\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16698898 0.09374706 0.00338728], LR: [[6.2618368768939444e-06], [1.3091945352131578e-05], [8.58962534553353e-06]], Loss: 0.08804110528193027\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04395777 0.04294149 0.02462388], LR: [[6.2618368768939444e-06], [1.3091945352131578e-05], [8.58962534553353e-06]], Loss: 0.037174376901239155\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0725865  0.05543053 0.00725155], LR: [[6.2618368768939444e-06], [1.178275081691842e-05], [8.58962534553353e-06]], Loss: 0.04508952391489099\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02083629 0.02046376 0.1359606 ], LR: [[6.2618368768939444e-06], [1.178275081691842e-05], [8.58962534553353e-06]], Loss: 0.05908688355993944\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07640145 0.01740199 0.15542868], LR: [[6.2618368768939444e-06], [1.178275081691842e-05], [7.730662810980177e-06]], Loss: 0.08307737172232009\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04748135 0.08198257 0.15892619], LR: [[6.2618368768939444e-06], [1.178275081691842e-05], [7.730662810980177e-06]], Loss: 0.09613003875187132\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.00745077 0.01597989 0.04270189], LR: [[6.2618368768939444e-06], [1.178275081691842e-05], [7.730662810980177e-06]], Loss: 0.022044183453544974\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11421975 0.08346607 0.02037224], LR: [[6.2618368768939444e-06], [1.178275081691842e-05], [7.730662810980177e-06]], Loss: 0.07268601895154765\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12680991 0.10414156 0.18749091], LR: [[5.6356531892045504e-06], [1.178275081691842e-05], [7.730662810980177e-06]], Loss: 0.1394807934388518\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04678357 0.15671671 0.24552506], LR: [[5.6356531892045504e-06], [1.0604475735226578e-05], [7.730662810980177e-06]], Loss: 0.1496751122510371\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1879996  0.07043729 0.10710018], LR: [[5.6356531892045504e-06], [1.0604475735226578e-05], [7.730662810980177e-06]], Loss: 0.12184569018737723\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.062991   0.04623147 0.11072732], LR: [[5.6356531892045504e-06], [1.0604475735226578e-05], [6.95759652988216e-06]], Loss: 0.07331659550468127\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07322392 0.09924987 0.04752953], LR: [[5.6356531892045504e-06], [1.0604475735226578e-05], [6.95759652988216e-06]], Loss: 0.07333443796262144\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09569882 0.04286499 0.15101135], LR: [[5.6356531892045504e-06], [1.0604475735226578e-05], [6.95759652988216e-06]], Loss: 0.09652505524262474\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11424865 0.08539215 0.24605204], LR: [[5.072087870284095e-06], [1.0604475735226578e-05], [6.95759652988216e-06]], Loss: 0.14856428005407604\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03739424 0.01911247 0.18853312], LR: [[5.072087870284095e-06], [1.0604475735226578e-05], [6.95759652988216e-06]], Loss: 0.0816799456917215\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0312728  0.00224574 0.12228767], LR: [[5.072087870284095e-06], [1.0604475735226578e-05], [6.95759652988216e-06]], Loss: 0.05193540034000762\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12183553 0.04355219 0.02410881], LR: [[5.072087870284095e-06], [1.0604475735226578e-05], [6.95759652988216e-06]], Loss: 0.0631655058908897\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2226145  0.10515415 0.13441429], LR: [[5.072087870284095e-06], [9.544028161703921e-06], [6.95759652988216e-06]], Loss: 0.1540609831735492\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2701532  0.13104903 0.02025136], LR: [[5.072087870284095e-06], [9.544028161703921e-06], [6.95759652988216e-06]], Loss: 0.14048452714535717\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1187747  0.09775559 0.13084887], LR: [[5.072087870284095e-06], [9.544028161703921e-06], [6.2618368768939444e-06]], Loss: 0.11579304979338001\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15394945 0.00642363 0.00452233], LR: [[4.564879083255686e-06], [9.544028161703921e-06], [6.2618368768939444e-06]], Loss: 0.054965137244046974\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12054791 0.08673257 0.11282401], LR: [[4.564879083255686e-06], [9.544028161703921e-06], [6.2618368768939444e-06]], Loss: 0.10670149852521717\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13993748 0.01987984 0.13328887], LR: [[4.564879083255686e-06], [9.544028161703921e-06], [6.2618368768939444e-06]], Loss: 0.09770206197553004\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08909306 0.0403044  0.1494556 ], LR: [[4.564879083255686e-06], [9.544028161703921e-06], [6.2618368768939444e-06]], Loss: 0.0929510162761532\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05984871 0.05953691 0.19856474], LR: [[4.564879083255686e-06], [8.58962534553353e-06], [5.6356531892045504e-06]], Loss: 0.10598345163588722\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26899712 0.13885589 0.11549988], LR: [[4.564879083255686e-06], [8.58962534553353e-06], [5.6356531892045504e-06]], Loss: 0.17445096589780104\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18434108 0.08373842 0.08444029], LR: [[4.564879083255686e-06], [8.58962534553353e-06], [5.6356531892045504e-06]], Loss: 0.11750659790510933\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20072954 0.12496758 0.15611031], LR: [[4.564879083255686e-06], [8.58962534553353e-06], [5.6356531892045504e-06]], Loss: 0.16060247699419658\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02980643 0.02273403 0.1643386 ], LR: [[4.564879083255686e-06], [8.58962534553353e-06], [5.6356531892045504e-06]], Loss: 0.07229302280582488\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09003457 0.1067743  0.13851522], LR: [[4.1083911749301174e-06], [8.58962534553353e-06], [5.6356531892045504e-06]], Loss: 0.11177469456878801\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11396273 0.05670233 0.05292322], LR: [[4.1083911749301174e-06], [8.58962534553353e-06], [5.6356531892045504e-06]], Loss: 0.07452942866713176\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07598847 0.06629923 0.1808012 ], LR: [[4.1083911749301174e-06], [7.730662810980177e-06], [5.6356531892045504e-06]], Loss: 0.10769629714079203\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02578599 0.04253669 0.04797313], LR: [[4.1083911749301174e-06], [7.730662810980177e-06], [5.6356531892045504e-06]], Loss: 0.03876526947812333\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11553288 0.0473329  0.13666165], LR: [[4.1083911749301174e-06], [7.730662810980177e-06], [5.072087870284095e-06]], Loss: 0.09984247740047673\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21599036 0.08264658 0.09586354], LR: [[4.1083911749301174e-06], [7.730662810980177e-06], [5.072087870284095e-06]], Loss: 0.13150016291998326\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2043399  0.06320649 0.3143485 ], LR: [[4.1083911749301174e-06], [7.730662810980177e-06], [5.072087870284095e-06]], Loss: 0.1939649648824707\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10407971 0.14737496 0.05812312], LR: [[4.1083911749301174e-06], [7.730662810980177e-06], [5.072087870284095e-06]], Loss: 0.10319259689965594\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09120162 0.02251794 0.03903644], LR: [[4.1083911749301174e-06], [7.730662810980177e-06], [5.072087870284095e-06]], Loss: 0.050918667339719836\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11838259 0.1142261  0.00193915], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.07818261458926525\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10468039 0.04024135 0.17487921], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.1066003141916978\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13148826 0.02122897 0.02801567], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.060244300247480474\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08066136 0.1479473  0.14585669], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.1248217834128688\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04479631 0.07746503 0.1164411 ], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.07956748237057278\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05577646 0.02629942 0.09646024], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.05951203993676851\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.01583381 0.0359529  0.01453569], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.022107464875443836\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13150396 0.01783369 0.00061299], LR: [[3.697552057437106e-06], [6.95759652988216e-06], [5.072087870284095e-06]], Loss: 0.04998354975755016\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13450326 0.09146872 0.18704573], LR: [[3.3277968516933954e-06], [6.95759652988216e-06], [4.564879083255686e-06]], Loss: 0.13767257087553542\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19872329 0.02166428 0.13796093], LR: [[3.3277968516933954e-06], [6.95759652988216e-06], [4.564879083255686e-06]], Loss: 0.1194494977343129\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20998129 0.08034407 0.02056103], LR: [[3.3277968516933954e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.10362879929016344\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19404588 0.02382889 0.04576019], LR: [[3.3277968516933954e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.08787831996257107\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15763946 0.13039438 0.04494299], LR: [[3.3277968516933954e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.11099227736393609\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18330452 0.03351453 0.00706747], LR: [[3.3277968516933954e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.07462884185370057\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.01612915 0.01396339 0.03232968], LR: [[3.3277968516933954e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.020807410075018806\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10113078 0.06362472 0.16931519], LR: [[2.995017166524056e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.11135689565601448\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05088378 0.00383324 0.16571747], LR: [[2.995017166524056e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.0734781640364478\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.227082   0.15946738 0.05063016], LR: [[2.995017166524056e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.14572651057550803\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15150908 0.07829575 0.02203074], LR: [[2.995017166524056e-06], [6.2618368768939444e-06], [4.564879083255686e-06]], Loss: 0.08394518791659115\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1983015  0.03899558 0.15734694], LR: [[2.995017166524056e-06], [6.2618368768939444e-06], [4.1083911749301174e-06]], Loss: 0.13154800574605663\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14066203 0.03664456 0.0555688 ], LR: [[2.995017166524056e-06], [6.2618368768939444e-06], [4.1083911749301174e-06]], Loss: 0.07762512969978465\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18494294 0.04316126 0.12473326], LR: [[2.995017166524056e-06], [5.6356531892045504e-06], [4.1083911749301174e-06]], Loss: 0.11761248704325529\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04212915 0.0258969  0.14882363], LR: [[2.995017166524056e-06], [5.6356531892045504e-06], [4.1083911749301174e-06]], Loss: 0.0722832251060754\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17474741 0.10947148 0.09549254], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [4.1083911749301174e-06]], Loss: 0.12657047955493908\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08161961 0.10441603 0.00124414], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [4.1083911749301174e-06]], Loss: 0.06242659357589825\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06061241 0.0634384  0.20213123], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [4.1083911749301174e-06]], Loss: 0.10872734581527765\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04535001 0.0378388  0.02088048], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [4.1083911749301174e-06]], Loss: 0.03468976241691659\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19220096 0.10569048 0.07416386], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [3.697552057437106e-06]], Loss: 0.12401843401448182\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09164438 0.0962737  0.14947355], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [3.697552057437106e-06]], Loss: 0.11246387556893751\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2090539  0.03374422 0.02381812], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [3.697552057437106e-06]], Loss: 0.08887208343638726\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12565612 0.015313   0.10840264], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [3.697552057437106e-06]], Loss: 0.08312391909770668\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08855291 0.08193746 0.12152791], LR: [[2.69551544987165e-06], [5.6356531892045504e-06], [3.697552057437106e-06]], Loss: 0.09733942476964634\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.25132406 0.14578447 0.05818771], LR: [[2.69551544987165e-06], [5.072087870284095e-06], [3.697552057437106e-06]], Loss: 0.15176541705518806\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2817454  0.13013916 0.05632439], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.697552057437106e-06]], Loss: 0.1560696500702761\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10700121 0.06115851 0.13316914], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.1004429538951566\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22664955 0.02076302 0.12923086], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.12554780741144592\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16466678 0.09246305 0.02046175], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.09253052514550897\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19021032 0.00341415 0.07044098], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.08802181826972326\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13078997 0.10423451 0.00085338], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.07862595467440163\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02082836 0.02217497 0.17463382], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.07254571911995299\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0258742  0.04727815 0.09198531], LR: [[2.425963904884485e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.055045886940715716\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16195424 0.00289742 0.04446848], LR: [[2.1833675143960364e-06], [5.072087870284095e-06], [3.3277968516933954e-06]], Loss: 0.06977337824374748\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0232263  0.14176847 0.02404315], LR: [[2.1833675143960364e-06], [4.564879083255686e-06], [3.3277968516933954e-06]], Loss: 0.06301263834427422\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05778358 0.06746126 0.10407796], LR: [[2.1833675143960364e-06], [4.564879083255686e-06], [3.3277968516933954e-06]], Loss: 0.07644093263583879\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2373712  0.07227581 0.06902312], LR: [[2.1833675143960364e-06], [4.564879083255686e-06], [3.3277968516933954e-06]], Loss: 0.12622337564011105\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09816181 0.05008603 0.06398227], LR: [[2.1833675143960364e-06], [4.564879083255686e-06], [3.3277968516933954e-06]], Loss: 0.07074336983030662\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02249813 0.07674726 0.06574946], LR: [[2.1833675143960364e-06], [4.564879083255686e-06], [2.995017166524056e-06]], Loss: 0.054998284967150535\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02557554 0.04587557 0.02040305], LR: [[2.1833675143960364e-06], [4.564879083255686e-06], [2.995017166524056e-06]], Loss: 0.030618049988212687\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22519138 0.02482111 0.17373047], LR: [[1.965030762956433e-06], [4.564879083255686e-06], [2.995017166524056e-06]], Loss: 0.14124765129585284\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.01428456 0.02640958 0.00096265], LR: [[1.965030762956433e-06], [4.564879083255686e-06], [2.995017166524056e-06]], Loss: 0.013885594855067516\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17400318 0.05965725 0.20727764], LR: [[1.965030762956433e-06], [4.1083911749301174e-06], [2.995017166524056e-06]], Loss: 0.14697935529906925\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18223516 0.06668945 0.04058575], LR: [[1.965030762956433e-06], [4.1083911749301174e-06], [2.995017166524056e-06]], Loss: 0.09650345685193314\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19211372 0.12723016 0.10815125], LR: [[1.965030762956433e-06], [4.1083911749301174e-06], [2.995017166524056e-06]], Loss: 0.1424983758215482\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.01762521 0.01658323 0.04455655], LR: [[1.965030762956433e-06], [4.1083911749301174e-06], [2.995017166524056e-06]], Loss: 0.026254995188986264\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04072349 0.05362842 0.0580423 ], LR: [[1.7685276866607898e-06], [4.1083911749301174e-06], [2.69551544987165e-06]], Loss: 0.050798070367576054\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10152719 0.00051782 0.04408496], LR: [[1.7685276866607898e-06], [4.1083911749301174e-06], [2.69551544987165e-06]], Loss: 0.048709991937503215\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.19197975 0.15784015 0.12576224], LR: [[1.7685276866607898e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.1585273791066235\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12809099 0.02210695 0.02249987], LR: [[1.7685276866607898e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.057565936780107824\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15218574 0.01419217 0.17611101], LR: [[1.7685276866607898e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.11416297322197351\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.24075616 0.01486478 0.02592174], LR: [[1.591674917994711e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.09384755770035554\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22490814 0.04628961 0.18574164], LR: [[1.591674917994711e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.15231313023716211\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.29777028 0.06282105 0.16675526], LR: [[1.591674917994711e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.1757821956851209\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02095631 0.02254256 0.0292841 ], LR: [[1.591674917994711e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.024260991448148462\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.11166049 0.01799144 0.00192742], LR: [[1.591674917994711e-06], [3.697552057437106e-06], [2.69551544987165e-06]], Loss: 0.04385978405286248\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17323259 0.00577403 0.10188224], LR: [[1.591674917994711e-06], [3.697552057437106e-06], [2.425963904884485e-06]], Loss: 0.09362961979718724\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08809516 0.08169525 0.18108483], LR: [[1.591674917994711e-06], [3.3277968516933954e-06], [2.425963904884485e-06]], Loss: 0.11695841277639073\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02581847 0.04382796 0.02115471], LR: [[1.591674917994711e-06], [3.3277968516933954e-06], [2.425963904884485e-06]], Loss: 0.030267046458320696\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.25868262 0.08812128 0.04967619], LR: [[1.43250742619524e-06], [3.3277968516933954e-06], [2.425963904884485e-06]], Loss: 0.13216002974551277\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08551492 0.04562653 0.        ], LR: [[1.43250742619524e-06], [3.3277968516933954e-06], [2.425963904884485e-06]], Loss: 0.04371381593480086\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13102379 0.10041555 0.1529563 ], LR: [[1.43250742619524e-06], [3.3277968516933954e-06], [2.425963904884485e-06]], Loss: 0.12813188054598867\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13108333 0.02094888 0.05921481], LR: [[1.43250742619524e-06], [3.3277968516933954e-06], [2.425963904884485e-06]], Loss: 0.07041567449535553\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.08291563 0.00096075 0.00017226], LR: [[1.43250742619524e-06], [3.3277968516933954e-06], [2.425963904884485e-06]], Loss: 0.02801621283947801\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06861775 0.08015169 0.08398141], LR: [[1.43250742619524e-06], [3.3277968516933954e-06], [2.1833675143960364e-06]], Loss: 0.07758361864058923\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.17188484 0.15001982 0.08372125], LR: [[1.43250742619524e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.135208635797729\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07047243 0.02051311 0.07179763], LR: [[1.43250742619524e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.05426105317465651\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.090959   0.04168414 0.10211798], LR: [[1.289256683575716e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.07825370905416397\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06388051 0.03721506 0.03514158], LR: [[1.289256683575716e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.04541238190795411\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.25016845 0.16087382 0.06      ], LR: [[1.289256683575716e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.15701408903812988\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13502348 0.06188435 0.06015088], LR: [[1.289256683575716e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.0856862366492957\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2144262  0.04461286 0.02386734], LR: [[1.289256683575716e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.09430213371446977\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.05380548 0.08750093 0.01421729], LR: [[1.289256683575716e-06], [2.995017166524056e-06], [2.1833675143960364e-06]], Loss: 0.051841233576027046\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06617539 0.00474924 0.12574997], LR: [[1.289256683575716e-06], [2.995017166524056e-06], [1.965030762956433e-06]], Loss: 0.06555819877578567\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21304039 0.04861967 0.14265123], LR: [[1.1603310152181443e-06], [2.69551544987165e-06], [1.965030762956433e-06]], Loss: 0.1347704325352485\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04425207 0.05894751 0.08594762], LR: [[1.1603310152181443e-06], [2.69551544987165e-06], [1.965030762956433e-06]], Loss: 0.0630490649753483\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04182846 0.108004   0.04397287], LR: [[1.1603310152181443e-06], [2.69551544987165e-06], [1.965030762956433e-06]], Loss: 0.06460177580670765\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10840173 0.01539944 0.05089847], LR: [[1.1603310152181443e-06], [2.69551544987165e-06], [1.965030762956433e-06]], Loss: 0.05823321238082523\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02355925 0.03340384 0.12873272], LR: [[1.1603310152181443e-06], [2.69551544987165e-06], [1.965030762956433e-06]], Loss: 0.061898600218895206\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10287684 0.09253061 0.04123985], LR: [[1.1603310152181443e-06], [2.425963904884485e-06], [1.965030762956433e-06]], Loss: 0.0788824315648526\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02924095 0.03503181 0.08958044], LR: [[1.1603310152181443e-06], [2.425963904884485e-06], [1.7685276866607898e-06]], Loss: 0.05128440196975135\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.22860306 0.0680941  0.02025589], LR: [[1.1603310152181443e-06], [2.425963904884485e-06], [1.7685276866607898e-06]], Loss: 0.10565101760051525\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04385161 0.08572279 0.06149928], LR: [[1.1603310152181443e-06], [2.425963904884485e-06], [1.7685276866607898e-06]], Loss: 0.06369122823700309\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.27708523 0.03734051 0.04452866], LR: [[1.04429791369633e-06], [2.425963904884485e-06], [1.7685276866607898e-06]], Loss: 0.11965146698406898\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.15207548 0.02556729 0.02135319], LR: [[1.04429791369633e-06], [2.425963904884485e-06], [1.7685276866607898e-06]], Loss: 0.06633198499291514\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04348814 0.04521614 0.18713146], LR: [[1.04429791369633e-06], [2.425963904884485e-06], [1.7685276866607898e-06]], Loss: 0.09194524687870094\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13530453 0.04053436 0.0400554 ], LR: [[1.04429791369633e-06], [2.425963904884485e-06], [1.7685276866607898e-06]], Loss: 0.07196476295202349\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10995502 0.08672297 0.02012032], LR: [[1.04429791369633e-06], [2.1833675143960364e-06], [1.7685276866607898e-06]], Loss: 0.07226610104708621\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26255747 0.15084833 0.06364666], LR: [[1.04429791369633e-06], [2.1833675143960364e-06], [1.7685276866607898e-06]], Loss: 0.15901748367158386\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1431092  0.013504   0.00054564], LR: [[1.04429791369633e-06], [2.1833675143960364e-06], [1.7685276866607898e-06]], Loss: 0.05238627783522437\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.20391057 0.03506597 0.00024859], LR: [[1.04429791369633e-06], [2.1833675143960364e-06], [1.7685276866607898e-06]], Loss: 0.07974171021667037\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.18329362 0.05027216 0.12890056], LR: [[1.04429791369633e-06], [2.1833675143960364e-06], [1.591674917994711e-06]], Loss: 0.1208221122546456\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0318661 0.0337025 0.02     ], LR: [[1.04429791369633e-06], [2.1833675143960364e-06], [1.591674917994711e-06]], Loss: 0.028522867136246837\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.10479436 0.00108638 0.02443184], LR: [[9.39868122326697e-07], [2.1833675143960364e-06], [1.591674917994711e-06]], Loss: 0.04343752522487194\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.32263207 0.06301503 0.17574796], LR: [[9.39868122326697e-07], [1.965030762956433e-06], [1.591674917994711e-06]], Loss: 0.18713168828806373\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03798232 0.03973565 0.02020912], LR: [[9.39868122326697e-07], [1.965030762956433e-06], [1.591674917994711e-06]], Loss: 0.032642364637771\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1225077  0.04550861 0.02003551], LR: [[9.39868122326697e-07], [1.965030762956433e-06], [1.591674917994711e-06]], Loss: 0.06268393967401305\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1811044 0.0200937 0.0854492], LR: [[9.39868122326697e-07], [1.965030762956433e-06], [1.591674917994711e-06]], Loss: 0.09554910211211487\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03445579 0.08563211 0.02782481], LR: [[9.39868122326697e-07], [1.965030762956433e-06], [1.591674917994711e-06]], Loss: 0.04930423457728466\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02371411 0.04344298 0.22243602], LR: [[9.39868122326697e-07], [1.965030762956433e-06], [1.43250742619524e-06]], Loss: 0.0965310362279221\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.00940591 0.03444627 0.00035625], LR: [[9.39868122326697e-07], [1.965030762956433e-06], [1.43250742619524e-06]], Loss: 0.014736142557424803\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16060413 0.02798262 0.0295941 ], LR: [[8.458813100940273e-07], [1.965030762956433e-06], [1.43250742619524e-06]], Loss: 0.072726949856927\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.21682709 0.00410065 0.10007154], LR: [[8.458813100940273e-07], [1.965030762956433e-06], [1.43250742619524e-06]], Loss: 0.10699975695072983\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04484211 0.08523821 0.02860323], LR: [[8.458813100940273e-07], [1.965030762956433e-06], [1.43250742619524e-06]], Loss: 0.0528945145336911\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12130696 0.06       0.17287418], LR: [[8.458813100940273e-07], [1.965030762956433e-06], [1.43250742619524e-06]], Loss: 0.11806038125418128\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.03238596 0.02       0.02      ], LR: [[8.458813100940273e-07], [1.965030762956433e-06], [1.43250742619524e-06]], Loss: 0.024128651980621123\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07432186 0.02373764 0.22148608], LR: [[8.458813100940273e-07], [1.7685276866607898e-06], [1.289256683575716e-06]], Loss: 0.10651519416132942\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02917242 0.01675966 0.23009882], LR: [[8.458813100940273e-07], [1.7685276866607898e-06], [1.289256683575716e-06]], Loss: 0.09201029905673447\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.00187987 0.08220775 0.1114162 ], LR: [[8.458813100940273e-07], [1.7685276866607898e-06], [1.289256683575716e-06]], Loss: 0.0651679388138776\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12926357 0.03471593 0.17255259], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.289256683575716e-06]], Loss: 0.11217736209315869\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [1.02068513e-01 1.52725221e-04 2.00009295e-01], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.289256683575716e-06]], Loss: 0.10074351136184607\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.2202184  0.06014671 0.12480104], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.289256683575716e-06]], Loss: 0.13505538498284295\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.09843318 0.05712753 0.02911763], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.289256683575716e-06]], Loss: 0.06155944845561559\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16976327 0.0454534  0.14700369], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.1603310152181443e-06]], Loss: 0.1207401206078551\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14147852 0.02039553 0.02029477], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.1603310152181443e-06]], Loss: 0.06072293926185618\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.12257371 0.00346006 0.00054919], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.1603310152181443e-06]], Loss: 0.04219432190312849\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.02336254 0.02       0.02727672], LR: [[7.612931790846246e-07], [1.7685276866607898e-06], [1.1603310152181443e-06]], Loss: 0.023546419881361847\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.25408526 0.070993   0.02051991], LR: [[7.612931790846246e-07], [1.591674917994711e-06], [1.1603310152181443e-06]], Loss: 0.11519939074215169\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.01279114 0.00071329 0.01953396], LR: [[7.612931790846246e-07], [1.591674917994711e-06], [1.1603310152181443e-06]], Loss: 0.011012794464671363\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.0401322  0.04985465 0.16980427], LR: [[6.851638611761621e-07], [1.591674917994711e-06], [1.1603310152181443e-06]], Loss: 0.08659703940153123\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04423237 0.05604367 0.07909269], LR: [[6.851638611761621e-07], [1.591674917994711e-06], [1.1603310152181443e-06]], Loss: 0.059789579625551904\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.26362431 0.05423047 0.11900381], LR: [[6.851638611761621e-07], [1.591674917994711e-06], [1.1603310152181443e-06]], Loss: 0.1456195303638621\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14533538 0.05087285 0.10715259], LR: [[6.851638611761621e-07], [1.591674917994711e-06], [1.1603310152181443e-06]], Loss: 0.10112027101102285\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.16454686 0.04008107 0.11666074], LR: [[6.851638611761621e-07], [1.591674917994711e-06], [1.04429791369633e-06]], Loss: 0.10709622169379145\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04038283 0.03646013 0.00682806], LR: [[6.851638611761621e-07], [1.591674917994711e-06], [1.04429791369633e-06]], Loss: 0.027890338112677757\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.13334183 0.02624264 0.04238071], LR: [[6.166474750585459e-07], [1.591674917994711e-06], [1.04429791369633e-06]], Loss: 0.06732172667805571\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07593644 0.04480953 0.03558713], LR: [[6.166474750585459e-07], [1.591674917994711e-06], [1.04429791369633e-06]], Loss: 0.05211103383839751\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.07821662 0.         0.02864504], LR: [[6.166474750585459e-07], [1.591674917994711e-06], [1.04429791369633e-06]], Loss: 0.0356205520479125\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1724067  0.0796397  0.16071022], LR: [[6.166474750585459e-07], [1.43250742619524e-06], [1.04429791369633e-06]], Loss: 0.13758553738628201\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.24623403 0.0467114  0.03216689], LR: [[6.166474750585459e-07], [1.43250742619524e-06], [1.04429791369633e-06]], Loss: 0.1083707686366203\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.14100888 0.03475632 0.02045975], LR: [[6.166474750585459e-07], [1.43250742619524e-06], [1.04429791369633e-06]], Loss: 0.06540831764626394\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.1062206  0.04864687 0.06200707], LR: [[6.166474750585459e-07], [1.43250742619524e-06], [1.04429791369633e-06]], Loss: 0.07229150996310636\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.06561073 0.         0.01841707], LR: [[6.166474750585459e-07], [1.43250742619524e-06], [1.04429791369633e-06]], Loss: 0.02800926552464565\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.04599999 0.11228952 0.11869078], LR: [[6.166474750585459e-07], [1.43250742619524e-06], [9.39868122326697e-07]], Loss: 0.09232676494924817\n",
      "Unsupervised Training.. 0.0\n",
      "Unsupervised Training.. 10.0\n",
      "Unsupervised Training.. 20.0\n",
      "Unsupervised Training.. 30.0\n",
      "Unsupervised Training.. 40.0\n",
      "Unsupervised Training.. 50.0\n",
      "Unsupervised Training.. 60.0\n",
      "Unsupervised Training.. 70.0\n",
      "Unsupervised Training.. 80.0\n",
      "Unsupervised Training.. 90.0\n",
      "Losses: [0.00924122 0.00363525 0.00058917], LR: [[6.166474750585459e-07], [1.43250742619524e-06], [9.39868122326697e-07]], Loss: 0.004488549557281659\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "classification_set = {\n",
    "    0 : [],\n",
    "    1 : [],\n",
    "    2 : []\n",
    "}\n",
    "\n",
    "PRETRAINING = True\n",
    "target = 0.01\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ensemble = Ensemble(size=3, output_size=5, lr_series=[15e-4, 15e-4, 15e-4], learning_decay=0.9, decay_step=4, threshold=6.0, weight_decay=1e-4)\n",
    "ensemble.load_ensemble(\"toy-SIMCLR\")\n",
    "sampled_dataset = SwarmDataset(\"data/toy\", rank=0)\n",
    "data = sampled_dataset\n",
    "\n",
    "# Separate\n",
    "for i in range(len(sampled_dataset)):\n",
    "    _class = sampled_dataset[i][1][0]\n",
    "    classification_set[_class].append(i)\n",
    "\n",
    "# Pair Up\n",
    "SAMPLES = 3000\n",
    "triplets = []\n",
    "for i in range(SAMPLES):\n",
    "    classA = random.randint(0, 2)\n",
    "    classB = random.randint(0, 2)\n",
    "\n",
    "    anchor = random.choice(classification_set[classA])\n",
    "    positive = random.choice(classification_set[classA])\n",
    "    negative = random.choice(classification_set[classB])\n",
    "    triplet = [anchor, positive, negative]\n",
    "    if triplet not in triplets:\n",
    "        triplets.append(triplet)\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 5\n",
    "EPOCH_DATA_LIM = 500\n",
    "while loss > target:\n",
    "    total_updates = 0\n",
    "    total_loss = np.array([0.0 for i in range(len(ensemble.ensemble))])\n",
    "    random.shuffle(triplets)\n",
    "    temp_triplets = triplets[:EPOCH_DATA_LIM]\n",
    "    for i in range(0, len(temp_triplets), BATCH_SIZE):\n",
    "        if total_updates % 10 == 0:\n",
    "            print(f\"Unsupervised Training.. {(total_updates * BATCH_SIZE * 100) / len(temp_triplets)}\")\n",
    "\n",
    "        if i + BATCH_SIZE > len(triplets):\n",
    "            break\n",
    "\n",
    "        anchors = np.array([data[temp_triplets[i + j][0]][0] for j in range(BATCH_SIZE)])\n",
    "        positives = np.array([data[temp_triplets[i + j][1]][0] for j in range(BATCH_SIZE)])\n",
    "        negatives = np.array([data[temp_triplets[i + j][2]][0] for j in range(BATCH_SIZE)])\n",
    "\n",
    "        anchors = np.expand_dims(anchors, axis=1)\n",
    "        positives = np.expand_dims(positives, axis=1)\n",
    "        negatives = np.expand_dims(negatives, axis=1)\n",
    "\n",
    "        losses = ensemble.train_batch(anchors, positives, negatives)\n",
    "        total_loss += losses\n",
    "        total_updates += 1\n",
    "\n",
    "    l = total_loss / total_updates\n",
    "    lr = ensemble.evaluate_lr(l)\n",
    "    loss = sum(l) / len(l)\n",
    "    print(f\"Losses: {l}, LR: {lr}, Loss: {loss}\")\n",
    "\n",
    "print(\"Complete!\")\n",
    "# ensemble.save_ensemble(f\"{int(time.time())}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "ensemble.save_ensemble(f\"{int(time.time())}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize Embeddings with Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from networks.archive import DataAggregationArchive\n",
    "from networks.ensemble import Ensemble\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "PRETRAINING = True\n",
    "ENSEMBLE_MEMBER = 0\n",
    "target = 0.01\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ensemble = Ensemble(size=3, output_size=32, lr_series=[15e-4, 15e-4, 15e-4], learning_decay=0.7, decay_step=1, threshold=9.0, weight_decay=0.0, new_model=True)\n",
    "ensemble.load_ensemble(\"tiny-toy-CLR-d32-C\")\n",
    "# ensemble.load_ensemble(\"tiny-toy-C\")\n",
    "# ensemble.load_ensemble(\"toy-HIL-forced\")\n",
    "ensemble.eval_mode()\n",
    "sampled_dataset = SwarmDataset(\"data/tinytoy\", rank=0)\n",
    "data = sampled_dataset\n",
    "\n",
    "embeddings = []\n",
    "classes = []\n",
    "for i in range(len(data)):\n",
    "    image, _class = sampled_dataset[i][0], sampled_dataset[i][1][0]\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    embed = ensemble.ensemble[ENSEMBLE_MEMBER].forward(torch.tensor(image, device=device, dtype=torch.float))\n",
    "    embed = embed.detach().cpu().squeeze(dim=0).numpy()\n",
    "    embeddings.append(embed)\n",
    "    classes.append(_class)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/Desktop/research/SwarmNoveltyNetwork/.env/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:996: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reduced = TSNE(\n",
    "    n_components=2,\n",
    "    learning_rate=\"auto\",\n",
    "    init=\"pca\",\n",
    "    perplexity=200,\n",
    "    early_exaggeration=1\n",
    ").fit_transform(embeddings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7efc21e163b0>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "\n",
    "color_classes = {\n",
    "    0: [1, 0, 0],\n",
    "    1: [0, 1, 0],\n",
    "    2: [0, 0, 1],\n",
    "    4: [1, 1, 0],\n",
    "}\n",
    "\n",
    "label_classes = {\n",
    "    0: \"Circles\",\n",
    "    1: \"Triangles\",\n",
    "    2: \"Squares\",\n",
    "    4: \"OOD\",\n",
    "}\n",
    "\n",
    "lim = len(reduced)\n",
    "x = [reduced[i][0] for i in range(lim)]\n",
    "y = [reduced[i][1] for i in range(lim)]\n",
    "colors = [color_classes[classes[i]] for i in range(lim)]\n",
    "labels = [label_classes[classes[i]] for i in range(lim)]\n",
    "plot.grid(True)\n",
    "plot.scatter(x, y, c=colors)\n",
    "plot.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x7efc60135900>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list\n",
    "\n",
    "color_classes = {\n",
    "    0: [1, 0, 0],\n",
    "    1: [0, 1, 0],\n",
    "    2: [0, 0, 1],\n",
    "}\n",
    "\n",
    "label_classes = {\n",
    "    0: \"Circles\",\n",
    "    1: \"Triangles\",\n",
    "    2: \"Squares\",\n",
    "}\n",
    "\n",
    "triples = []\n",
    "ensemble.eval_mode()\n",
    "OUT = \"data/toy-oracle\"\n",
    "with open(os.path.join(OUT, \"triplets.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        triplet = CSVLineToVec(line)\n",
    "        triples.append([int(triplet[0]), int(triplet[1]), int(triplet[2])])\n",
    "\n",
    "target_triplet = triples[13]\n",
    "lim = 500\n",
    "x = [reduced[i][0] for i in range(lim)]\n",
    "y = [reduced[i][1] for i in range(lim)]\n",
    "colors = [color_classes[classes[i]] for i in range(lim)]\n",
    "labels = [label_classes[classes[i]] for i in range(lim)]\n",
    "\n",
    "# colors[target_triplet[0]] = [0, 0, 0]\n",
    "# colors[target_triplet[1]] = [0, 0, 0]\n",
    "# colors[target_triplet[2]] = [0.2, 0, 0]\n",
    "\n",
    "plot.grid(True)\n",
    "plot.scatter(x, y, c=colors)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
