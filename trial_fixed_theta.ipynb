{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "from NovelSwarmBehavior.novel_swarms.config.AgentConfig import DiffDriveAgentConfig\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.BehaviorDiscovery import BehaviorDiscovery\n",
    "from NovelSwarmBehavior.novel_swarms.config.EvolutionaryConfig import GeneticEvolutionConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.WorldConfig import RectangularWorldConfig\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.GeneRule import GeneRule\n",
    "from NovelSwarmBehavior.novel_swarms.config.OutputTensorConfig import OutputTensorConfig\n",
    "from NovelSwarmBehavior.novel_swarms.sensors.BinaryLOSSensor import BinaryLOSSensor\n",
    "from NovelSwarmBehavior.novel_swarms.sensors.SensorSet import SensorSet\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "from generation.halted_evolution import HaltedEvolution\n",
    "\n",
    "def build_random_dataset():\n",
    "    sensors = SensorSet([\n",
    "        BinaryLOSSensor(angle=-(np.pi/4)),\n",
    "        # BinaryFOVSensor(theta=14 / 2, distance=125, degrees=True)\n",
    "    ])\n",
    "\n",
    "    agent_config = DiffDriveAgentConfig(\n",
    "        sensors=sensors,\n",
    "    )\n",
    "\n",
    "    genotype = [\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "        GeneRule(_max=1.0, _min=-1.0, mutation_step=0.4, round_digits=4),\n",
    "    ]\n",
    "\n",
    "    phenotype = ConfigurationDefaults.BEHAVIOR_VECTOR\n",
    "\n",
    "    world_config = RectangularWorldConfig(\n",
    "        size=(500, 500),\n",
    "        n_agents=24,\n",
    "        behavior=phenotype,\n",
    "        agentConfig=agent_config,\n",
    "        padding=15\n",
    "    )\n",
    "\n",
    "    novelty_config = GeneticEvolutionConfig(\n",
    "        gene_rules=genotype,\n",
    "        phenotype_config=phenotype,\n",
    "        n_generations=100,\n",
    "        n_population=100,\n",
    "        crossover_rate=0.7,\n",
    "        mutation_rate=0.15,\n",
    "        world_config=world_config,\n",
    "        k_nn=8,\n",
    "        simulation_lifespan=800,\n",
    "        display_novelty=False,\n",
    "        save_archive=False,\n",
    "        show_gui=True\n",
    "    )\n",
    "\n",
    "    pygame.init()\n",
    "    pygame.display.set_caption(\"Evolutionary Novelty Search\")\n",
    "    screen = pygame.display.set_mode((world_config.w, world_config.h))\n",
    "\n",
    "    output_config = OutputTensorConfig(\n",
    "        timeless=True,\n",
    "        total_frames=80,\n",
    "        steps_between_frames=2,\n",
    "        screen=screen\n",
    "    )\n",
    "\n",
    "    halted_evolution = HaltedEvolution(\n",
    "        world=world_config,\n",
    "        evolution_config=novelty_config,\n",
    "        output_config=output_config\n",
    "    )\n",
    "\n",
    "    baseline_data = DataBuilder(\"data/full-fixed-theta\", steps=1200, agents=24, ev=halted_evolution, screen=screen)\n",
    "    baseline_data.create()\n",
    "    baseline_data.evolution.close()\n",
    "\n",
    "build_random_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embed into Latent Space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from generation.evolution import ModifiedHaltingEvolution\n",
    "from networks.archive import DataAggregationArchive\n",
    "from hil.HIL import HIL\n",
    "\n",
    "def update_network(network, loss_fn, optim, anchor_image, pos_image, neg_image):\n",
    "    optim.zero_grad()\n",
    "    anchor_out, pos_out, neg_out = network.network_with_transforms(anchor_image, pos_image, neg_image)\n",
    "    loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "def pretraining(data, network, loss_fn, optim, data_size=500):\n",
    "    samples = np.random.random_integers(0, len(data) - 1, (data_size, 2))\n",
    "    total_loss = 0\n",
    "    total_updates = 0\n",
    "    for i, s in enumerate(samples):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Unsupervised Training.. {(i * 100) / data_size}\")\n",
    "        anchor_image = data[s[0]][0]\n",
    "        pos_image = data[s[0]][0]\n",
    "\n",
    "\n",
    "        if len(data) - 2 > s[1] > 2:\n",
    "            neg_images = [\n",
    "                # data[s[1] - 2][0],\n",
    "                data[s[1] - 1][0],\n",
    "                data[s[1]][0],\n",
    "                # data[s[1] + 1][0],\n",
    "                # data[s[1] + 2][0],\n",
    "            ]\n",
    "        else:\n",
    "            neg_images = [\n",
    "                data[s[1]][0]\n",
    "            ]\n",
    "\n",
    "        for neg in neg_images:\n",
    "            loss = update_network(network, loss_fn, optim, anchor_image, pos_image, neg)\n",
    "            total_loss += loss.item()\n",
    "            total_updates += 1\n",
    "\n",
    "    return total_loss / total_updates\n",
    "\n",
    "\n",
    "def human_in_the_loop(anchor_dataset, network, optimizer, loss_fn, HIL_archive, random_archive, stop_at):\n",
    "    print(\"HIL TIME!\")\n",
    "    improvements, human_loss, triplet_helpfulness, embedded_archive = hil.humanInput(anchor_dataset, network, optimizer, loss_fn, HIL_archive, random_archive, stop_at)\n",
    "    print(f\"Improvement Count: {improvements}, loss: {human_loss}\")\n",
    "\n",
    "    HIL_archive.save_to_file(f\"data/queries/{trial_name}_hil.csv\")\n",
    "    random_archive.save_to_file(f\"data/queries/{trial_name}_rand.csv\")\n",
    "    return improvements, human_loss, triplet_helpfulness, embedded_archive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from generation.evolution import ModifiedHaltingEvolution\n",
    "from networks.archive import DataAggregationArchive\n",
    "from hil.HIL import HIL\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import ndimage\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "trial_name = f\"{str(int(time.time()))}\"\n",
    "\n",
    "TRAIN = True\n",
    "CLUSTER_AND_DISPLAY = True\n",
    "WRITE_OUT = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "writer = SummaryWriter()\n",
    "network = NoveltyEmbedding().to(device)\n",
    "\n",
    "SAVE_CLUSTER_IMAGES = True\n",
    "SAVE_CLUSTER_MEDOIDS = True\n",
    "PRETRAINING = True\n",
    "HUMAN_IN_LOOP = True\n",
    "SYNTHETIC_HIL = True\n",
    "EVOLUTION = True\n",
    "EPOCHS = 20\n",
    "DATA_SIZE = 10000\n",
    "EVOLUTIONS_PER_EPOCH = 12\n",
    "K_CLUSTERS = 8\n",
    "\n",
    "anchor_dataset = ContinuingDataset(\"data\")\n",
    "sampled_dataset = SwarmDataset(\"data/full\", rank=0)\n",
    "# evolution, _ = ModifiedHaltingEvolution.defaultEvolver(steps=800, n_agents=24, evolve_population=100, seed=None)\n",
    "evolution, _ = ModifiedHaltingEvolution.defaultEvolver(steps=1200, n_agents=24, evolve_population=80, seed=None)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.8)\n",
    "\n",
    "# Margin was 10\n",
    "loss_fn = torch.nn.TripletMarginLoss(margin=15)\n",
    "hil = HIL(name=trial_name, synthetic=SYNTHETIC_HIL, data_limiter=DATA_SIZE, clusters=K_CLUSTERS)\n",
    "HIL_archive = DataAggregationArchive()\n",
    "random_archive = DataAggregationArchive(scalar=True)\n",
    "EPSILON = 0.5\n",
    "\n",
    "simulation_time = 0\n",
    "evolution_time = 0\n",
    "training_time = 0\n",
    "hil_time = 0\n",
    "\n",
    "# TODO: Add Randomly-sampled Contrastive/Triplet Loss\n",
    "if PRETRAINING:\n",
    "    network.load_model(\"unsupervised_decay_0.7_target_0.08\")\n",
    "    print(\"Pretrained model loaded!\")\n",
    "\n",
    "dataset = anchor_dataset if EVOLUTION else sampled_dataset\n",
    "\n",
    "if TRAIN:\n",
    "    STOP_FLAG = False\n",
    "    for epoch in range(EPOCHS):\n",
    "        if STOP_FLAG:\n",
    "            break\n",
    "\n",
    "        start_time = time.time()\n",
    "        if EVOLUTION:\n",
    "            for gen in range(EVOLUTIONS_PER_EPOCH if epoch > 0 else 3):\n",
    "                # Simulate current population + Save Data\n",
    "                for i in range(len(evolution.getPopulation())):\n",
    "                    # The collection of the original behavior vector below is only used to collect data to compare with the baseline\n",
    "                    visual_behavior, genome, baseline_behavior = evolution.next()\n",
    "                    dataset.new_entry(visual_behavior, genome, baseline_behavior)\n",
    "                simulation_time += (time.time() - start_time)\n",
    "\n",
    "                # Then, evolve\n",
    "                start_time = time.time()\n",
    "                embedded_archive = hil.getEmbeddedArchive(dataset, network)\n",
    "                evolution.overwriteArchive(embedded_archive, random_archive)\n",
    "                embedded_behavior = embedded_archive.archive[-evolution.evolve_config.population:]\n",
    "                evolution.overwriteBehavior(embedded_behavior)\n",
    "                evolution.evolve()\n",
    "                evolution.restart_screen()\n",
    "                evolution_time += (time.time() - start_time)\n",
    "\n",
    "                print(f\"Evolution complete for e{epoch} and gen{gen}\")\n",
    "\n",
    "                # Record the accuracy of the medoids with respect to the synthetic policy\n",
    "                medoid_acc, cluster_acc = 0, 0\n",
    "                if len(dataset) > 0 and SAVE_CLUSTER_MEDOIDS:\n",
    "                    if EVOLUTION:\n",
    "                        hil.synthetic_knowledge = hil.syntheticBehaviorSpace(dataset)\n",
    "                        print(f\"Synthetic Human Knowledge Size: {len(hil.synthetic_knowledge.labels_)}\")\n",
    "                    medoid_acc, cluster_acc = hil.record_medoids(network, dataset)\n",
    "\n",
    "                # Cluster current dataset, display clusters, and save for analysis\n",
    "                if len(dataset) > 0 and SAVE_CLUSTER_IMAGES:\n",
    "                    hil.embed_and_cluster(network, dataset, auto_quit=True)\n",
    "                    evolution.restart_screen()\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Human in the Loop determines behavior embedding\n",
    "        if HUMAN_IN_LOOP:\n",
    "            improvements, human_loss, triplet_helpfulness, _ =   human_in_the_loop(\n",
    "                                                                dataset,\n",
    "                                                                network,\n",
    "                                                                optimizer,\n",
    "                                                                loss_fn,\n",
    "                                                                HIL_archive,\n",
    "                                                                random_archive,\n",
    "                                                                len(dataset)\n",
    "                                                            )\n",
    "            hil_time += (time.time() - start_time)\n",
    "\n",
    "        # Train on past user information\n",
    "        start_time = time.time()\n",
    "        loss = None\n",
    "\n",
    "        average_loss = 50\n",
    "        loop = 0\n",
    "        TRANSFORMS = 20\n",
    "        network.train()\n",
    "        while (average_loss > 0.01) and loop < 20:\n",
    "\n",
    "            loss_sum = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for i, (anchor, pos, neg) in enumerate(HIL_archive):\n",
    "                anchor_encoding = dataset[anchor][0]\n",
    "                similar_encoding = dataset[pos][0]\n",
    "                anti_encoding = dataset[neg][0]\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                anchor_out, pos_out, neg_out = network.network_from_numpy(anchor_encoding, similar_encoding, anti_encoding)\n",
    "                loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "                loss_sum += loss.item()\n",
    "                if loss.item() > 0:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                total_loss += 1\n",
    "\n",
    "                # anchor_out, pos_out, neg_out = network.network_with_transforms(anchor_encoding, similar_encoding, anti_encoding)\n",
    "                # loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "                # loss_sum += loss.item()\n",
    "                # if loss.item() > 0:\n",
    "                #     loss.backward()\n",
    "                #     optimizer.step()\n",
    "                # total_loss += 1\n",
    "\n",
    "                if loss is not None and i > 0 and i % 20 == 0:\n",
    "                    print(f\"Epoch Progress: {(i*100) / len(HIL_archive)}%, Immediate Loss: {loss.item()}\")\n",
    "\n",
    "            average_loss = loss_sum / (total_loss + 1)\n",
    "            print(f\"Loop: {loop}, Average Loss: {average_loss}\")\n",
    "            loop += 1\n",
    "\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        training_time += (time.time() - start_time)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Average\", loss_sum / (total_loss + 1), epoch)\n",
    "\n",
    "        if SAVE_CLUSTER_MEDOIDS:\n",
    "            writer.add_scalar(\"Accuracy/MedoidClassification\", medoid_acc, epoch)\n",
    "            writer.add_scalar(\"Accuracy/RandomSampleClassification\", cluster_acc, epoch)\n",
    "\n",
    "        if HUMAN_IN_LOOP:\n",
    "            writer.add_scalar(\"Loss/TripletQuality\", triplet_helpfulness, epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Human_Queries\", epoch*8*9, epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Triplets_Generated\", len(HIL_archive), epoch)\n",
    "            writer.add_scalar(\"Queries/Total_Random_Classes\", len(random_archive), epoch)\n",
    "\n",
    "        if EVOLUTION:\n",
    "            writer.add_scalar(\"Novelty/Highest\", evolution.behavior_discovery.getBestScore(), epoch)\n",
    "            writer.add_scalar(\"Novelty/Average\", evolution.behavior_discovery.getAverageScore(), epoch)\n",
    "\n",
    "        writer.add_scalar(\"Time/Simulation\", simulation_time, epoch)\n",
    "        writer.add_scalar(\"Time/HIL\", hil_time, epoch)\n",
    "        writer.add_scalar(\"Time/Evolution\", evolution_time, epoch)\n",
    "        writer.add_scalar(\"Time/Training\", training_time, epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "evolution.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save Model\n",
    "target = 0.08\n",
    "loss = 50\n",
    "network = NoveltyEmbedding().to(device)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "if PRETRAINING:\n",
    "    epochs = 0\n",
    "    while loss > target:\n",
    "        loss = pretraining(sampled_dataset, network, loss_fn, optimizer, data_size=1000)\n",
    "        print(f\"Epoch {epochs}, loss: {loss}\")\n",
    "        epochs += 1\n",
    "        scheduler.step()\n",
    "\n",
    "network.save_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.save_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(optimizer.lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from NovelSwarmBehavior.novel_swarms.config.ResultsConfig import ResultsConfig\n",
    "from NovelSwarmBehavior.novel_swarms.results.results import main as results\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding().to(device)\n",
    "network.load_model(\"trialA-10-18-2022\")\n",
    "# anchor_dataset = SwarmDataset(\"data/full\", rank=0)\n",
    "\n",
    "WRITE_OUT = True\n",
    "if WRITE_OUT:\n",
    "    network.eval()\n",
    "    test_archive = NoveltyArchive()\n",
    "    for i in range(len(anchor_dataset)):\n",
    "        anchor_encoding, genome, _ = anchor_dataset[i]\n",
    "        anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "        embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "        test_archive.addToArchive(embedding, genome)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "import pygame\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=test_archive, k_clusters=6, world_config=world_config, tsne_perplexity=16, tsne_early_exaggeration=12, skip_tsne=False)\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()\n",
    "# results(config)\n",
    "pygame.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering + Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ui.clustering_gui import ClusteringGUI\n",
    "\n",
    "# Cluster over saved behaviors\n",
    "archive = NoveltyArchive()\n",
    "for i, (_, genome, behavior, _, _, _, _) in enumerate(anchor_dataset):\n",
    "    archive.addToArchive(vec=behavior, genome=genome)\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=archive, k_clusters=7, world_config=world_config, tsne_perplexity=20, tsne_early_exaggeration=2, skip_tsne=False)\n",
    "\n",
    "gui = ClusteringGUI(config)\n",
    "gui.displayGUI()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(human_l_hist, \"b\", label='Human Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Time (Epochs)\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
