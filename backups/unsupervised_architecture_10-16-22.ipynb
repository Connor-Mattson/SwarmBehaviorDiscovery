{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "\n",
    "baseline_data = DataBuilder(\"data/full\", steps=1200, agents=24)\n",
    "baseline_data.create()\n",
    "baseline_data.evolution.close()\n",
    "\n",
    "# similar_data = DataBuilder(\"data/similar\", is_similar=True)\n",
    "# similar_data.create()\n",
    "# similar_data.evolution.close()\n",
    "#\n",
    "# anti_data = DataBuilder(\"data/anti\", is_anti=True)\n",
    "# anti_data.create()\n",
    "# anti_data.evolution.close()\n",
    "# from data.swarmset import SwarmDataset, DataBuilder\n",
    "# from networks.encoder import BehaviorAutoEncoder\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import torch\n",
    "#\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# dataset = SwarmDataset(\"data/new\")\n",
    "#\n",
    "# encoder = BehaviorAutoEncoder()\n",
    "# encoder.load_model(\"cp_C-90\")\n",
    "# encoder.to(device)\n",
    "# encoder.eval()\n",
    "#\n",
    "# def resizeInput(X):\n",
    "#     frame = X.astype(np.uint8)\n",
    "#     resized = cv2.resize(frame, dsize=(200, 200), interpolation=cv2.INTER_CUBIC)\n",
    "#     return resized\n",
    "#\n",
    "# def imgToEncoded(X):\n",
    "#     X = torch.Tensor(resizeInput(X)).to(device)\n",
    "#     X = torch.reshape(X, (1, 200*200))\n",
    "#     f_X = encoder.encoded(X).to(device)\n",
    "#     return X, f_X\n",
    "#\n",
    "# def imgToDecoded(X):\n",
    "#     X = torch.Tensor(resizeInput(X)).to(device)\n",
    "#     X = torch.reshape(X, (1, 200*200))\n",
    "#     d = encoder(X).to(device)\n",
    "#     d = torch.reshape(d, (200, 200))\n",
    "#     return X, d\n",
    "#\n",
    "# def encode_dataset(dataset):\n",
    "#     print(\"Encoding Dataset!\")\n",
    "#     for i, (image, genome, behavior, _, _, _, _) in enumerate(dataset):\n",
    "#         _, encoded_val = imgToEncoded(image)\n",
    "#         encoded_val = encoded_val.squeeze(0).cpu().detach().numpy()\n",
    "#         _, decoded = imgToDecoded(image)\n",
    "#         decoded = decoded.cpu().detach().numpy()\n",
    "#         dataset.add_rank(i, encoded_val, is_array=True)\n",
    "#         dataset.add_image(i, decoded)\n",
    "#     dataset.set_rank(1)\n",
    "#\n",
    "# baseline_data = SwarmDataset(\"data/new\")\n",
    "# anti_data = SwarmDataset(\"data/anti\")\n",
    "# similar_data = SwarmDataset(\"data/similar\")\n",
    "#\n",
    "# encode_dataset(baseline_data)\n",
    "# encode_dataset(anti_data)\n",
    "# encode_dataset(similar_data)\n",
    "#\n",
    "# print(\"Done!\")\n",
    "# Embed into Latent Space\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = NoveltyEmbedding().to(device)\n",
    "print(network)\n",
    "\n",
    "\n",
    "def network_from_numpy(network, anchor_img, pos_img, neg_img):\n",
    "    anchor_input = torch.from_numpy(anchor_img).to(device).float()\n",
    "    pos_input = torch.from_numpy(pos_img).to(device).float()\n",
    "    neg_input = torch.from_numpy(neg_img).to(device).float()\n",
    "\n",
    "    anchor_out = network(anchor_input.unsqueeze(0))\n",
    "    pos_out = network(pos_input.unsqueeze(0))\n",
    "    neg_out = network(neg_input.unsqueeze(0))\n",
    "    return anchor_out, pos_out, neg_out\n",
    "\n",
    "\n",
    "class DataAggregationArchive:\n",
    "    def __init__(self):\n",
    "        self.archive = np.array([[]])\n",
    "\n",
    "    def append(self, anchor, pos, neg):\n",
    "        if len(self.archive[0]) == 0:\n",
    "            self.archive = np.array([[anchor, pos, neg]])\n",
    "        else:\n",
    "            self.archive = np.concatenate((self.archive, np.array([[anchor, pos, neg]])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.archive)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.archive[item][0], self.archive[item][1], self.archive[item][2]\n",
    "\n",
    "\n",
    "# Human in the Loop\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from ui.class_similarity import SimilarityGUI\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CLUSTERS = 8\n",
    "\n",
    "\n",
    "def humanInput(anchor_dataset, network, optim, loss_fn, data_archive):\n",
    "    # Begin by clustering all known embeddings into n classes.\n",
    "    # Sample from these clusters and use the medoids as the anchors to hopefully scoop up hard samples.\n",
    "    network.eval()\n",
    "    archive = NoveltyArchive()\n",
    "    for i, (anchor_encoding, genome, _, _, _, _, _) in enumerate(anchor_dataset):\n",
    "        anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "        embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "        archive.addToArchive(vec=embedding, genome=genome)\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=CLUSTERS, random_state=0).fit(archive.archive)\n",
    "    labels = kmedoids.labels_\n",
    "    medoids = kmedoids.medoid_indices_\n",
    "\n",
    "    network.train()\n",
    "    user_help_cases, avg_loss, total_attempts = 0, 0, 0\n",
    "    for cluster_class in range(CLUSTERS + 1):\n",
    "        # Run Triplet Query on the Medoids themselves\n",
    "        if cluster_class == 0:\n",
    "            anchor_image = anchor_dataset[medoids[0]][0]\n",
    "            samples = [medoids[med] for med in range(1, len(medoids))]\n",
    "            subject_images = [anchor_dataset[sampled_class][0] for sampled_class in samples]\n",
    "            ui_input = SimilarityGUI(anchor_image, subject_images)\n",
    "            ui_input.run()\n",
    "            user_responses = np.array(ui_input.assignment)\n",
    "        else:\n",
    "            cluster_class -= 1\n",
    "            examples = np.where(labels == cluster_class)[0]\n",
    "            samples = np.random.choice(examples, min(10, len(examples)))\n",
    "            anchor_image = anchor_dataset[medoids[cluster_class]][0]\n",
    "            subject_images = [anchor_dataset[sampled_class][0] for sampled_class in samples]\n",
    "            ui_input = SimilarityGUI(anchor_image, subject_images)\n",
    "            ui_input.run()\n",
    "            user_responses = np.array(ui_input.assignment)\n",
    "\n",
    "        for i in np.where(user_responses == 1)[0]:\n",
    "            for j in np.where(user_responses == 0)[0]:\n",
    "                anchor_image = anchor_dataset[medoids[cluster_class]][0]\n",
    "                pos_image = anchor_dataset[samples[j]][0]\n",
    "                neg_image = anchor_dataset[samples[i]][0]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                anchor_out, pos_out, neg_out = network_from_numpy(network, anchor_image, pos_image, neg_image)\n",
    "\n",
    "                loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "                avg_loss += loss.item()\n",
    "                total_attempts += 1\n",
    "                print(\"User loss: \", loss)\n",
    "                if loss.item() > 0:\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    user_help_cases += 1\n",
    "                    print(\"BACKPROP!\")\n",
    "\n",
    "                # Add to User Collected Archive\n",
    "                data_archive.append(medoids[cluster_class], samples[j], samples[i])\n",
    "\n",
    "    return user_help_cases, avg_loss / total_attempts\n",
    "\n",
    "\n",
    "import torch\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "from networks.embedding import NoveltyEmbedding\n",
    "from generation.halted_evolution import HaltedEvolution\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from NovelSwarmBehavior.novel_swarms.config.ResultsConfig import ResultsConfig\n",
    "from NovelSwarmBehavior.novel_swarms.results.results import main as results\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from matplotlib import pyplot as plot\n",
    "import numpy as np\n",
    "\n",
    "TRAIN = True\n",
    "CLUSTER_AND_DISPLAY = True\n",
    "WRITE_OUT = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "network = NoveltyEmbedding().to(device)\n",
    "anchor_dataset = SwarmDataset(\"data/new\", rank=1)\n",
    "anti_dataset = SwarmDataset(\"data/anti\", rank=1)\n",
    "similar_dataset = SwarmDataset(\"data/similar\", rank=1)\n",
    "evolution, screen = HaltedEvolution.defaultEvolver(steps=200, n_agents=10)\n",
    "\n",
    "\n",
    "def shakeGenome(genome):\n",
    "    return genome + (np.random.rand(len(genome)) * 0.01)\n",
    "\n",
    "\n",
    "def antiGenome(genome):\n",
    "    return -1 * shakeGenome(genome)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.TripletMarginLoss(margin=10)\n",
    "\n",
    "EPOCHS = 10\n",
    "self_l_hist = []\n",
    "human_l_hist = [0.0 for _ in range(EPOCHS)]\n",
    "HIL_archive = DataAggregationArchive()\n",
    "\n",
    "if TRAIN:\n",
    "\n",
    "    STOP_FLAG = False\n",
    "    for epoch in range(EPOCHS):\n",
    "        if STOP_FLAG:\n",
    "            break\n",
    "\n",
    "        loss_sum = 0\n",
    "        total_loss = 0\n",
    "        # Train on known archive\n",
    "        print(f\"User Data at size: {len(HIL_archive)}\")\n",
    "        if len(HIL_archive) < len(anchor_dataset) // 1.5:\n",
    "            for i, (anchor_encoding, _, _, _, _, _, _) in enumerate(anchor_dataset):\n",
    "                similar_encoding, _, _, _, _, _, _ = similar_dataset[i]\n",
    "                anti_encoding, _, _, _, _, _, _ = anti_dataset[i]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                anchor_out, pos_out, neg_out = network_from_numpy(network, anchor_encoding, similar_encoding,\n",
    "                                                                  anti_encoding)\n",
    "\n",
    "                loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "                loss_sum += loss.item()\n",
    "                if loss.item() > 0:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "        total_loss += i\n",
    "\n",
    "        # Train on past user information\n",
    "        for i, (anchor, pos, neg) in enumerate(HIL_archive):\n",
    "            anchor_encoding = anchor_dataset[anchor][0]\n",
    "            similar_encoding = anchor_dataset[pos][0]\n",
    "            anti_encoding = anchor_dataset[neg][0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_out, pos_out, neg_out = network_from_numpy(network, anchor_encoding, similar_encoding, anti_encoding)\n",
    "            loss = loss_fn(anchor_out, pos_out, neg_out)\n",
    "            loss_sum += loss.item()\n",
    "            if loss.item() > 0:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += i\n",
    "        print(f\"Average Loss: {loss_sum / (i + 1)}\")\n",
    "        self_l_hist.append(loss_sum / (i + 1))\n",
    "\n",
    "        if len(self_l_hist) > 3 and self_l_hist[-1] == self_l_hist[-2] and self_l_hist[-2] == self_l_hist[-3]:\n",
    "            loss_sum = 0\n",
    "\n",
    "        EPSILON = -0.0001\n",
    "        if loss_sum / (i + 1) < EPSILON:\n",
    "            print(\"HIL TIME!\")\n",
    "            improvements, human_loss = humanInput(anchor_dataset, network, optimizer, loss_fn, HIL_archive)\n",
    "            print(f\"Improvement Count: {improvements}, loss: {human_loss}\")\n",
    "            human_l_hist[epoch] = human_loss\n",
    "\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "\n",
    "if CLUSTER_AND_DISPLAY:\n",
    "    network.eval()\n",
    "    archive = NoveltyArchive()\n",
    "    for i, (anchor_encoding, genome, _, _, _, _, _) in enumerate(anchor_dataset):\n",
    "        anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "        embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "        archive.addToArchive(vec=embedding, genome=genome)\n",
    "\n",
    "    agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "    world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "    world_config.addAgentConfig(agent_config)\n",
    "    config = ResultsConfig(archive=archive, k_clusters=8, world_config=world_config, tsne_perplexity=20,\n",
    "                           tsne_early_exaggeration=2, skip_tsne=False)\n",
    "    results(config)\n",
    "\n",
    "evolution.close()\n",
    "import time\n",
    "\n",
    "WRITE_OUT = False\n",
    "if WRITE_OUT:\n",
    "    i = input(\"QUIT NOW IF YOU DONT WANT TO WRITE OUT\")\n",
    "    print(\"Writing in 3 seconds...\")\n",
    "    time.sleep(2)\n",
    "    print(\"Writing in 1 seconds...\")\n",
    "    time.sleep(1)\n",
    "    print(\"Writing out.\")\n",
    "    network.eval()\n",
    "    for i, (anchor_encoding, _, _, _, _, _, _) in enumerate(anchor_dataset):\n",
    "        anchor_encoding = torch.from_numpy(anchor_encoding).to(device).float()\n",
    "        embedding = network(anchor_encoding.unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n",
    "        anchor_dataset.add_rank(i, embedding, is_array=True)\n",
    "    anchor_dataset.set_rank(2)\n",
    "# Clustering + Analysis\n",
    "# Cluster over saved embeddings\n",
    "from NovelSwarmBehavior.novel_swarms.novelty.NoveltyArchive import NoveltyArchive\n",
    "from NovelSwarmBehavior.novel_swarms.config.ResultsConfig import ResultsConfig\n",
    "from NovelSwarmBehavior.novel_swarms.results.results import main as results\n",
    "from NovelSwarmBehavior.novel_swarms.config.defaults import ConfigurationDefaults\n",
    "from data.swarmset import SwarmDataset, DataBuilder\n",
    "\n",
    "archive = NoveltyArchive()\n",
    "anchor_dataset = SwarmDataset(\"data/new\", rank=2)\n",
    "for i, (_, genome, _, _, _, embedding, _) in enumerate(anchor_dataset):\n",
    "    archive.addToArchive(vec=embedding, genome=genome)\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=archive, k_clusters=7, world_config=world_config, tsne_perplexity=20,\n",
    "                       tsne_early_exaggeration=2, skip_tsne=False)\n",
    "results(config)\n",
    "# Cluster over saved behaviors\n",
    "archive = NoveltyArchive()\n",
    "for i, (_, genome, behavior, _, _, _, _) in enumerate(anchor_dataset):\n",
    "    archive.addToArchive(vec=behavior, genome=genome)\n",
    "\n",
    "agent_config = ConfigurationDefaults.DIFF_DRIVE_AGENT\n",
    "world_config = ConfigurationDefaults.RECTANGULAR_WORLD\n",
    "world_config.addAgentConfig(agent_config)\n",
    "config = ResultsConfig(archive=archive, k_clusters=7, world_config=world_config, tsne_perplexity=80,\n",
    "                       tsne_early_exaggeration=2, skip_tsne=False)\n",
    "results(config)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(human_l_hist, \"b\", label='Human Loss')\n",
    "plt.plot(self_l_hist, \"r\", label='Self Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Time (Epochs)\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
