{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "def vecToCSVLine(vector):\n",
    "    line = \"\"\n",
    "    for i, val in enumerate(vector):\n",
    "        line += str(val)\n",
    "        if i < len(vector) - 1:\n",
    "            line += \", \"\n",
    "    line += \"\\n\"\n",
    "    return line\n",
    "\n",
    "def CSVLineToVec(line):\n",
    "    line_list = line.strip().replace(\"\\n\", \"\").split(\",\")\n",
    "    float_list = []\n",
    "    for i in line_list:\n",
    "        float_list.append(float(i))\n",
    "    float_list = np.array(float_list)\n",
    "    return float_list\n",
    "\n",
    "def oracleTriplet(p1, p2, p3):\n",
    "    b_1, i1 = p1\n",
    "    b_2, i2 = p2\n",
    "    b_3, i3 = p3\n",
    "    d12 = np.linalg.norm(b_1 - b_2)\n",
    "    d23 = np.linalg.norm(b_2 - b_3)\n",
    "    d31 = np.linalg.norm(b_3 - b_1)\n",
    "    if d12 == min([d12, d23, d31]):\n",
    "        trip = [i1, i2, i3]\n",
    "    elif d23 == min([d12, d23, d31]):\n",
    "        trip = [i2, i3, i1]\n",
    "    else:\n",
    "        trip = [i3, i1, i2]\n",
    "    return trip\n",
    "\n",
    "def train(ensemble, anchor_img, pos_img, neg_img, transform=True):\n",
    "    if transform:\n",
    "        pos_images = np.stack([\n",
    "            [ndimage.rotate(pos_img, 90)],\n",
    "            [ndimage.rotate(pos_img, 180)],\n",
    "            [ndimage.rotate(pos_img, 270)],\n",
    "        ])\n",
    "    else:\n",
    "        pos_images = np.array([[pos_img]])\n",
    "    anchor_images = np.stack([[anchor_img] for _ in pos_images])\n",
    "    neg_images = np.stack([[neg_img] for _ in pos_images])\n",
    "    losses = ensemble.train_batch(anchor_images, pos_images, neg_images)\n",
    "    return np.array(losses)\n",
    "\n",
    "def train_batch(ensemble, anchor_batch, pos_batch, neg_batch, transform=True):\n",
    "    losses = ensemble.train_batch(anchor_batch, pos_batch, neg_batch)\n",
    "    return np.array(losses)\n",
    "\n",
    "def visualize_projection(e):\n",
    "    e.eval_mode()\n",
    "    embeddings = []\n",
    "    classes = []\n",
    "    # for i in range(len(data)):\n",
    "    for i in range(1000):\n",
    "        image, _class = sampled_dataset[i][0], sampled_dataset[i][1][0]\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        embed = e.ensemble[0].forward(torch.tensor(image, device=device, dtype=torch.float))\n",
    "        embed = embed.detach().cpu().squeeze(dim=0).numpy()\n",
    "        embeddings.append(embed)\n",
    "        classes.append(_class)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    reduced = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate=\"auto\",\n",
    "        init=\"pca\",\n",
    "        perplexity=40,\n",
    "        early_exaggeration=1\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "    color_classes = {\n",
    "        -1: [0.33, 0.33, 0.33],\n",
    "        0: [1, 0, 0],\n",
    "        1: [0, 1, 0],\n",
    "        2: [0, 0, 1],\n",
    "        3: [0, 1, 1], # Cyan\n",
    "        4: [1, 1, 0], # Yellow\n",
    "        5: [0.5, 0, 0.25] # Pink\n",
    "    }\n",
    "\n",
    "    label_classes = {\n",
    "        -1: \"Unlabeled\",\n",
    "        0: \"Random\",\n",
    "        1: \"Cyclic Pursuit\",\n",
    "        2: \"Milling\",\n",
    "        3: \"Aggregation\",\n",
    "        4: \"Dispersal\",\n",
    "        5: \"Wall Following\"\n",
    "    }\n",
    "\n",
    "    lim = len(reduced)\n",
    "    classes = [-1 for i in range(lim)]\n",
    "\n",
    "    OUT = \"data/oracle\"\n",
    "    with open(os.path.join(OUT, \"original-hand-labeled-classes.txt\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            triplet = CSVLineToVec(line)\n",
    "            classes[int(triplet[0])] = int(triplet[1])\n",
    "\n",
    "    x = [reduced[i][0] for i in range(lim)]\n",
    "    y = [reduced[i][1] for i in range(lim)]\n",
    "    colors = [color_classes[classes[i]] for i in range(lim)]\n",
    "    labels = [label_classes[classes[i]] for i in range(lim)]\n",
    "    plot.grid(True)\n",
    "    # plot.xlim(-5000, 20000)\n",
    "    # plot.ylim(-15000, 10000)\n",
    "    plot.scatter(x, y, c=colors)\n",
    "    plot.legend()\n",
    "    plot.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "sampled_dataset = SwarmDataset(\"data/full-mini\", rank=0)\n",
    "\n",
    "DATA_TEST_SIZE = 1000\n",
    "OUT = \"data/oracle\"\n",
    "\n",
    "with open(os.path.join(OUT, \"triplets.txt\"), \"w\") as f:\n",
    "    for i in range(DATA_TEST_SIZE):\n",
    "        b = np.random.randint(len(sampled_dataset), size=3)\n",
    "        a, p, n = b[0], b[1], b[2]\n",
    "        b1, b2, b3 = sampled_dataset[b[0]][2], sampled_dataset[b[1]][2], sampled_dataset[b[2]][2]\n",
    "        triplet = oracleTriplet((b1, a), (b2, p), (b3, n))\n",
    "        f.write(vecToCSVLine(triplet))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from networks.ensemble import Ensemble\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "PRETRAINING = True\n",
    "target = 0.0001\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ensembleA = Ensemble(size=3, output_size=5, lr=15e-4, learning_decay=0.9, decay_step=3, new_model=True, init=None)\n",
    "ensembleA.load_ensemble(\"full-mini-r2\")\n",
    "# ensembleA.trim(3)\n",
    "\n",
    "sampled_dataset = SwarmDataset(\"data/full-mini\", rank=0)\n",
    "data = sampled_dataset\n",
    "\n",
    "total_queries = 0\n",
    "validation_accuracy = []\n",
    "per_network_acc = [[] for _ in ensembleA.ensemble]\n",
    "overall_training_average = []\n",
    "\n",
    "\n",
    "# Get initial Accuracy\n",
    "# Validation on Oracle Dataset\n",
    "validation_accuracy = []\n",
    "OUT = \"data/oracle\"\n",
    "with open(os.path.join(OUT, \"Dec14-original-model-human-labeled.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    total_score = 0\n",
    "    network_score = np.array([0.0 for _ in ensembleA.ensemble])\n",
    "    for line in lines:\n",
    "        triplet = CSVLineToVec(line)\n",
    "        majority, l = ensembleA.majority_belief(sampled_dataset[int(triplet[0])][0], sampled_dataset[int(triplet[1])][0], sampled_dataset[int(triplet[2])][0])\n",
    "        bin_loss = []\n",
    "        for i, loss in enumerate(l):\n",
    "            bin_loss.append(l[i] < ensembleA.margin)\n",
    "        network_score += np.array(bin_loss)\n",
    "        total_score += majority\n",
    "    acc = total_score / len(lines)\n",
    "    net_acc = network_score / len(lines)\n",
    "\n",
    "    for i, net in enumerate(net_acc):\n",
    "        per_network_acc[i].append(net)\n",
    "        print(f\"Network {i + 1}, Accuracy: {net * 100}%\")\n",
    "\n",
    "    validation_accuracy.append(acc)\n",
    "    print(f\"Pre-Query Validation Complete. Accuracy: {acc * 100}%\")\n",
    "    visualize_projection(ensembleA)\n",
    "\n",
    "TRIAL_SIZE = 50\n",
    "training_triplets = []\n",
    "pretraining_triplets = []\n",
    "for global_epochs in range(TRIAL_SIZE):\n",
    "    # Get Least Agreed Upon Triplets, then query the oracle\n",
    "    SAMPLES = 50000\n",
    "    rand_samp = [\n",
    "        (random.randrange(0, len(data)), random.randrange(0, len(data)), random.randrange(0, len(data))) for i in range(SAMPLES)\n",
    "    ]\n",
    "\n",
    "    entropy = []\n",
    "    for i in range(SAMPLES):\n",
    "        ensembleA.eval_mode()\n",
    "        h = ensembleA.variance(data[rand_samp[i][0]][0], positive=data[rand_samp[i][1]][0], negative=data[rand_samp[i][2]][0])\n",
    "        entropy.append((h, i))\n",
    "    entropy.sort(reverse=True)\n",
    "\n",
    "    # Query the N most-disagreed upon Triplets\n",
    "    QUERIES = 100\n",
    "    N = min(SAMPLES, QUERIES)\n",
    "    for i in range(N):\n",
    "        # Generate UserTriplets\n",
    "        samp_index = entropy[i][1]\n",
    "        a, p, n = rand_samp[samp_index]\n",
    "        b1, b2, b3 = data[a][2], data[p][2], data[n][2]\n",
    "        triplet = oracleTriplet((b1, a), (b2, p), (b3, n))\n",
    "        training_triplets.append(triplet)\n",
    "\n",
    "        # Generate Pretraining Triplets\n",
    "        a, n = rand_samp[i][0], rand_samp[i][2]\n",
    "        pretraining_triplets.append([a, n])\n",
    "\n",
    "    total_queries += N\n",
    "\n",
    "    # Train on the training triplets for 5 epochs\n",
    "    EPOCHS = 500 - (10 * global_epochs)\n",
    "    CONTINUE_PRE = 0\n",
    "    pretraining_counter = 0\n",
    "    pretraining_max = 1000\n",
    "\n",
    "    assert pretraining_max < SAMPLES\n",
    "\n",
    "    training_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        ensembleA.training_mode()\n",
    "        total_loss = [0.0 for _ in ensembleA.ensemble]\n",
    "        pretraining_loss = [0.0 for _ in ensembleA.ensemble]\n",
    "        updates = 0\n",
    "        pre_updates = 0\n",
    "        BATCH_SIZE = min(64, len(training_triplets) // 4)\n",
    "\n",
    "        if BATCH_SIZE < CONTINUE_PRE:\n",
    "            raise Exception(\"Invalid Config: Batch Size must be larger than the continue pretraining metric.\")\n",
    "\n",
    "        random.shuffle(training_triplets)\n",
    "        for i in range(0, len(training_triplets), BATCH_SIZE - CONTINUE_PRE):\n",
    "            anchors = []\n",
    "            positives = []\n",
    "            negatives = []\n",
    "            if (i + BATCH_SIZE - CONTINUE_PRE) > len(training_triplets):\n",
    "                break\n",
    "\n",
    "            for j in range(BATCH_SIZE - CONTINUE_PRE):\n",
    "                triplet = training_triplets[i + j]\n",
    "                anchors.append(data[triplet[0]][0])\n",
    "                positives.append(data[triplet[1]][0])\n",
    "                negatives.append(data[triplet[2]][0])\n",
    "\n",
    "            for l in range(CONTINUE_PRE):\n",
    "                triplet = rand_samp[pretraining_counter % pretraining_max]\n",
    "                anchors.append(data[triplet[0]][0])\n",
    "                positives.append(ndimage.rotate(data[triplet[0]][0], 90))\n",
    "                negatives.append(data[triplet[2]][0])\n",
    "                pretraining_counter += 1\n",
    "\n",
    "            assert len(anchors) == BATCH_SIZE, \"Batch Size is inconsistent in code!\"\n",
    "\n",
    "            anchors = np.expand_dims(anchors, axis=1)\n",
    "            positives = np.expand_dims(positives, axis=1)\n",
    "            negatives = np.expand_dims(negatives, axis=1)\n",
    "            total_loss += train_batch(ensembleA, np.array(anchors), np.array(positives), np.array(negatives))\n",
    "            updates += 1\n",
    "\n",
    "        lrs = ensembleA.evaluate_lr(total_loss / updates)\n",
    "        print(f\"LR: {lrs}\")\n",
    "\n",
    "        average_loss = sum(total_loss) / (len(ensembleA.ensemble) * updates)\n",
    "        training_losses.append(average_loss)\n",
    "\n",
    "        WINDOW_SIZE = 7\n",
    "        window_average = sum(training_losses[-WINDOW_SIZE:]) / WINDOW_SIZE\n",
    "        if len(training_losses) > WINDOW_SIZE and window_average < target:\n",
    "            break\n",
    "        print(f\"Epoch {epoch} Complete. Network Loss: {total_loss / updates}. Ensemble Loss: {average_loss}\")\n",
    "        print(f\"Window Average Loss: {window_average}\")\n",
    "\n",
    "    overall_training_average.append(sum(training_losses) / len(training_losses))\n",
    "\n",
    "    # Validation on Oracle Dataset\n",
    "    if global_epochs % 3 == 0 or global_epochs == TRIAL_SIZE - 1:\n",
    "        ensembleA.eval_mode()\n",
    "        OUT = \"data/oracle\"\n",
    "        with open(os.path.join(OUT, \"Dec14-original-model-human-labeled.txt\"), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            total_score = 0\n",
    "            network_score = np.array([0.0 for _ in ensembleA.ensemble])\n",
    "            for line in lines:\n",
    "                triplet = CSVLineToVec(line)\n",
    "                majority, l = ensembleA.majority_belief(sampled_dataset[int(triplet[0])][0], sampled_dataset[int(triplet[1])][0], sampled_dataset[int(triplet[2])][0])\n",
    "                bin_loss = []\n",
    "                for i, loss in enumerate(l):\n",
    "                    bin_loss.append(l[i] < ensembleA.margin)\n",
    "                network_score += np.array(bin_loss)\n",
    "                total_score += majority\n",
    "            acc = total_score / len(lines)\n",
    "            net_acc = network_score / len(lines)\n",
    "\n",
    "            print(f\"Validation for Epoch: {global_epochs}, total_queries: {len(training_triplets)}\")\n",
    "\n",
    "            for i, net in enumerate(net_acc):\n",
    "                per_network_acc[i].append(net)\n",
    "                print(f\"Network {i + 1}, Accuracy: {net * 100}%\")\n",
    "\n",
    "            validation_accuracy.append(acc)\n",
    "            print(f\"Validation Complete. Accuracy: {acc * 100}%\")\n",
    "            visualize_projection(ensembleA)\n",
    "\n",
    "    ensembleA.set_lr(15e-4, 0.95)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(training_triplets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "ensembleA.save_ensemble(f\"{int(time.time())}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "print(validation_accuracy)\n",
    "plot.plot(validation_accuracy)\n",
    "plot.title(\"Ensemble Accuracy Over Time\")\n",
    "plot.ylabel(\"Accuracy\")\n",
    "plot.xlabel(\"HIL Rounds (epochs)\")\n",
    "plot.show()\n",
    "\n",
    "print(overall_training_average)\n",
    "plot.plot(overall_training_average)\n",
    "plot.title(\"Training Loss\")\n",
    "plot.ylabel(\"Loss\")\n",
    "plot.xlabel(\"Epochs\")\n",
    "plot.show()\n",
    "\n",
    "print(per_network_acc)\n",
    "plot.plot(per_network_acc[0], label=\"Network A\")\n",
    "plot.plot(per_network_acc[1], label=\"Network B\")\n",
    "plot.plot(per_network_acc[2], label=\"Network C\")\n",
    "plot.title(\"Network Accuracy within Ensemble\")\n",
    "plot.ylabel(\"Loss\")\n",
    "plot.xlabel(\"Epochs\")\n",
    "plot.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from networks.ensemble import Ensemble\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "PRETRAINING = True\n",
    "target = 0.01\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ensembleA = Ensemble(size=5, output_size=5, lr=6e-4, learning_decay=0.95, decay_step=3)\n",
    "ensembleA.load_ensemble(\"Nov21-subzerofive\")\n",
    "ensembleA.trim(1)\n",
    "\n",
    "sampled_dataset = SwarmDataset(\"data/full-dual-sensors\", rank=0)\n",
    "data = sampled_dataset\n",
    "\n",
    "total_queries = 0\n",
    "validation_accuracy = [0.6039603960396039]\n",
    "per_network_acc = [[] for _ in ensembleA.ensemble]\n",
    "overall_training_average = []\n",
    "\n",
    "\n",
    "# Get initial Accuracy\n",
    "# Validation on Oracle Dataset\n",
    "validation_accuracy = []\n",
    "OUT = \"data/oracle\"\n",
    "with open(os.path.join(OUT, \"Nov21-Triplets.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    total_score = 0\n",
    "    network_score = np.array([0.0 for _ in ensembleA.ensemble])\n",
    "    for line in lines:\n",
    "        triplet = CSVLineToVec(line)\n",
    "        majority, l = ensembleA.majority_belief(sampled_dataset[int(triplet[0])][0], sampled_dataset[int(triplet[1])][0], sampled_dataset[int(triplet[2])][0])\n",
    "        bin_loss = []\n",
    "        for i, loss in enumerate(l):\n",
    "            bin_loss.append(l[i] == 0.0)\n",
    "        network_score += np.array(bin_loss)\n",
    "        total_score += majority\n",
    "    acc = total_score / len(lines)\n",
    "    net_acc = network_score / len(lines)\n",
    "\n",
    "    for i, net in enumerate(net_acc):\n",
    "        per_network_acc[i].append(net)\n",
    "        print(f\"Network {i + 1}, Accuracy: {net * 100}%\")\n",
    "\n",
    "    validation_accuracy.append(acc)\n",
    "    print(f\"Pre-Query Validation Complete. Accuracy: {acc * 100}%\")\n",
    "\n",
    "TRIAL_SIZE = 10\n",
    "training_triplets = []\n",
    "pretraining_triplets = []\n",
    "for global_epochs in range(TRIAL_SIZE):\n",
    "    # Get Least Agreed Upon Triplets, then query the oracle\n",
    "    SAMPLES = 4000\n",
    "    rand_samp = [\n",
    "        (random.randrange(0, len(data)), random.randrange(0, len(data)), random.randrange(0, len(data))) for i in range(SAMPLES)\n",
    "    ]\n",
    "\n",
    "    # Query the N most-disagreed upon Triplets\n",
    "    N = min(SAMPLES, 25)\n",
    "    for i in range(N):\n",
    "        # Generate UserTriplets\n",
    "        a, p, n = rand_samp[i]\n",
    "        b1, b2, b3 = data[a][2], data[p][2], data[n][2]\n",
    "        triplet = oracleTriplet((b1, a), (b2, p), (b3, n))\n",
    "        training_triplets.append(triplet)\n",
    "\n",
    "        # Generate Pretraining Triplets\n",
    "        a, n = rand_samp[i][0], rand_samp[i][2]\n",
    "        pretraining_triplets.append([a, n])\n",
    "\n",
    "    total_queries += N\n",
    "\n",
    "    # Train on the training triplets for 5 epochs\n",
    "    EPOCHS = 500\n",
    "    CONTINUE_PRE = 2\n",
    "    pretraining_counter = 0\n",
    "    pretraining_max = 1200\n",
    "\n",
    "    assert pretraining_max < SAMPLES\n",
    "\n",
    "    training_losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        ensembleA.training_mode()\n",
    "        total_loss = [0.0 for _ in ensembleA.ensemble]\n",
    "        pretraining_loss = [0.0 for _ in ensembleA.ensemble]\n",
    "        updates = 0\n",
    "        pre_updates = 0\n",
    "        BATCH_SIZE = 4\n",
    "\n",
    "        if BATCH_SIZE < CONTINUE_PRE:\n",
    "            raise Exception(\"Invalid Config: Batch Size must be larger than the continue pretraining metric.\")\n",
    "\n",
    "        random.shuffle(training_triplets)\n",
    "        for i in range(0, len(training_triplets), BATCH_SIZE - CONTINUE_PRE):\n",
    "            anchors = []\n",
    "            positives = []\n",
    "            negatives = []\n",
    "            if (i + BATCH_SIZE - CONTINUE_PRE) > len(training_triplets):\n",
    "                break\n",
    "\n",
    "            for j in range(BATCH_SIZE - CONTINUE_PRE):\n",
    "                triplet = training_triplets[i + j]\n",
    "                anchors.append(data[triplet[0]][0])\n",
    "                positives.append(data[triplet[1]][0])\n",
    "                negatives.append(data[triplet[2]][0])\n",
    "\n",
    "            for l in range(CONTINUE_PRE):\n",
    "                triplet = rand_samp[pretraining_counter % pretraining_max]\n",
    "                anchors.append(data[triplet[0]][0])\n",
    "                positives.append(ndimage.rotate(data[triplet[0]][0], 90))\n",
    "                negatives.append(data[triplet[2]][0])\n",
    "                pretraining_counter += 1\n",
    "\n",
    "            assert len(anchors) == BATCH_SIZE, \"Batch Size is inconsistent in code!\"\n",
    "\n",
    "            anchors = np.expand_dims(anchors, axis=1)\n",
    "            positives = np.expand_dims(positives, axis=1)\n",
    "            negatives = np.expand_dims(negatives, axis=1)\n",
    "            total_loss += train_batch(ensembleA, np.array(anchors), np.array(positives), np.array(negatives))\n",
    "            updates += 1\n",
    "\n",
    "        lrs = ensembleA.evaluate_lr(total_loss / updates)\n",
    "        print(f\"LR: {lrs}\")\n",
    "\n",
    "        average_loss = sum(total_loss) / (len(ensembleA.ensemble) * updates)\n",
    "        training_losses.append(average_loss)\n",
    "\n",
    "        WINDOW_SIZE = 7\n",
    "        window_average = sum(training_losses[-WINDOW_SIZE:]) / WINDOW_SIZE\n",
    "        if len(training_losses) > WINDOW_SIZE and window_average < 0.01:\n",
    "            break\n",
    "        print(f\"Epoch {epoch} Complete. Network Loss: {total_loss / updates}. Ensemble Loss: {average_loss}\")\n",
    "        print(f\"Window Average Loss: {window_average}\")\n",
    "\n",
    "    overall_training_average.append(sum(training_losses) / len(training_losses))\n",
    "\n",
    "    # Validation on Oracle Dataset\n",
    "    ensembleA.eval_mode()\n",
    "    OUT = \"data/oracle\"\n",
    "    with open(os.path.join(OUT, \"Nov21-Triplets.txt\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        total_score = 0\n",
    "        network_score = np.array([0.0 for _ in ensembleA.ensemble])\n",
    "        for line in lines:\n",
    "            triplet = CSVLineToVec(line)\n",
    "            majority, l = ensembleA.majority_belief(sampled_dataset[int(triplet[0])][0], sampled_dataset[int(triplet[1])][0], sampled_dataset[int(triplet[2])][0])\n",
    "            bin_loss = []\n",
    "            for i, loss in enumerate(l):\n",
    "                bin_loss.append(l[i] == 0.0)\n",
    "            network_score += np.array(bin_loss)\n",
    "            total_score += majority\n",
    "        acc = total_score / len(lines)\n",
    "        net_acc = network_score / len(lines)\n",
    "\n",
    "        for i, net in enumerate(net_acc):\n",
    "            per_network_acc[i].append(net)\n",
    "            print(f\"Network {i + 1}, Accuracy: {net * 100}%\")\n",
    "\n",
    "        validation_accuracy.append(acc)\n",
    "        print(f\"Validation Complete. Accuracy: {acc * 100}%\")\n",
    "\n",
    "    ensembleA.set_lr(5e-4, 0.95)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "print(validation_accuracy)\n",
    "plot.plot(validation_accuracy)\n",
    "plot.title(\"Ensemble Accuracy Over Time\")\n",
    "plot.ylabel(\"Accuracy\")\n",
    "plot.xlabel(\"HIL Rounds (epochs)\")\n",
    "plot.show()\n",
    "\n",
    "print(overall_training_average)\n",
    "plot.plot(overall_training_average)\n",
    "plot.title(\"Training Loss\")\n",
    "plot.ylabel(\"Loss\")\n",
    "plot.xlabel(\"Epochs\")\n",
    "plot.show()\n",
    "\n",
    "print(per_network_acc)\n",
    "plot.plot(per_network_acc[0], label=\"Network A\")\n",
    "plot.plot(per_network_acc[1], label=\"Network B\")\n",
    "plot.plot(per_network_acc[2], label=\"Network C\")\n",
    "plot.title(\"Network Accuracy within Ensemble\")\n",
    "plot.ylabel(\"Loss\")\n",
    "plot.xlabel(\"Epochs\")\n",
    "plot.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Obtain info on No Training Model\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from networks.ensemble import Ensemble\n",
    "from data.swarmset import ContinuingDataset, SwarmDataset\n",
    "\n",
    "PRETRAINING = True\n",
    "target = 0.01\n",
    "loss = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ensemble_random = Ensemble(size=1, output_size=5, lr=6e-4, learning_decay=0.95, decay_step=3, init=\"Random\", new_model=True)\n",
    "ensemble_random.load_ensemble(\"full-mini-SIMCLR-3\")\n",
    "sampled_dataset = SwarmDataset(\"data/full-mini\", rank=0)\n",
    "data = sampled_dataset\n",
    "\n",
    "total_queries = 0\n",
    "validation_accuracy = []\n",
    "per_network_acc = [[] for _ in ensemble_random.ensemble]\n",
    "overall_training_average = []\n",
    "\n",
    "# Get initial Accuracy\n",
    "# Validation on Oracle Dataset\n",
    "OUT = \"data/oracle\"\n",
    "with open(os.path.join(OUT, \"Dec3-Original-Mini.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    total_score = 0\n",
    "    network_score = np.array([0.0 for _ in ensemble_random.ensemble])\n",
    "    for line in lines:\n",
    "        triplet = CSVLineToVec(line)\n",
    "        majority, l = ensemble_random.majority_belief(sampled_dataset[int(triplet[0])][0], sampled_dataset[int(triplet[1])][0], sampled_dataset[int(triplet[2])][0])\n",
    "        bin_loss = []\n",
    "        for i, loss in enumerate(l):\n",
    "            bin_loss.append(l[i] < 10.0)\n",
    "        network_score += np.array(bin_loss)\n",
    "        total_score += majority\n",
    "    acc = total_score / len(lines)\n",
    "    net_acc = network_score / len(lines)\n",
    "\n",
    "    for i, net in enumerate(net_acc):\n",
    "        per_network_acc[i].append(net)\n",
    "        print(f\"Network {i + 1}, Accuracy: {net * 100}%\")\n",
    "\n",
    "    validation_accuracy.append(acc)\n",
    "    print(f\"Pre-Query Validation Complete. Accuracy: {acc * 100}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "hil_ensemble = [0.603960396,\t0.6247524752,\t0.6316831683,\t0.6346534653,\t0.6227722772]\n",
    "hil_random = [0.598019802,\t0.5841584158415841,\t0.603960396,\t0.603960396,\t0.6158415842]\n",
    "untrained = [0.572277, 0.572277, 0.572277, 0.572277, 0.572277]\n",
    "hil_samples = [i * 30 for i in range(len(hil_ensemble))]\n",
    "\n",
    "plot.ylim([0.5, 0.65])\n",
    "plot.plot(hil_samples, hil_ensemble, label=\"Ensemble HIL Queries\")\n",
    "plot.plot(hil_samples, hil_random, label=\"Random HIL Queries\")\n",
    "plot.plot(hil_samples, untrained, label=\"Untrained (Random)\")\n",
    "plot.title(\"Early Learning from HIL Triplets\")\n",
    "plot.ylabel(\"Accuracy\")\n",
    "plot.xlabel(\"HIL Samples\")\n",
    "plot.legend()\n",
    "\n",
    "plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
